<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linux on On The Dock</title>
    <link>https://onthedock.github.io/tags/linux/index.xml</link>
    <description>Recent content in Linux on On The Dock</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Handmade with &amp;#9829; by Xavi Aznar</copyright>
    <atom:link href="https://onthedock.github.io/tags/linux/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Espacios de nombres en Kubernetes</title>
      <link>https://onthedock.github.io/post/170723-espacios-de-nombres-en-k8s/</link>
      <pubDate>Sun, 23 Jul 2017 20:04:45 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170723-espacios-de-nombres-en-k8s/</guid>
      <description>&lt;p&gt;Los &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&#34;&gt;&lt;em&gt;namespaces&lt;/em&gt; (espacios de nombres)&lt;/a&gt; en Kubernetes permiten establecer un nivel adicional de separación entre los contenedores que comparten los recursos de un clúster.&lt;/p&gt;

&lt;p&gt;Esto es especialmente útil cuando diferentes grupos de DevOps usan el mismo clúster y existe el riesgo potencial de colisión de nombres de los &lt;em&gt;pods&lt;/em&gt;, etc usados por los diferentes equipos.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Los espacios de nombres también facilitan la creación de cuotas para limitar los recursos disponibles para cada &lt;em&gt;namespace&lt;/em&gt;. Puedes considerar los espacios de nombres como clústers &lt;em&gt;virtuales&lt;/em&gt; sobre el clúster físico de Kubernetes. De esta forma, proporcionan separación lógica entre los entornos de diferentes equipos.&lt;/p&gt;

&lt;p&gt;Kubernetes proporciona dos &lt;em&gt;namespaces&lt;/em&gt; por defecto: &lt;code&gt;kube-system&lt;/code&gt; y &lt;code&gt;default&lt;/code&gt;. A &lt;em&gt;grosso modo&lt;/em&gt;, los objetos &amp;ldquo;de usuario&amp;rdquo; se crean en el espacio de nombres &lt;code&gt;default&lt;/code&gt;, mientras que los de &amp;ldquo;sistema&amp;rdquo; se encuentran en &lt;code&gt;kube-system&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Para ver los espacios de nombres en el clúster, ejecuta:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get namespaces
NAME          STATUS    AGE
default       Active    21d
kube-system   Active    21d
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Puedes obtener el mismo resultado usando &lt;code&gt;ns&lt;/code&gt; en vez de &lt;code&gt;namespaces&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Para comprobar la separación lógica entre los objetos de diferentes &lt;em&gt;namespaces&lt;/em&gt;, lista los pods mediante &lt;code&gt;kubectl get pods&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-3225377387-xdth3   1/1       Running   0          7d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Analizando al detalle el &lt;em&gt;pod&lt;/em&gt; mediante &lt;code&gt;kubectl describe pod nginx-3225377387-xdth3&lt;/code&gt;, observa como se encuentra en el espacio de nombres &lt;code&gt;default&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe pod nginx-3225377387-xdth3
Name:    nginx-3225377387-xdth3
Namespace:  default
Node:    k8s-snc/192.168.1.10
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compara los resultados obtenidos con los comandos anteriores con el de &lt;code&gt;kubectl get pods --all-namespaces&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                              READY     STATUS    RESTARTS   AGE
default       nginx-3225377387-xdth3            1/1       Running   0          7d
kube-system   etcd-k8s-snc                      1/1       Running   3          21d
kube-system   kube-apiserver-k8s-snc            1/1       Running   3          21d
kube-system   kube-controller-manager-k8s-snc   1/1       Running   3          21d
kube-system   kube-dns-2425271678-xbzt8         3/3       Running   12         21d
kube-system   kube-proxy-tbstt                  1/1       Running   3          21d
kube-system   kube-scheduler-k8s-snc            1/1       Running   3          21d
kube-system   weave-net-snspp                   2/2       Running   9          20d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La primera columna de la salida del comando anterior indica el espacio de nombres en el que se encuentra cada &lt;em&gt;pod&lt;/em&gt;, en este caso.&lt;/p&gt;

&lt;h1 id=&#34;crea-un-nuevo-espacio-de-nombres&#34;&gt;Crea un nuevo espacio de nombres&lt;/h1&gt;

&lt;p&gt;Para crear un &lt;em&gt;namespace&lt;/em&gt;, crea un fichero &lt;code&gt;YAML&lt;/code&gt; como el siguiente:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Namespace
metadata:
   name: developers
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;El nombre del &lt;em&gt;namespace&lt;/em&gt; debe ser compatible con una entrada válida de DNS.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Para crear el &lt;em&gt;namespace&lt;/em&gt;, ejecuta:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl create -f ns-developers.yaml
namespace &amp;quot;developers&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al obtener la lista de espacios de nombres disponibles, observa que ahora el nuevo &lt;em&gt;namespace&lt;/em&gt; aparece:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get ns
NAME          STATUS    AGE
default       Active    21d
developers    Active    55s
kube-system   Active    21d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Observa con detalle el &lt;em&gt;namespace&lt;/em&gt; creado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe ns developers
Name:    developers
Labels:     &amp;lt;none&amp;gt;
Annotations:   &amp;lt;none&amp;gt;
Status:     Active

No resource quota.

No resource limits.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Idealmente, el particionamiento del clúster en espacios de nombres permite repartir los recursos del clúster imponiendo cuotas, de manera que los objetos de un determinado &lt;em&gt;namespace&lt;/em&gt; no acaparen todos los recursos disponibles.&lt;/p&gt;

&lt;p&gt;A continuación indico cómo establecer algunos límites para el &lt;em&gt;namespace&lt;/em&gt; (basado en &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/apply-resource-quota-limit/&#34;&gt;Apply Resource Quotas and Limits&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;aplicando-quotas-al-número-de-objetos-en-el-namespace&#34;&gt;Aplicando quotas al número de objetos en el &lt;em&gt;namespace&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Para aplicar una cuota, creamos un fichero &lt;code&gt;YAML&lt;/code&gt; del tipo &lt;code&gt;ResourceQuota&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    persistentvolumeclaims: &amp;quot;2&amp;quot;
    services.loadbalancers: &amp;quot;2&amp;quot;    
    services.nodeports: &amp;quot;0&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta cuota limita el número de:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;volúmenes persistentes (2)&lt;/li&gt;
&lt;li&gt;balanceadores de carga (2)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;node ports&lt;/em&gt; (0)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para crear la cuota, aplica el fichero &lt;code&gt;YAML&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Debes especificar el &lt;em&gt;namespace&lt;/em&gt; donde aplicar la cuota.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl create -f quota-object-counts.yaml --namespace developers
resourcequota &amp;quot;object-counts&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Comprobamos que se ha aplicado la cuota al &lt;em&gt;namespace&lt;/em&gt; &lt;code&gt;developers&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe ns developers
Name:    developers
Labels:     &amp;lt;none&amp;gt;
Annotations:   &amp;lt;none&amp;gt;
Status:     Active

Resource Quotas
 Name:         object-counts
 Resource      Used  Hard
 --------      ---   ---
 persistentvolumeclaims 0  2
 services.loadbalancers 0  2
 services.nodeports  0  0

No resource limits.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta cuota impide la creación de más objetos de cada tipo de los especificados en la cuota (es decir, como máximo, puede haber dos &lt;em&gt;load balancers&lt;/em&gt; en el espacio de nombres &lt;code&gt;developers&lt;/code&gt;).&lt;/p&gt;

&lt;h2 id=&#34;aplicando-cuotas-a-los-recursos-del-namespace&#34;&gt;Aplicando cuotas a los recursos del &lt;em&gt;namespace&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Habitualmente los límites que se suelen establecer para cada espacio de nombres están enfocados a limitar los recursos de CPU y memoria del &lt;em&gt;namespace&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;El siguiente fichero &lt;code&gt;YAML&lt;/code&gt; especifica un límite de 2 CPUs y 2GB de memoria. Además, especifica una limitación en cuanto a las peticiones que debe realizar un &lt;em&gt;pod&lt;/em&gt; en este espacio de nombres. Finalmente, también se establece una limitación de como máximo, 4 &lt;em&gt;pods&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    limits.cpu: &amp;quot;2&amp;quot;
    limits.memory: 2Gi    
    requests.cpu: &amp;quot;1&amp;quot;
    requests.memory: 1Gi 
    pods: &amp;quot;4&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aplicamos la nueva cuota mediante:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;De nuevo, recuerda que debes especificar el &lt;em&gt;namespace&lt;/em&gt; al que aplicar la cuota.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl create -f quota-compute-resources.yaml --namespace developers
resourcequota &amp;quot;compute-resources&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El espacio de nombres está limitado ahora de la siguiente manera:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe ns developers
Name:          developers
Labels:        &amp;lt;none&amp;gt;
Annotations:   &amp;lt;none&amp;gt;
Status:     Active

Resource Quotas
 Name:            compute-resources
 Resource         Used  Hard
 --------         ---   ---
 limits.cpu       0     2
 limits.memory    0     2Gi
 pods             0     4
 requests.cpu     0     1
 requests.memory  0     1Gi

 Name:                  object-counts
 Resource               Used  Hard
 --------               ---   ---
 persistentvolumeclaims  0     2
 services.loadbalancers  0     2
 services.nodeports      0     0

No resource limits.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La limitación impuesta en las peticiones (&lt;code&gt;requests&lt;/code&gt;) de memoria y CPU &lt;strong&gt;obligan a que se especifiquen límites en la definición de los recursos asignados a cada &lt;em&gt;pod&lt;/em&gt;&lt;/strong&gt;. En general, al crear la definición de un &lt;em&gt;deployment&lt;/em&gt; no se especifican estos límites, lo que puede provocar algo de desconcierto.&lt;/p&gt;

&lt;p&gt;Vamos a crear un &lt;em&gt;Deployment&lt;/em&gt; en el &lt;em&gt;namespace&lt;/em&gt; &lt;code&gt;Developers&lt;/code&gt;. Aunque asignamos el &lt;em&gt;deployment&lt;/em&gt; al &lt;em&gt;namespace&lt;/em&gt; desde la línea de comando, en un fichero &lt;code&gt;YAML&lt;/code&gt; usaríamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
metadata:
   name: ejemplo
   namespace: developers  
spec:
   ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creamos un &lt;em&gt;deployment&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl run nginx --image=nginx --replicas=1 --namespace=developers
deployment &amp;quot;nginx&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Todo parece ok hasta que buscamos el &lt;em&gt;pod&lt;/em&gt; que debería crearse:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get pods --namespace developers
No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Analizamos el detalle del &lt;em&gt;deployment&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe deployment nginx --namespace developers
Name:       nginx
Namespace:     developers
CreationTimestamp:   Sun, 23 Jul 2017 21:19:57 +0200
Labels:        run=nginx
Annotations:      deployment.kubernetes.io/revision=1
Selector:      run=nginx
Replicas:      1 desired | 0 updated | 0 total | 0 available | 1 unavailable
StrategyType:     RollingUpdate
MinReadySeconds:  0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:   run=nginx
  Containers:
   nginx:
    Image:     nginx
    Port:      &amp;lt;none&amp;gt;
    Environment:  &amp;lt;none&amp;gt;
    Mounts:    &amp;lt;none&amp;gt;
  Volumes:     &amp;lt;none&amp;gt;
Conditions:
  Type         Status   Reason
  ----         ------   ------
  Available       True  MinimumReplicasAvailable
  ReplicaFailure  True  FailedCreate
OldReplicaSets:      &amp;lt;none&amp;gt;
NewReplicaSet:    nginx-4217019353 (0/1 replicas created)
Events:
  FirstSeen LastSeen Count From        SubObjectPath  Type     Reason         Message
  --------- -------- ----- ----        -------------  -------- ------         -------
  2m     2m    1  deployment-controller         Normal      ScalingReplicaSet Scaled up replica set nginx-4217019353 to 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No se ha creado el &lt;em&gt;ReplicaSet&lt;/em&gt;. Vamos a ver porqué:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe rs nginx-4217019353 --namespace developers
Name:    nginx-4217019353
Namespace:  developers
Selector:   pod-template-hash=4217019353,run=nginx
Labels:     pod-template-hash=4217019353
      run=nginx
Annotations:   deployment.kubernetes.io/desired-replicas=1
      deployment.kubernetes.io/max-replicas=2
      deployment.kubernetes.io/revision=1
Controlled By: Deployment/nginx
Replicas:   0 current / 1 desired
Pods Status:   0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:   pod-template-hash=4217019353
      run=nginx
  Containers:
   nginx:
    Image:     nginx
    Port:      &amp;lt;none&amp;gt;
    Environment:  &amp;lt;none&amp;gt;
    Mounts:    &amp;lt;none&amp;gt;
  Volumes:     &amp;lt;none&amp;gt;
Conditions:
  Type         Status   Reason
  ----         ------   ------
  ReplicaFailure  True  FailedCreate
Events:
  FirstSeen LastSeen Count From        SubObjectPath  Type     Reason      Message
  --------- -------- ----- ----        -------------  -------- ------      -------
  4m     1m    16 replicaset-controller         Warning     FailedCreate   Error creating: pods &amp;quot;nginx-4217019353-&amp;quot; is forbidden: failed quota: compute-resources: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El &lt;em&gt;deployment&lt;/em&gt; crea un &lt;em&gt;ReplicaSet&lt;/em&gt;, que a su vez intenta crear uno o más &lt;em&gt;pods&lt;/em&gt;. Como en el &lt;em&gt;Deployment&lt;/em&gt; no se ha especificado un límite para la CPU y memoria del &lt;em&gt;pod&lt;/em&gt; y lo hemos exigido en las cuotas impuestas al &lt;em&gt;namespace&lt;/em&gt;, la creación del &lt;em&gt;pod&lt;/em&gt; falla. El mensaje de error es claro:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error creating: pods &amp;quot;nginx-4217019353-&amp;quot; is forbidden: failed quota: compute-resources: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si creamos el &lt;em&gt;pod&lt;/em&gt; especificando los límites:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl run nginx \
  --image=nginx \
  --replicas=1 \
  --requests=cpu=100m,memory=256Mi \
  --limits=cpu=200m,memory=512Mi \
  --namespace=developers

$ kubectl get pods --namespace developers
NAME                     READY     STATUS    RESTARTS   AGE
nginx-2432944439-1zqs7   1/1       Running   0          19s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora, al revisar el &lt;em&gt;namespace&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe ns developers
Name:    developers
Labels:     &amp;lt;none&amp;gt;
Annotations:   &amp;lt;none&amp;gt;
Status:     Active

Resource Quotas
 Name:            compute-resources
 Resource         Used  Hard
 --------         ---   ---
 limits.cpu       200m  2
 limits.memory    512Mi 2Gi
 pods             1     4
 requests.cpu     100m  1
 requests.memory  256Mi 1Gi

 Name:                  object-counts
 Resource               Used  Hard
 --------               ---   ---
 persistentvolumeclaims 0     2
 services.loadbalancers 0     2
 services.nodeports     0     0

No resource limits.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;namespace-y-dns&#34;&gt;&lt;em&gt;Namespace&lt;/em&gt; y DNS&lt;/h1&gt;

&lt;p&gt;Cuando se crear un &lt;em&gt;service&lt;/em&gt;, se crea la correspondiente entrada en el DNS. Esta entrada es de la forma &lt;code&gt;&amp;lt;nombre-servicio&amp;gt;.&amp;lt;espacio-de-nombres&amp;gt;.svc.cluster.local&lt;/code&gt;, lo que significa que si un contenedor usa únicamente &lt;code&gt;&amp;lt;nombre-de-servicio&amp;gt;&lt;/code&gt;, la resolución del nombre se realizará de forma local en el espacio de nombres en el que se encuentre.&lt;/p&gt;

&lt;p&gt;Esta configuración permite usar la misma configuración entre diferentes espacios de nombres (por ejemplo &lt;em&gt;Desarrollo&lt;/em&gt;, &lt;em&gt;Integración&lt;/em&gt; y &lt;em&gt;Producción&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Para que un contenedor pueda resolver el nombre de otro contenedor en otro &lt;em&gt;namespace&lt;/em&gt;, debes usar el FQDN.&lt;/p&gt;

&lt;h1 id=&#34;borrando-un-namespace&#34;&gt;Borrando un &lt;em&gt;namespace&lt;/em&gt;&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;AVISO: Al borrar un &lt;em&gt;namespace&lt;/em&gt; se borran &lt;strong&gt;todos los objetos&lt;/strong&gt; del espacio de nombres.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Para borrar un &lt;em&gt;namespace&lt;/em&gt;, usa el comando &lt;code&gt;delete&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl delete ns developers
namespace &amp;quot;developers&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El borrado del &lt;em&gt;namespace&lt;/em&gt; es asíncrono, por lo que puedes verlo como &lt;code&gt;Terminating&lt;/code&gt; hasta que se realiza el borrado definitivo del mismo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get ns
NAME          STATUS        AGE
default       Active        21d
developers    Terminating   2h
kube-system   Active        21d
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;resumen&#34;&gt;Resumen&lt;/h1&gt;

&lt;p&gt;En este artículo hemos visto qué es un &lt;em&gt;Namespace&lt;/em&gt; y para qué sirve.&lt;/p&gt;

&lt;p&gt;También hemos visto cómo crear un espacio de nombres, obtener información sobre él y crear objetos en el &lt;em&gt;namespace&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Hemos aplicado cuotas para limitar los recursos disponibles y hemos visto cómo afecta a la creación de &lt;em&gt;deployments&lt;/em&gt; en el espacio de nombres.&lt;/p&gt;

&lt;p&gt;Finalmente, hemos eliminado el espacio de nombres (y todos los objetos contenidos en él).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mi primera aplicación en Kubernetes</title>
      <link>https://onthedock.github.io/post/170716-mi-primera-app-en-kubernetes/</link>
      <pubDate>Sun, 16 Jul 2017 19:38:17 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170716-mi-primera-app-en-kubernetes/</guid>
      <description>&lt;p&gt;Después de &lt;a href=&#34;https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/&#34;&gt;crear un cluster de un solo nodo&lt;/a&gt;, en esta entrada explico los pasos para publicar una aplicación en el clúster.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;objetivo-publicar-una-aplicación&#34;&gt;Objetivo: publicar una aplicación&lt;/h1&gt;

&lt;p&gt;El objetivo de esta entrada es publicar una aplicación en el clúster accesible a través de la IP del &lt;em&gt;host&lt;/em&gt;. Es decir, a todos los efectos, el usuario accede a la aplicación sin ningún conocimiento de que se encuentra en el clúster, corriendo sobre uno o varios contenedores.&lt;/p&gt;

&lt;h1 id=&#34;usa-ficheros-yaml&#34;&gt;Usa ficheros YAML&lt;/h1&gt;

&lt;p&gt;Los dos objetos necesarios para &lt;em&gt;publicar&lt;/em&gt; la aplicación, el &lt;em&gt;Deployment&lt;/em&gt; y el &lt;em&gt;Service&lt;/em&gt;, pueden crearse directamente o usando un fichero YAML.&lt;/p&gt;

&lt;p&gt;El &lt;a href=&#34;https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-intro/&#34;&gt;tutorial de Kubernetes&lt;/a&gt; usa el método &lt;em&gt;online&lt;/em&gt;, creando los objetos directamente en la línea de comando. Sin embargo, creo que este método no debería usarse nunca en producción (porque pueden pasar cosas &lt;strong&gt;&lt;a href=&#34;https://aws.amazon.com/message/41926/&#34;&gt;muy&lt;/a&gt; &lt;a href=&#34;http://money.cnn.com/2017/03/02/technology/amazon-s3-outage-human-error/index.html&#34;&gt;malas&lt;/a&gt;&lt;/strong&gt;), por lo que vamos a usar los ficheros de configuración YAML para crear y actualizar los objetos en el clúster.&lt;/p&gt;

&lt;h2 id=&#34;ejemplo-de-proceso-de-aprobación&#34;&gt;Ejemplo de proceso de aprobación&lt;/h2&gt;

&lt;p&gt;Idealmente, cualquier cambio en el estado de la aplicación en el clúster debería seguir un flujo similar al de cualquier otra modificación que afecte a un sistema de producción; es decir, el DevOp clona el repositorio con la configuración de la aplicación en su equipo, hace los cambios y los verifica. Una vez satisfecho, los sube al repositorio (pasando por el sistema de aprobación establecido, como por ejemplo, una &lt;a href=&#34;https://help.github.com/articles/about-pull-requests/&#34;&gt;&lt;em&gt;pull request&lt;/em&gt;&lt;/a&gt;). Una vez aprobado, se incorpora el cambio al repositorio de configuración de la aplicación en producción, desde donde se aplica al entorno de producción.&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170716/pull-request-flow.png&#34; alt=&#34;Mi primera aplicación en Kubernetes images/170716/pull-request-flow.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    &lt;a href=&#34;https://docs.rhodecode.com/RhodeCode-Enterprise/collaboration/pr-flow.html&#34; target=&#34;_blank&#34;&gt;Proceso de aprobación basado en pull request.&lt;/a&gt;
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;h1 id=&#34;define-el-deployment&#34;&gt;Define el &lt;em&gt;deployment&lt;/em&gt;&lt;/h1&gt;

&lt;p&gt;Como vimos en la entrada de &lt;a href=&#34;https://onthedock.github.io/post/170528-revision-de-conceptos/&#34;&gt;revisión de conceptos&lt;/a&gt;, el primer paso para desplegar una aplicación en un clúster Kubernetes es crear un &lt;em&gt;deployment&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;El fichero de declaración del &lt;em&gt;deployment&lt;/em&gt; tiene tres bloques (la estructura es común al resto de objetos):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Cabecera&lt;/li&gt;
&lt;li&gt;Metadatos&lt;/li&gt;
&lt;li&gt;Especificaciones&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx 
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable-alpine
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cabecera&#34;&gt;Cabecera&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1beta1
kind: Deployment
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En primer lugar, la &lt;a href=&#34;https://kubernetes.io/docs/api-reference/v1.6/#deployment-v1beta1-apps&#34;&gt;versión de API&lt;/a&gt; que usamos para definir el &lt;em&gt;deployment&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;En &lt;code&gt;kind&lt;/code&gt;, especificamos el tipo de objeto que vamos a definir.&lt;/p&gt;

&lt;h2 id=&#34;metadatos&#34;&gt;Metadatos&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;metadata:
  name: nginx 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Datos que describen el &lt;em&gt;deployment&lt;/em&gt;; el único necesario es el nombre del &lt;em&gt;deployment&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;especificaciones&#34;&gt;Especificaciones&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable-alpine
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En el apartado de especificaciones se declaran el número de réplicas de la aplicación que queremos en ejecución. Kubernetes se encarga de mantener este número de réplicas en ejecución en todo momento, respondiendo automáticamente en caso de fallo de alguno de los &lt;em&gt;pods&lt;/em&gt; o de los nodos del clúster.&lt;/p&gt;

&lt;p&gt;Para poder crear nuevos &lt;em&gt;pods&lt;/em&gt; si es necesario, debemos definir una &lt;code&gt;template&lt;/code&gt; (plantilla) a partir de la cual crearlos.&lt;/p&gt;

&lt;p&gt;Como, al fin y al cabo estamos definiendo un objeto de tipo &lt;em&gt;pod&lt;/em&gt;, tenemos de nuevo una sección de metadatos (en este caso, etiquetas para los &lt;em&gt;pods&lt;/em&gt; del &lt;em&gt;deployment&lt;/em&gt;) y las especificaciones para crear un &lt;em&gt;pod&lt;/em&gt;: el nombre, la imagen base para el contenedor del &lt;em&gt;pod&lt;/em&gt; y el puerto publicado por el contenedor.&lt;/p&gt;

&lt;p&gt;Pueden definirse muchos otros parámetros, pero estos son los mínimos indispensables para tener un &lt;em&gt;deployment&lt;/em&gt; funcional (el único requerido es el &lt;code&gt;name&lt;/code&gt;).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;La imagen se descarga por defecto desde DockerHub; si quieres usar un &lt;em&gt;registry&lt;/em&gt; alternativo, debes indicar la URL completa al recurso.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;crea-el-deployment&#34;&gt;Crea el &lt;em&gt;deployment&lt;/em&gt;&lt;/h1&gt;

&lt;p&gt;En el &lt;a href=&#34;https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/&#34;&gt;cluster de un solo nodo&lt;/a&gt; de pruebas, he subido el fichero &lt;code&gt;nginx-deployment.yaml&lt;/code&gt; con la definición del apartado anterior.&lt;/p&gt;

&lt;p&gt;Para crear el &lt;em&gt;deployment&lt;/em&gt; usamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl create -f nginx-deployment.yaml
deployment &amp;quot;nginx&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Otra opción es usar la opción &lt;code&gt;apply&lt;/code&gt;, que actualiza el &lt;em&gt;deployment&lt;/em&gt; si ya existe.&lt;/p&gt;

&lt;p&gt;Una vez creado, lo examinamos en detalle con &lt;code&gt;describe&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe deployment nginx
Name:       nginx
Namespace:     default
CreationTimestamp:   Sun, 16 Jul 2017 11:06:48 +0200
Labels:        app=nginx
Annotations:      deployment.kubernetes.io/revision=1
Selector:      app=nginx
Replicas:      1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:     RollingUpdate
MinReadySeconds:  0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:   app=nginx
  Containers:
   nginx:
    Image:     nginx:stable-alpine
    Port:      80/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:    &amp;lt;none&amp;gt;
  Volumes:     &amp;lt;none&amp;gt;
Conditions:
  Type      Status   Reason
  ----      ------   ------
  Available    True  MinimumReplicasAvailable
  Progressing  True  NewReplicaSetAvailable
OldReplicaSets:   &amp;lt;none&amp;gt;
NewReplicaSet: nginx-3287103792 (1/1 replicas created)
Events:
  FirstSeen LastSeen Count From        SubObjectPath  Type     Reason         Message
  --------- -------- ----- ----        -------------  -------- ------         -------
  4m     4m    1  deployment-controller         Normal      ScalingReplicaSet Scaled up replica set nginx-3287103792 to 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El &lt;em&gt;pod&lt;/em&gt; creado por Kubernetes tiene una IP asignada y está escuchando en el puerto 80, pero de momento sólo es accesible &lt;em&gt;desde dentro&lt;/em&gt; del clúster.&lt;/p&gt;

&lt;p&gt;Puedes comprobarlo obteniendo el nombre del &lt;em&gt;pod&lt;/em&gt; y usando &lt;code&gt;describe&lt;/code&gt; para obtener su IP. A continuación, mediante &lt;code&gt;curl&lt;/code&gt; obtén la página de bienvenida de Nginx:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;$ curl http://10.32.0.4:80
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si intentas algo parecido &lt;em&gt;desde fuera&lt;/em&gt; del clúster, no tendrás respuesta.&lt;/p&gt;

&lt;h2 id=&#34;modifica-el-número-de-réplicas&#34;&gt;Modifica el número de réplicas&lt;/h2&gt;

&lt;p&gt;A modo de experimento, puedes modificar el número de réplicas directamente desde la línea de comandos usando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl scale deployment nginx --replicas 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Y revisando el resultado del comando anterior:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get pods -l app=nginx
NAME                     READY     STATUS    RESTARTS   AGE
nginx-3225377387-l0hwp   1/1       Running   0          3m
nginx-3225377387-ptkdt   1/1       Running   0          3m
nginx-3225377387-rrz1x   1/1       Running   0          11m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lo correcto, de todas maneras, habría sido modificar el fichero &lt;code&gt;nginx-deployment.yaml&lt;/code&gt; y aplicar la nueva configuración mediante &lt;code&gt;kubectl apply -f nginx-deployment.yaml&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&#34;publica-la-aplicación&#34;&gt;Publica la aplicación&lt;/h1&gt;

&lt;p&gt;La aplicación está disponible en el clúster, pero sólo es accesible &lt;em&gt;desde dentro&lt;/em&gt; del clúster. De este modo no es especialmente útil (teniendo en cuenta que se trata de un servidor web).&lt;/p&gt;

&lt;p&gt;El siguiente paso es definir un &lt;em&gt;Service&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Un &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34;&gt;service&lt;/a&gt; define un conjunto de &lt;em&gt;pods&lt;/em&gt; y una política de acceso.&lt;/p&gt;

&lt;p&gt;Esto quiere decir que el servicio agrupa &lt;em&gt;pods&lt;/em&gt; independientes en un conjunto que &amp;ldquo;trabaja en equipo&amp;rdquo; para hacer &amp;ldquo;algo&amp;rdquo; (que es lo que se llama &lt;em&gt;microservice&lt;/em&gt;, en terminología &lt;em&gt;DevOp&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;En cuanto a la política de acceso, es una forma de indicar que -en la mayoría de casos- se asigna un puerto &lt;strong&gt;público&lt;/strong&gt; para que se pueda acceder a la aplicación &lt;em&gt;desde fuera&lt;/em&gt; del clúster.&lt;/p&gt;

&lt;h1 id=&#34;define-el-service&#34;&gt;Define el &lt;em&gt;service&lt;/em&gt;&lt;/h1&gt;

&lt;p&gt;El &lt;em&gt;service&lt;/em&gt; tiene los mismos tres bloques que el &lt;em&gt;deployment&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
  type: LoadBalancer
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cabecera-1&#34;&gt;Cabecera&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la cabecera tenemos la versión de la API. Los &lt;em&gt;services&lt;/em&gt; existen desde la primera versión del API de Kubernetes, a diferencia de los &lt;em&gt;deployments&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Recuerda de la entrada de &lt;a href=&#34;https://onthedock.github.io/post/170528-revision-de-conceptos/&#34;&gt;revisión de conceptos&lt;/a&gt; que antes de los &lt;em&gt;deployments&lt;/em&gt; se usaban directamente los &lt;em&gt;Replication Controllers&lt;/em&gt;, que después de mejoraron y se convirtieron en &lt;em&gt;ReplicaSets&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En cuanto al tipo de objeto, en este caso definimos un &lt;code&gt;Service&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;metadatos-1&#34;&gt;Metadatos&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;metadata:
  name: nginx
  labels:
    app: nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tampoco hay sorpresas en cuanto a los metadatos del servicio; en este caso, además del nombre, he añadido la etiqueta &lt;code&gt;app: nginx&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;especificaciones-1&#34;&gt;Especificaciones&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
  type: LoadBalancer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Como hemos visto en la definición de &lt;em&gt;servicio&lt;/em&gt;, éste sirve para agrupar &lt;em&gt;pods&lt;/em&gt;. El conjunto de &lt;em&gt;pods&lt;/em&gt; se define mediante un &lt;strong&gt;selector&lt;/strong&gt;, que en este caso, se trata de la etiqueta &lt;code&gt;app: nginx&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Usar etiquetas permite organizar los &lt;em&gt;pods&lt;/em&gt; con total flexibilidad. Por ejemplo, podrías tener diferentes frontales (Nginx para contenido estático y Apache para el resto, por ejemplo) y agruparlos todos bajo una etiqueta llamada &lt;code&gt;tier: front-end&lt;/code&gt;, por ejemplo, aunque cada tipo tenga, además, etiquetas específicas (&lt;code&gt;app: nginx&lt;/code&gt; y &lt;code&gt;app: apache&lt;/code&gt;).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;La elección de las etiquetas es un asunto &lt;strong&gt;muy importante&lt;/strong&gt; que debes tener lo mejor organizado posible para tus aplicaciones.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A continuación, en la definición del servicio, tenemos la parte de &lt;em&gt;política de acceso&lt;/em&gt;: una sección de &lt;code&gt;ports&lt;/code&gt; en la que se define el protocolo y puerto (o puertos) y el tipo de &amp;ldquo;conexión&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Finalmente, mediante &lt;code&gt;type&lt;/code&gt;, define el tipo de acceso &lt;em&gt;desde el exterior&lt;/em&gt;. En el ejemplo he elegido el tipo &lt;code&gt;LoadBalancer&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Este tipo de acceso creo que no está disponible en Minikube, por lo que si usas este entorno para las pruebas, quizás debas usar &lt;code&gt;type: NodePort&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En resumen, el servicio expone el conjunto de &lt;em&gt;pods&lt;/em&gt; que verifican la condición expuesta en el &lt;code&gt;selector&lt;/code&gt; y conecta los puertos locales -de los contenedores- con puertos externos a nivel de clúster de Kubernetes.&lt;/p&gt;

&lt;h1 id=&#34;crea-el-service&#34;&gt;Crea el &lt;em&gt;service&lt;/em&gt;&lt;/h1&gt;

&lt;p&gt;El método para crear/actualizar el &lt;em&gt;service&lt;/em&gt; es el mismo que para el &lt;em&gt;deployment&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl apply -f nginx-service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Una vez aplicado el &lt;em&gt;service&lt;/em&gt;, usa &lt;code&gt;describe&lt;/code&gt; para inspeccionar en detalle el objeto:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl describe service nginx
Name:       nginx
Namespace:     default
Labels:        app=nginx
Annotations:      kubectl.kubernetes.io/last-applied-configuration={&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Service&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx&amp;quot;},&amp;quot;name&amp;quot;:&amp;quot;nginx&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;ports&amp;quot;:[{&amp;quot;port...
Selector:      app=nginx
Type:       LoadBalancer
IP:         10.107.44.10
Port:       &amp;lt;unset&amp;gt;  80/TCP
NodePort:      &amp;lt;unset&amp;gt;  31010/TCP
Endpoints:     10.32.0.3:80
Session Affinity: None
Events:        &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para saber con qué puerto externo se ha conectado el puerto local 80/TCP necesitamos revisar el detalle del servicio creado. Como no hemos especificado ningún &lt;code&gt;targetPort&lt;/code&gt; Kubernetes asigna uno libre al azar.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;El comportamiento es el mismo que cuando se lanza un contenedor mediante &lt;code&gt;docker run -P&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En mi caso el &lt;code&gt;NodePort&lt;/code&gt; asignado es el 31010/TCP. Esto significa que si acceso a la IP del clúster a través del puerto 31010, Kubernetes redirigirá la petición al puerto 80 del servicio, con lo que obtendré la página de bienvenida de Nginx.&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170716/nginx-en-kubernetes.png&#34; alt=&#34;Mi primera aplicación en Kubernetes images/170716/nginx-en-kubernetes.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Este es el resultado marcado como éxito para este experimento.&lt;/p&gt;

&lt;h1 id=&#34;resumen&#34;&gt;Resumen&lt;/h1&gt;

&lt;p&gt;En este artículo he explicado los pasos necesarios para publicar una aplicación en un clúster de Kubernetes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Crear un &lt;em&gt;Deployment&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Crear un &lt;em&gt;Service&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;El &lt;em&gt;deployment&lt;/em&gt; crea los diferentes &lt;em&gt;pods&lt;/em&gt; que componen la aplicación. El &lt;em&gt;service&lt;/em&gt; los agrupa de manera funcional y los expone para que sean accesibles &lt;em&gt;desde el exterior&lt;/em&gt; del clúster.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Crear un cluster de un solo nodo</title>
      <link>https://onthedock.github.io/post/170702-crear-un-cluster-de-un-solo-nodo/</link>
      <pubDate>Sun, 02 Jul 2017 23:14:22 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170702-crear-un-cluster-de-un-solo-nodo/</guid>
      <description>&lt;p&gt;Para tener un clúster de desarrollo con la versatilidad de poder hacer y deshacer cambios (usando los &lt;em&gt;snapshots&lt;/em&gt; de una máquina virtual), lo más sencillo es disponer de un clúster de Kubernetes de un solo nodo.&lt;/p&gt;

&lt;p&gt;
Por defecto, el nodo master de un clúster de Kubernetes no ejecuta ningún tipo de carga de trabajo relacionada con los pods desplegados en el clúster, centrándose en las tareas de gestión de los &lt;em&gt;pods&lt;/em&gt; y del propio clúster.&lt;/p&gt;

&lt;p&gt;Para permitir que el nodo master pueda ejecutar &lt;em&gt;pods&lt;/em&gt;, debemos modificar las opciones por defecto de Kubernetes.&lt;/p&gt;

&lt;p&gt;En primer lugar, comprobamos que todos los pods del espacio de nombres de sistema han arrancado y se ejecutan correctamente:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get nodes --all-namespaces
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para que el nodo master admita el despliegue de &lt;em&gt;pods&lt;/em&gt;, modificamos mediante:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl taint nodes --all node-role.kubernetes.io/master-
node &amp;quot;k8s-snc&amp;quot; untainted
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>IP en mensaje prelogin</title>
      <link>https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/</link>
      <pubDate>Sun, 02 Jul 2017 22:07:18 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/</guid>
      <description>&lt;p&gt;En la pantalla de &lt;em&gt;login&lt;/em&gt; en modo consola de los sistemas Linux se muestra un mensaje de bienvenida.&lt;/p&gt;

&lt;p&gt;En este artículo se muestra cómo hacer que se muestre la IP del equipo.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Cuando tenemos una máquina (virtual) configurada para obtener la IP de forma dinámica mediante DHCP, al desconocer por adelantado la IP que se le ha asignado, es necesario conectarse al hipervisor, hacer login en la máquina virtual para, finalmente, obtener la IP &lt;em&gt;actual&lt;/em&gt; de la VM.&lt;/p&gt;

&lt;p&gt;Entonces podemos conectarnos &lt;em&gt;remotamente&lt;/em&gt; usando PuTTY (desde Windows) o un emulador de terminal desde Linux/OSX.&lt;/p&gt;

&lt;p&gt;Sin embargo, hay una manera de agilizar este proceso, aprovechando el mensaje que se muestra antes del login.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Aunque en este caso he usado Alpine Linux, las instrucciones son igualmente válidas para la mayoría de distribuciones.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En primer lugar necesitamos ejecutar un &lt;em&gt;script&lt;/em&gt; durante el arranque del sistema operativo. En el caso concreto de Alpine Linux, he encontrado la solución en &lt;a href=&#34;https://forum.alpinelinux.org/forum/general-discussion/run-script-boot&#34;&gt;run script on boot&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Ya que no vamos a escribir nuestro propio servicio, usaremos el servicio &lt;em&gt;local&lt;/em&gt;. Para ello, hay que añadir nuestro &lt;em&gt;script&lt;/em&gt; en la carpeta &lt;code&gt;/etc/local.d/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;rc-update add local default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El &lt;code&gt;README&lt;/code&gt; ubicado en &lt;code&gt;/etc/local.d/README&lt;/code&gt; indica que cualquier fichero ejecutable con extensión &lt;code&gt;.start&lt;/code&gt; se lanza al arrancar el servicio, mientras que si la extensión es &lt;code&gt;.stop&lt;/code&gt;, se ejecuta al parar el servicio.&lt;/p&gt;

&lt;p&gt;En mi caso, he usado el &lt;em&gt;script&lt;/em&gt; para obtener la IP de la máquina como se indica en &lt;a href=&#34;http://offbytwo.com/2008/05/09/show-ip-address-of-vm-as-console-pre-login-message.html&#34;&gt;Show IP address of VM as console pre-login message&lt;/a&gt; y la he escrito en el fichero &lt;code&gt;/etc/issue&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/sbin/ifconfig | grep &amp;quot;inet addr&amp;quot; | grep -v &amp;quot;127.0.0.1&amp;quot; | awk &#39;{ print $2 }&#39; | awk -F: &#39;{ print $2 }&#39; &amp;gt; /etc/issue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de convertir el &lt;em&gt;script&lt;/em&gt; en ejecutable, he reinciado la máquina para probar que todo funcionaba como esperaba.&lt;/p&gt;

&lt;p&gt;Tras los mensajes de arranque, se muestra:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.1.208
alpine login:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Así no hace falta hacer &lt;em&gt;login&lt;/em&gt; en la máquina para obtener la IP.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instalación de Alpine linux</title>
      <link>https://onthedock.github.io/post/170604-instalacion-de-alpine-linux/</link>
      <pubDate>Sun, 04 Jun 2017 18:26:48 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170604-instalacion-de-alpine-linux/</guid>
      <description>&lt;p&gt;Alpine Linux se ha convertido en la distribución por defecto con la que construir contenedores.&lt;/p&gt;

&lt;p&gt;Alpine tiene sus propias particularidades, ya que no deriva de otra distribución, de manera que he pensado que sería una buena idea tener una máquina virtual con la que entrenarme.&lt;/p&gt;

&lt;p&gt;En este artículo explico qué diferencias he encontrado en Alpine.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;descargando-alpine-linux&#34;&gt;Descargando Alpine Linux&lt;/h2&gt;

&lt;p&gt;La primera diferencia respecto al resto de distribuciones es el tamaño de la ISO de instalación. En la &lt;a href=&#34;https://alpinelinux.org/downloads/&#34;&gt;página de descarga&lt;/a&gt; de Alpine Linux, tienes varias versiones para descargar. Además de las habituales, en función de la arquitectura (x86, x86-64, Raspberry Pi, Generic ARM), tienes disponibles versiones orientadas a entornos virtuales, para Xen, etc.&lt;/p&gt;

&lt;p&gt;En mi caso he descargado la versión &lt;code&gt;Virtual&lt;/code&gt;, orientada a sistemas virtuales y la imagen de instalación ocupa 35MB!.&lt;/p&gt;

&lt;h2 id=&#34;máquina-virtual&#34;&gt;Máquina virtual&lt;/h2&gt;

&lt;p&gt;He creado una máquina virtual y he conectado la ISO.&lt;/p&gt;

&lt;p&gt;Al arrancar la máquina, el sistema arranca en modo &lt;em&gt;live-CD&lt;/em&gt;, ejecutándose completamente en memoria.&lt;/p&gt;

&lt;p&gt;Para acceder al sistema, teclea &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mi primera sorpresa ha sido que no se ha solicitado la contraseña.&lt;/p&gt;

&lt;p&gt;Una vez dentro, ara configurar el sistema, lanza la utilidad &lt;code&gt;setup-alpine&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Welcome to Alpine!

The Alpine Wiki contains a large amount of how-to guides and general
information about administrating Alpine systems.
See &amp;lt;http://wiki.alpinelinux.org&amp;gt;.

You can setup the system with the command: setup-alpine

You may change this message by editing /etc/motd.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El teclado, por defecto, está en inglés, por lo que el &lt;code&gt;-&lt;/code&gt; se encuentra bajo la tecla &lt;code&gt;?&lt;/code&gt; en un teclado en castellano.&lt;/p&gt;

&lt;p&gt;El script de instalación pasa por los diferentes pasos de configuración:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;teclado: &lt;code&gt;es&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;variacion de teclado: &lt;code&gt;es-cat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;nombre del equipo: &lt;code&gt;alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;inicializar interfaz: &lt;code&gt;eth0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;configuración de IP: &lt;code&gt;dhcp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;¿quieres realizar alguna configuración de red manual?: &lt;code&gt;no&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;establecer el password del &lt;code&gt;root&lt;/code&gt;:&lt;/li&gt;
&lt;li&gt;zona horaria: &lt;code&gt;CET&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Proxy: &lt;code&gt;none&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Elección del &lt;em&gt;mirror&lt;/em&gt;: &lt;code&gt;f&lt;/code&gt; (se selecciona el más rápido)&lt;/li&gt;
&lt;li&gt;instalación de servidor de SSH: &lt;code&gt;openssh&lt;/code&gt; (opción por defecto)&lt;/li&gt;
&lt;li&gt;cliente NTP: &lt;code&gt;chrony&lt;/code&gt; (opción por defecto)&lt;/li&gt;
&lt;li&gt;selección de disco: &lt;code&gt;sda&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;uso del disco: &lt;code&gt;sys&lt;/code&gt; (selecciona &lt;code&gt;?&lt;/code&gt; para ver las diferencias entre las opciones presentadas)&lt;/li&gt;
&lt;li&gt;confirmar el borrado del disco: &lt;code&gt;y&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Y ¡ya está! Sólo queda reiniciar.&lt;/p&gt;

&lt;p&gt;En mi caso, he escrito &lt;code&gt;reboot&lt;/code&gt; y el sistema se ha reiniciado al cabo de unos pocos segundos.&lt;/p&gt;

&lt;p&gt;Al hacer login de nuevo, me ha sorprendido que no se me haya solicitado el password y que se haya perdido la configuración introducida :(&lt;/p&gt;

&lt;p&gt;Alpine Linux es una distribución tan ligera -y en la máquina de laboratorio tengo un SSD- que me ha costado un momento darme cuenta de que, al reiniciar, la máquina virtual no ha perdido la configuración, sino que ha arrancado de nuevo la versión del &lt;em&gt;live-CD&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Una vez expulsada la ISO, he reiniciado de nuevo y he accedido al sistema ya instalado en la VM ;)&lt;/p&gt;

&lt;h2 id=&#34;acceso-remoto-vía-ssh&#34;&gt;Acceso remoto vía SSH&lt;/h2&gt;

&lt;p&gt;Por comodidad, prefiero trabajar desde la consola del Mac, pero no quiero crear un nuevo usuario.&lt;/p&gt;

&lt;p&gt;Por defecto, OpenSSH no permite la conexión remota del usuario &lt;code&gt;root&lt;/code&gt;, así que el siguiente paso es modificar el fichero de configuración.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# vi /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Desplázate hasta la línea &lt;code&gt;PermitRootLogin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pulsa &lt;code&gt;i&lt;/code&gt; para entrar en el modo interactivo de Vi&lt;/li&gt;
&lt;li&gt;Escribe en una nueva línea: &lt;code&gt;PermitRootLogin yes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pulsa &lt;code&gt;ESC&lt;/code&gt; para volver al modo de comandos&lt;/li&gt;
&lt;li&gt;Escribe &lt;code&gt;:wq&lt;/code&gt; (&lt;em&gt;write&lt;/em&gt;, &lt;em&gt;quit&lt;/em&gt;) para guardar los cambios y salir de Vi.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para que los cambios tengan efecto, reinicia el servicio SSH:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# service sshd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;siguientes-pasos&#34;&gt;Siguientes pasos&lt;/h2&gt;

&lt;p&gt;A continuación realizaré la instalación de algunos paquetes.&lt;/p&gt;

&lt;p&gt;El objetivo es probar el proceso que se realiza durante la creación de una imagen en Docker, pero en un entorno donde poder observar la salida de los comandos ejecutados, etc.&lt;/p&gt;

&lt;h2 id=&#34;resumen&#34;&gt;Resumen&lt;/h2&gt;

&lt;p&gt;En este artículo hemos instalado Alpine Linux en una máquina virtual.&lt;/p&gt;

&lt;p&gt;También hemos modificado el servidor SSH para poder conectar remotamente como &lt;code&gt;root&lt;/code&gt; (por comodidad, en un entorno seguro de laboratorio).&lt;/p&gt;

&lt;p&gt;En los próximos artículos seguiremos familiarizándonos con Alpine Linux.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Protege el acceso remoto via API a Docker</title>
      <link>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sun, 07 May 2017 18:33:16 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; explicaba cómo configurar el acceso remoto al API de Docker. El problema es que de esta forma no hay manera de restringir el acceso.&lt;/p&gt;

&lt;p&gt;En este artículo protegemos el acceso usando TLS de manera que sólo se permitan conexiones que presenten un certificado firmado por una CA de confianza.
&lt;/p&gt;

&lt;p&gt;Seguiremos las instrucciones oficiales de Docker &lt;a href=&#34;https://docs.docker.com/engine/security/https/&#34;&gt;Protect the Docker daemon socket&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;creamos-una-ca-claves-para-el-cliente-y-el-servidor-con-openssl&#34;&gt;Creamos una CA, claves para el cliente y el servidor con OpenSSL&lt;/h2&gt;

&lt;p&gt;Primero, en la máquina &lt;em&gt;host&lt;/em&gt; del Docker &lt;em&gt;daemon&lt;/em&gt;, generamos las claves públicas y privadas de la CA (&lt;em&gt;Certification Authority&lt;/em&gt;, la entidad certificadora):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -aes256 -out ca-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................++
..........................++
e is 65537 (0x10001)
Enter pass phrase for ca-key.pem:
Verifying - Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Y a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem
Enter pass phrase for ca-key.pem:
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &#39;.&#39;, the field will be left blank.
-----
Country Name (2 letter code) [AU]:ES
State or Province Name (full name) [Some-State]:Barcelona
Locality Name (eg, city) []:Barcelona
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Ameisin
Organizational Unit Name (eg, section) []:DevOps
Common Name (e.g. server FQDN or YOUR name) []:192.168.1.20
Email Address []: {REDACTED}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora que tenemos una CA, podemos crear la clave para el servidor y la petición de firmado del certificado (&lt;em&gt;certificate signing request&lt;/em&gt;, CSR). Por favor, verifica que &lt;code&gt;Common Name&lt;/code&gt; (es decir, el &lt;em&gt;FQDN&lt;/em&gt; o &lt;em&gt;YOUR Name&lt;/em&gt;) coincide con el nombre del &lt;em&gt;host&lt;/em&gt; que vas a usar para conectar a Docker.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out server-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................................................................................................++
...............................................................++
e is 65537 (0x10001)
# openssl req -subj &amp;quot;/CN=192.168.1.20&amp;quot; -sha256 -new -key server-key.pem -out server.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación vamos a firmar la clave pública con nuestra CA.&lt;/p&gt;

&lt;p&gt;Como las conexiones TLS pueden realizarse usando la dirección IP o un nombre DNS, deben especificarse durante la creación del certificado.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo subjectAltName = IP:192.168.1.20,IP:127.0.0.1 &amp;gt; extfile.cnf
# openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out server-cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=192.168.1.20
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;autenticación-del-cliente&#34;&gt;Autenticación del cliente&lt;/h2&gt;

&lt;p&gt;Para autenticar al cliente, crearemos una clave de cliente y una petición de firmado del certificado.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Para simplificar, los siguientes dos pasos pueden realizarse desde la máquina donde se encuentra el Docker &lt;em&gt;daemon&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out key.pem 4096
Generating RSA private key, 4096 bit long modulus
...............................................................................++
................................................................................................................................................................................++
e is 65537 (0x10001)
# openssl req -subj &#39;/CN=client&#39; -new -key key.pem -out client.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para que la clave permita autenticar al cliente, creamos un fichero de configuración de extensiones:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo extendedKeyUsage = clientAuth &amp;gt; extfile.cnf
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora firmamos la clave:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=client
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de haber generado &lt;code&gt;cert.pem&lt;/code&gt; y &lt;code&gt;server-cert.pem&lt;/code&gt; podemos eliminar las peticiones de firmado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ls
ca-key.pem  ca.srl    client.csr  extfile.cnf  server-cert.pem	server-key.pem
ca.pem	    cert.pem  key.pem     server.csr
# rm -v client.csr server.csr
removed ‘client.csr’
removed ‘server.csr’
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;protección-de-las-claves&#34;&gt;Protección de las claves&lt;/h2&gt;

&lt;p&gt;Con una máscara &lt;code&gt;umask&lt;/code&gt; por defecto de &lt;code&gt;022&lt;/code&gt; las claves secretas que hemos generado dan a todo el mundo acceso de lectura y de escritura a tu usuario y tu grupo.&lt;/p&gt;

&lt;p&gt;Para proteger las claves de daños accidentales, vamos a eliminar los permisos de escritura sobre ellas. Para hacerlas de sólo lectura para tu usuario, usamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0400 ca-key.pem key.pem server-key.pem
mode of ‘ca-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘server-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Los certificados pueden ser leídos por todo el mundo, pero para evitar daños accidentales, mejor eliminamos los permisos de escritura:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0444 ca.pem server-cert.pem cert.pem
mode of ‘ca.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘server-cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configurando-el-api-de-acceso-remoto-de-forma-segura&#34;&gt;Configurando el API de acceso remoto de forma segura&lt;/h2&gt;

&lt;p&gt;Para hacer que el Docker &lt;em&gt;daemon&lt;/em&gt; sólo acepte conexiones de clientes que proporcionen un certificado de confianza de tu CA.&lt;/p&gt;

&lt;p&gt;Para ello, modificamos las opciones de arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H=0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De manera que quede como (lo he dividido en varias líneas por claridad):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ExecStart=/usr/bin/dockerd --tlsverify 		\
         --tlscacert=/root/ca.pem 		\
         --tlscert=/root/server-cert.pem 	\
         --tlskey=/root/server-key.pem 		\
         -H=0.0.0.0:2376 			\
         -H fd://
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación, recargamos la configuración y reinciamos el servicio:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Los primeros intentos de arrancar el &lt;em&gt;daemon&lt;/em&gt; han fallado; ha sido necesario especificar la ruta completa a los certificados y las claves para conseguir que el servicio arrancara.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finalmente, comprobamos que podemos acceder usando el certificado con &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# curl https://192.168.1.20:2376/version --cert /root/cert.pem --key /root/key.pem --cacert /root/ca.pem
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A diferencia de lo que pasaba antes, cuando se intenta acceder a &lt;code&gt;https://192.168.1.9:2376/version&lt;/code&gt; desde otro equipo (sin usar el certificado), obtenemos un error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-url&#34;&gt;This site can’t be reached
192.168.1.9 refused to connect.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Configura un endpoint remoto en Portainer</title>
      <link>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</link>
      <pubDate>Sat, 06 May 2017 17:38:20 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/&#34;&gt;Portainer para gestionar tus contenedores en Docker&lt;/a&gt; usamos &lt;strong&gt;Portainer&lt;/strong&gt; para gestionar el Docker Engine local.&lt;/p&gt;

&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; habilitamos el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;En este artículo configuramos &lt;strong&gt;Portainer&lt;/strong&gt; para conectar con un &lt;em&gt;endpoint&lt;/em&gt; remoto (el API expuesta de un Docker Engine).
&lt;/p&gt;

&lt;p&gt;Accede a &lt;strong&gt;Portainer&lt;/strong&gt; y selecciona &lt;em&gt;Endpoints&lt;/em&gt; en el panel izquierdo.&lt;/p&gt;

&lt;p&gt;Para configurar el &lt;em&gt;endopoint&lt;/em&gt; remoto (no seguro) sólo necesitas proporcionar un nombre para el &lt;em&gt;endpoint&lt;/em&gt; y la URL de acceso:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/1-configure-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/1-configure-endpoint.png&#34; width=935 height=660 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Configura un nuevo endpoint
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;Para identificar qué Docker Engine estoy viendo en cada momento, indico la IP de la máquina, seguido de la plataforma y el &lt;em&gt;host&lt;/em&gt; en el que se encuentra.&lt;/p&gt;

&lt;p&gt;Para cambiar entre los diferentes &lt;em&gt;endpoints&lt;/em&gt; definidos en &lt;strong&gt;Portainer&lt;/strong&gt;, selecciona el que quieres gestionar en el desplegable de la parte superior del panel lateral:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/2-change-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/2-change-endpoint.png&#34; width=450 height=168 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Cambia entre los diferentes endpoints definidos
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Habilita el acceso remoto vía API a Docker</title>
      <link>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sat, 06 May 2017 15:23:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;Portainer permite gestionar &lt;em&gt;endpoints&lt;/em&gt; remotos para Docker (y Docker Swarm) mediante el API REST de Docker Engine. El problema es que el API está desactivado por defecto.&lt;/p&gt;

&lt;p&gt;A continuación indico cómo activar y verificar el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Buscando en Google cómo habilitar el API remoto de Docker Engine probablemente encuentres el artículo
&lt;a href=&#34;https://www.ivankrizsan.se/2016/05/18/enabling-docker-remote-api-on-ubuntu-16-04/&#34;&gt;Enabling Docker Remote API on Ubuntu 16.04&lt;/a&gt;. Como bien dice en el párrafo inicial, no es fácil encontrar unas instrucciones claras sobre cómo configurar el API de principio a fin.&lt;/p&gt;

&lt;p&gt;Lanzando &lt;code&gt;docker man&lt;/code&gt;, vemos que la opción que buscamos es:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-H, --host=[unix:///var/run/docker.sock]: tcp://[host]:[port][path] to bind or
       unix://[/path/to/socket] to use.
         The socket(s) to bind to in daemon mode specified using one or more
         tcp://host:port/path, unix:///path/to/socket, fd://* or fd://socketfd.
         If the tcp port is not specified, then it will default to either 2375 when
         --tls is off, or 2376 when --tls is on, or --tlsverify is specified.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta opción debe pasarse en el arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker. Para configurar esta opción durante el arranque de Docker Engine tenemos dos opciones:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;modificar el arranque del &lt;em&gt;daemon&lt;/em&gt; modificando la configuración de &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;añadiendo las opciones en el fichero de configuración de Docker Engine. Para sistemas Linux con &lt;em&gt;systemd&lt;/em&gt;, la &lt;a href=&#34;https://docs.docker.com/engine/admin/systemd/#start-automatically-at-system-boot&#34;&gt;configuración del &lt;em&gt;daemon&lt;/em&gt; de Docker&lt;/a&gt; se realiza a través del fichero &lt;code&gt;daemon.json&lt;/code&gt; ubicado en &lt;code&gt;/etc/docker/&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;He intentado configurar Docker Engine mediante el segundo método &lt;em&gt;daemon.json&lt;/em&gt; pero no he sido capaz de activar el API.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Primero, hacemos una copia de seguridad del fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# cp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.original
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editamos el fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target docker.socket firewalld.service
Wants=network-online.target
Requires=docker.socket

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H fd:// 
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=1048576
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process
# restart the docker process if it exits prematurely
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea &lt;code&gt;ExecStart=/usr/bin/dockerd -H fd://&lt;/code&gt; y añadimos: &lt;code&gt;-H tcp://0.0.0.0:2375&lt;/code&gt; de manera que quede:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Esto hace que &lt;em&gt;dockerd&lt;/em&gt; escuche en todas las interfaces disponibles. En el caso de la máquina virtual en la que estoy probando, sólo tengo una, pero lo correcto sería especificar la dirección IP donde quieres que escuche &lt;em&gt;dockerd&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Guardamos los cambios.&lt;/p&gt;

&lt;p&gt;Recargamos la configuración y reiniciamos el servicio.&lt;/p&gt;

&lt;p&gt;Para comprobar que hemos el API funciona, lanzamos una consulta usando &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
# curl http://localhost:2375/version
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Debes tener en cuenta que esta configuración &lt;strong&gt;supone un riesgo de seguridad&lt;/strong&gt; al permitir el acceso al API de Docker Engine sin ningún tipo de control.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Instala Weave Net en Kubernetes 1.6</title>
      <link>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</link>
      <pubDate>Fri, 05 May 2017 22:14:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</guid>
      <description>&lt;p&gt;Una de las cosas que más me sorprenden de Kubernetes es que es necesario instalar una &lt;em&gt;capa de red&lt;/em&gt; sobre el clúster.&lt;/p&gt;

&lt;p&gt;En el caso concreto del que he obtenido las &lt;em&gt;capturas de pantalla&lt;/em&gt;, el clúster corre sobre máquinas virtuales con Debian Jessie.
&lt;/p&gt;

&lt;p&gt;La instalación de Weave Net en Kubernetes consiste únicamente en una línea, como explica el artículo: &lt;a href=&#34;https://www.weave.works/weave-net-kubernetes-integration/&#34;&gt;Run Weave Net with Kubernetes in Just One Line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Antes de instalar la &lt;em&gt;red&lt;/em&gt; en el clúster (de momento, de un solo nodo), &lt;em&gt;kubectl&lt;/em&gt; indica que el estado del nodo es &lt;code&gt;NotReady&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k8s       NotReady   5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la salida del comando tenemos que la versión de Kubernetes es la 1.6.1. Este dato será importante más adelante a la hora de escoger el comando de instalación de Weave Net.&lt;/p&gt;

&lt;p&gt;Si obtenemos la lista de &lt;em&gt;pods&lt;/em&gt;, comprobamos que no tenemos ningún &lt;em&gt;pod de red&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   0          5h
kube-system   kube-apiserver-k8s            1/1       Running   0          5h
kube-system   kube-controller-manager-k8s   1/1       Running   0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending   0          5h
kube-system   kube-proxy-l02zn              1/1       Running   0          5h
kube-system   kube-scheduler-k8s            1/1       Running   0          5h
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, los &lt;em&gt;pods&lt;/em&gt; de &lt;em&gt;DNS&lt;/em&gt; &lt;code&gt;kube-dns-*&lt;/code&gt; están en estado &lt;code&gt;Pending&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Siguiendo las instrucciones del artículo de Weave Net, lanzamos el comando de instalación para versiones 1.6 (o superior):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl apply -f https://git.io/weave-kube-1.6
clusterrole &amp;quot;weave-net&amp;quot; created
serviceaccount &amp;quot;weave-net&amp;quot; created
clusterrolebinding &amp;quot;weave-net&amp;quot; created
daemonset &amp;quot;weave-net&amp;quot; created
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obtenemos la lista de &lt;em&gt;pods&lt;/em&gt; de nuevo y observamos que se están creando dos nuevos contenedores:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;operador@k8s:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             0          5h
kube-system   kube-apiserver-k8s            1/1       Running             0          5h
kube-system   kube-controller-manager-k8s   1/1       Running             0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending             0          5h
kube-system   kube-proxy-l02zn              1/1       Running             0          5h
kube-system   kube-scheduler-k8s            1/1       Running             0          5h
kube-system   weave-net-32ptg               0/2       ContainerCreating   0          12s
operador@k8s:~$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De hecho, se creado el &lt;em&gt;daemonset&lt;/em&gt; &amp;ldquo;weave-net&amp;rdquo;. Un &lt;em&gt;daemonset&lt;/em&gt; es un &lt;em&gt;pod&lt;/em&gt; que se crea en cada uno de los nodos del clúster automáticamente. Kubernetes se encarga de descargar la imagen desde DockerHub y arrancar un contenedor. Los nodos en la red creada por Weave Net forman una red &lt;em&gt;mesh&lt;/em&gt; que se configura automáticamente, de manera que es posible agregar nodos adicionales sin necesidad de cambiar ninguna configuración.&lt;/p&gt;

&lt;p&gt;Pasados unos segundos la creación de los nodos se ha completado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$  kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS         RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running        0          5h
kube-system   kube-apiserver-k8s            1/1       Running        0          5h
kube-system   kube-controller-manager-k8s   1/1       Running        0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       ErrImagePull   0          5h
kube-system   kube-proxy-l02zn              1/1       Running        0          5h
kube-system   kube-scheduler-k8s            1/1       Running        0          5h
kube-system   weave-net-32ptg               2/2       Running        0          1m
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalmente, verificamos que el primer nodo del clúster ya es operativo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k8s       Ready     5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, una vez que tenemos la red instalada en el clúster, el &lt;em&gt;pod&lt;/em&gt; &lt;code&gt;kube-dns&lt;/code&gt; comienza la creación de los contenedores (quizás tengas que reiniciar):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             1          11d
kube-system   kube-apiserver-k8s            1/1       Running             1          11d
kube-system   kube-controller-manager-k8s   1/1       Running             1          11d
kube-system   kube-dns-3913472980-4nlg9     0/3       ContainerCreating   0          11d
kube-system   kube-proxy-l02zn              1/1       Running             1          11d
kube-system   kube-scheduler-k8s            1/1       Running             1          11d
kube-system   weave-net-32ptg               2/2       Running             3          10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tras unos segundos, tenemos todos los &lt;em&gt;pods&lt;/em&gt; del clúster funcionales:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   1          11d
kube-system   kube-apiserver-k8s            1/1       Running   1          11d
kube-system   kube-controller-manager-k8s   1/1       Running   1          11d
kube-system   kube-dns-3913472980-4nlg9     3/3       Running   0          11d
kube-system   kube-proxy-l02zn              1/1       Running   1          11d
kube-system   kube-scheduler-k8s            1/1       Running   1          11d
kube-system   weave-net-32ptg               2/2       Running   3          10d
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo tenemos que añadir nodos &lt;em&gt;worker&lt;/em&gt; y hacer crecer el clúster.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Solución al error de instalación de Kubernetes en Debian Jessie (Missing cgroups: memory)</title>
      <link>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</link>
      <pubDate>Sat, 22 Apr 2017 07:57:14 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</guid>
      <description>&lt;p&gt;Al lanzar la inicialización del clúster con &lt;code&gt;kubeadm init&lt;/code&gt; en Debian Jessie, las comprobaciones inciales indican que no se encuentran los &lt;em&gt;cgroups&lt;/em&gt; para la memoria (échale un vistazo al artículo &lt;a href=&#34;https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/&#34;&gt;La instalación de Kubernetes falla en Debian Jessie (missing cgroups: memory)&lt;/a&gt;). Los &lt;em&gt;cgroups&lt;/em&gt; son una de las piezas fundamentales en las que se basa Docker para &lt;em&gt;aislar&lt;/em&gt; los procesos de los contenedores, por lo que la inicialización del clúster de Kubernetes se detiene.&lt;/p&gt;

&lt;p&gt;La solución es tan sencilla como habilitar los &lt;em&gt;cgroups&lt;/em&gt; durante el arranque.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;En primer lugar, verificamos la versión del &lt;em&gt;kernel&lt;/em&gt; que tenemos instalada:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# uname -a
Linux k8s 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al lanzar &lt;code&gt;kubeadm init&lt;/code&gt; obtenemos el error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubeadm init
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
OS: Linux
KERNEL_VERSION: 3.16.0-4-amd64
CONFIG_NAMESPACES: enabled
CONFIG_NET_NS: enabled
CONFIG_PID_NS: enabled
CONFIG_IPC_NS: enabled
CONFIG_UTS_NS: enabled
CONFIG_CGROUPS: enabled
CONFIG_CGROUP_CPUACCT: enabled
CONFIG_CGROUP_DEVICE: enabled
CONFIG_CGROUP_FREEZER: enabled
CONFIG_CGROUP_SCHED: enabled
CONFIG_CPUSETS: enabled
CONFIG_MEMCG: enabled
CONFIG_INET: enabled
CONFIG_EXT4_FS: enabled (as module)
CONFIG_PROC_FS: enabled
CONFIG_NETFILTER_XT_TARGET_REDIRECT: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled (as module)
CONFIG_OVERLAYFS_FS: not set - Required for overlayfs.
CONFIG_AUFS_FS: enabled (as module)
CONFIG_BLK_DEV_DM: enabled (as module)
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: missing
DOCKER_VERSION: 17.04.0-ce
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[preflight] Some fatal errors occurred:
	missing cgroups: memory
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;solución&#34;&gt;Solución&lt;/h2&gt;

&lt;p&gt;La solución la he encontrado en &lt;a href=&#34;https://phabricator.wikimedia.org/T122734&#34;&gt;Enable memory cgroups for default Jessie image&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Nos convertimos en &lt;em&gt;root&lt;/em&gt;: &lt;code&gt;sudo su -&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Editamos el fichero &lt;code&gt;/etc/default/grup&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;En los parámetros para el arranque de linux (&lt;code&gt;GRUB_CMDLINE_LINUX&lt;/code&gt;) añadimos &lt;code&gt;cgroup_enable=memory&lt;/code&gt;. En mi caso, la línea queda: &lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet cgroup_enable=memory&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Actualizamos &lt;em&gt;grub&lt;/em&gt;: &lt;code&gt;update-grub2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Reiniciamos la máquina: &lt;code&gt;reboot&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[preflight] Some fatal errors occurred:
	missing cgroups: memory
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
root@k8s:~# nano /etc/default/grub
root@k8s:~# update-grub2
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.16.0-4-amd64
Found initrd image: /boot/initrd.img-3.16.0-4-amd64
done
root@k8s:~# reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al lanzar &lt;code&gt;kubeadm init&lt;/code&gt; de nuevo, el clúster arranca con normalidad:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;root@k8s:~# kubeadm init
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] API Server serving cert is signed for DNS names [k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.99]
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key and public key.
[certificates] Generated front-proxy CA certificate and key.
[certificates] Generated front-proxy client certificate and key.
[certificates] Valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/scheduler.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/controller-manager.conf&amp;quot;
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 39.363656 seconds
[apiclient] Waiting for at least one node to register
[apiclient] First node has registered after 1.518215 seconds
[token] Using token: fe9e91.7142118e712eb019
[apiconfig] Created RBAC rules
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  sudo cp /etc/kubernetes/admin.conf $HOME/
  sudo chown $(id -u):$(id -g) $HOME/admin.conf
  export KUBECONFIG=$HOME/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token fe9e91.7142118e712eb019 192.168.1.99:6443

root@k8s:~#
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Configura curl para usar un proxy</title>
      <link>https://onthedock.github.io/post/170111-configura-curl-para-usar-proxy/</link>
      <pubDate>Wed, 11 Jan 2017 08:22:56 +0100</pubDate>
      
      <guid>https://onthedock.github.io/post/170111-configura-curl-para-usar-proxy/</guid>
      <description>&lt;p&gt;Cómo configurar &lt;code&gt;curl&lt;/code&gt; para salir a internet a través de un &lt;em&gt;proxy&lt;/em&gt; que requiere autenticación.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Como la VM está detrás de un &lt;em&gt;proxy&lt;/em&gt;, primero tienes que indicar a &lt;code&gt;curl&lt;/code&gt; la dirección del mismo. La manera más sencilla de solucionar el problema de una vez por todas es indicar la URL del &lt;em&gt;proxy&lt;/em&gt; en el fichero &lt;code&gt;.curlrc&lt;/code&gt;, en la carpeta &lt;em&gt;home&lt;/em&gt; del usuario.&lt;/p&gt;

&lt;p&gt;Si estás trabajando con el usuario &lt;code&gt;root&lt;/code&gt;, coloca el fichero en &lt;code&gt;/root/.curlrc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Edita el fichero y añade la dirección del &lt;em&gt;proxy&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;proxy = https://${USERNAME}:${PASSWORD}@proxy.ameisin.com:8080/proxy.pac
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Referencia: &lt;a href=&#34;http://stackoverflow.com/questions/7559103/how-to-setup-curl-to-permanently-use-a-proxy&#34;&gt;How to setup curl to permanently use a proxy? [closed]&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instala Docker en Ubuntu Server 16.04</title>
      <link>https://onthedock.github.io/post/170110-instala-docker-en-ubuntu-server-16.04/</link>
      <pubDate>Tue, 10 Jan 2017 15:12:46 +0100</pubDate>
      
      <guid>https://onthedock.github.io/post/170110-instala-docker-en-ubuntu-server-16.04/</guid>
      <description>&lt;p&gt;Cómo instalar Docker en Ubuntu Server 16.04.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Para instalar la última versión de Docker, usamos las instrucciones &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/como-instalar-y-usar-docker-en-ubuntu-16-04-es&#34;&gt;¿Cómo instalar y usar Docker en Ubuntu 16.04?&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;No funciona por algún motivo, probablemente por el &lt;em&gt;proxy&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Para instalar la clave GPG de Docker, el método que funciona es (&lt;small&gt;ref: &lt;a href=&#34;https://github.com/docker/docker/issues/17436#issuecomment-151870782&#34;&gt;Docker website encourages users to import GPG key for apt repository in unsafe ways #17436&lt;/a&gt;&lt;/small&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# curl -s  https://get.docker.com/gpg | apt-key add -
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Agregamos el repositorio de Docker a APT&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-add-repository &#39;deb https://apt.dockerproject.org/repo ubuntu-xenial main&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Actualizamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Una vez añadido, comprobamos mediante:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-cache policy docker-engine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalmente, instalamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install -y docker-engine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si ha habido problemas para validar la autenticidad del paquete de Docker, la instalación debe hacerse sin la aceptación automática (es decir, sin el parámetro &lt;code&gt;-y&lt;/code&gt;) o añadiendo &lt;code&gt;--allow-authenticate&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Verificamos que tenemos docker funcionando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# docker version
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Configura el proxy para APT en Ubuntu Server 16.04</title>
      <link>https://onthedock.github.io/post/170110-configura-apt-en-ubuntu-server-16.04/</link>
      <pubDate>Tue, 10 Jan 2017 15:01:55 +0100</pubDate>
      
      <guid>https://onthedock.github.io/post/170110-configura-apt-en-ubuntu-server-16.04/</guid>
      <description>&lt;p&gt;Cómo configurar &lt;code&gt;apt&lt;/code&gt; para salir a internet a través de un &lt;em&gt;proxy&lt;/em&gt; que requiere autenticación.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;La configuración del &lt;em&gt;proxy&lt;/em&gt; para &lt;code&gt;APT&lt;/code&gt; en Ubuntu Server 16.04 se realiza a través del fichero &lt;code&gt;/etc/apt/apt.conf&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Crea el fichero si no existe y escribe:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Acquire::http::Proxy &amp;quot;http://${USERNAME}:${PASSWORD}@proxy.ameisin.vwg:8080/amisin.pac&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación, ya puedes actualizar los repositorios usando &lt;code&gt;apt-get update&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>