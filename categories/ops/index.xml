<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ops on On The Dock</title>
    <link>https://onthedock.github.io/categories/ops/index.xml</link>
    <description>Recent content in Ops on On The Dock</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Handmade with &amp;#9829; by Xavi Aznar</copyright>
    <atom:link href="https://onthedock.github.io/categories/ops/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Troubleshooting Kubernetes (II)</title>
      <link>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</link>
      <pubDate>Sat, 06 May 2017 05:21:09 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</guid>
      <description>&lt;p&gt;Sigo con el &lt;em&gt;troubleshooting&lt;/em&gt; del &lt;em&gt;cuelgue&lt;/em&gt; de los nodos sobre Raspberry Pi 3 del clúster.&lt;/p&gt;

&lt;p&gt;Ayer estuve &lt;em&gt;haciendo limpieza&lt;/em&gt; siguiendo &lt;em&gt;vagamente&lt;/em&gt; la recomendación de &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;esta respuesta&lt;/a&gt; en el hilo &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;Kubernetes memory consumption explosion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;La solución de &lt;code&gt;RenaudWasTaken&lt;/code&gt; al problema de consumo excesivo de memoria (32GB) fue la realizar limpieza de las carpetas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/etcd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Antes de empezar a borrar &lt;em&gt;a lo loco&lt;/em&gt;, revisé el contenido de estas carpetas.&lt;/p&gt;

&lt;h2 id=&#34;var-run-kubernetes&#34;&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;En &lt;code&gt;/var/run/kubernetes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/run/kubernetes/ 
total 8
drwxr-xr-x  2 root root   80 May  5 18:43 .
drwxr-xr-x 18 root root  600 May  5 18:43 ..
-rw-r--r--  1 root root 1082 May  5 18:43 kubelet.crt
-rw-------  1 root root 1679 May  5 18:43 kubelet.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos ficheros son certificados, por lo que no parecen implicados en el problema y decido no borrarlos.&lt;/p&gt;

&lt;h2 id=&#34;var-lib-kubelet&#34;&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/h2&gt;

&lt;h3 id=&#34;nodo-k2&#34;&gt;Nodo &lt;strong&gt;k2&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Al intentar listar el contenido de la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt;, la Raspberry Pi 3 ha tardado una eternidad (en los primeros intentos he creído que había dejado de responder).&lt;/p&gt;

&lt;p&gt;Finalmente, el resultado del comando ha mostrado una gran cantidad de carpetas dentro de esta carpeta:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods
...
drwx------    2 root root    4096 May  4 23:11 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~207819844.deleting~559419937.deleting~142152710.deleting~494766199.deleting~952339001
drwx------    2 root root    4096 May  5 18:02 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~274250693.deleting~987962315.deleting~680794233.deleting~917929467
drwx------    2 root root    4096 May  4 23:37 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~292131322.deleting~049606881.deleting~105942520.deleting~463246644
drwx------    2 root root    4096 May  4 22:46 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~119491600.deleting~962328406.deleting~220005477.deleting~309794961.deleting~392355244.deleting~378832104.deleting~159122214.deleting~324365539
drwx------    2 root root    4096 Apr 15 20:39 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~162549394.deleting~296869341.deleting~353223099.deleting~018715754.deleting~526835026.deleting~320404022.deleting~453576282.deleting~001809150
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parece como si algo no hubiera funcionado correctamente y hubiera entrado en un bucle, creando carpetas y más carpetas. Además, en el nombre de alguna de estas carpetas aparece &lt;code&gt;..._flannel-cfg...&lt;/code&gt;. Esta ha sido la pista que me ha convencido; al intentar instalar el &lt;em&gt;dashboard&lt;/em&gt; de Kubernetes, tuve problemas precisamente porque no tengo instalado Flannel. Eliminé el &lt;em&gt;pod&lt;/em&gt; y no le di más vueltas.&lt;/p&gt;

&lt;p&gt;Sin embargo, la existencia de estas carpetas parece indicar que la eliminación no fue tan limpia como pensaba y que &lt;em&gt;algo&lt;/em&gt; se quedó atrapado en un bucle.&lt;/p&gt;

&lt;p&gt;He lanzado &lt;code&gt;rm -rf /var/lib/kubelet/pods/&lt;/code&gt; y el comando ha fallado indicando que uno de los &lt;em&gt;pods&lt;/em&gt; estaba en uso. Así que he eliminado poco a poco los &lt;em&gt;pods&lt;/em&gt; hasta que al final:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls /var/lib/kubelet/pods/ -la
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos &lt;em&gt;pods&lt;/em&gt;, sean los que sean, están en uso (no tengo nada desplegado en el clúster, así que deben ser &lt;em&gt;de sistema&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Tras la limpieza, he reiniciado el nodo.&lt;/p&gt;

&lt;h3 id=&#34;nodo-k3&#34;&gt;Nodo &lt;strong&gt;k3&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;El nodo &lt;strong&gt;k3&lt;/strong&gt; no presentaba estas &lt;em&gt;carpetas sospechosas&lt;/em&gt;, pero también he realizado limpieza:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ rm -rf /var/lib/kubelet/pods/
rm: cannot remove ‘/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap’: Directory not empty
rm: cannot remove ‘/var/lib/kubelet/pods/71290201-31bb-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/kube-proxy-token-7zk2k’: Device or resource busy
rm: cannot remove ‘/var/lib/kubelet/pods/ef887c6a-31ba-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/weave-net-token-61scv’: Device or resource busy
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Igual que en el nodo &lt;strong&gt;k2&lt;/strong&gt;, he reiniciado.&lt;/p&gt;

&lt;h2 id=&#34;resultados&#34;&gt;Resultados&lt;/h2&gt;

&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; siguen en estado &lt;code&gt;Ready&lt;/code&gt; tras unas siete y ocho horas, que es bastante más de lo que &lt;em&gt;aguantaban&lt;/em&gt; antes.&lt;/p&gt;

&lt;p&gt;He comprobado que en la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt; sólo aparecen dos &lt;em&gt;pods&lt;/em&gt; (en el nodo &lt;strong&gt;k2&lt;/strong&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En el nodo &lt;strong&gt;k3&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 28
drwxr-x--- 5 root root 12288 May  5 19:44 .
drwxr-x--- 4 root root  4096 Apr 15 14:10 ..
drwxr-x--- 3 root root  4096 May  5 19:33 3a5e2819-21e5-11e7-bcfd-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 514d4c93-31c9-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 c0b9753a-31c9-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Más adelante actualizaré el artículo para verificar si los nodos siguen activos y sin problemas.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instala Weave Net en Kubernetes 1.6</title>
      <link>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</link>
      <pubDate>Fri, 05 May 2017 22:14:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</guid>
      <description>&lt;p&gt;Una de las cosas que más me sorprenden de Kubernetes es que es necesario instalar una &lt;em&gt;capa de red&lt;/em&gt; sobre el clúster.&lt;/p&gt;

&lt;p&gt;En el caso concreto del que he obtenido las &lt;em&gt;capturas de pantalla&lt;/em&gt;, el clúster corre sobre máquinas virtuales con Debian Jessie.
&lt;/p&gt;

&lt;p&gt;La instalación de Weave Net en Kubernetes consiste únicamente en una línea, como explica el artículo: &lt;a href=&#34;https://www.weave.works/weave-net-kubernetes-integration/&#34;&gt;Run Weave Net with Kubernetes in Just One Line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Antes de instalar la &lt;em&gt;red&lt;/em&gt; en el clúster (de momento, de un solo nodo), &lt;em&gt;kubectl&lt;/em&gt; indica que el estado del nodo es &lt;code&gt;NotReady&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k8s       NotReady   5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la salida del comando tenemos que la versión de Kubernetes es la 1.6.1. Este dato será importante más adelante a la hora de escoger el comando de instalación de Weave Net.&lt;/p&gt;

&lt;p&gt;Si obtenemos la lista de &lt;em&gt;pods&lt;/em&gt;, comprobamos que no tenemos ningún &lt;em&gt;pod de red&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   0          5h
kube-system   kube-apiserver-k8s            1/1       Running   0          5h
kube-system   kube-controller-manager-k8s   1/1       Running   0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending   0          5h
kube-system   kube-proxy-l02zn              1/1       Running   0          5h
kube-system   kube-scheduler-k8s            1/1       Running   0          5h
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, los &lt;em&gt;pods&lt;/em&gt; de &lt;em&gt;DNS&lt;/em&gt; &lt;code&gt;kube-dns-*&lt;/code&gt; están en estado &lt;code&gt;Pending&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Siguiendo las instrucciones del artículo de Weave Net, lanzamos el comando de instalación para versiones 1.6 (o superior):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl apply -f https://git.io/weave-kube-1.6
clusterrole &amp;quot;weave-net&amp;quot; created
serviceaccount &amp;quot;weave-net&amp;quot; created
clusterrolebinding &amp;quot;weave-net&amp;quot; created
daemonset &amp;quot;weave-net&amp;quot; created
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obtenemos la lista de &lt;em&gt;pods&lt;/em&gt; de nuevo y observamos que se están creando dos nuevos contenedores:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;operador@k8s:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             0          5h
kube-system   kube-apiserver-k8s            1/1       Running             0          5h
kube-system   kube-controller-manager-k8s   1/1       Running             0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending             0          5h
kube-system   kube-proxy-l02zn              1/1       Running             0          5h
kube-system   kube-scheduler-k8s            1/1       Running             0          5h
kube-system   weave-net-32ptg               0/2       ContainerCreating   0          12s
operador@k8s:~$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De hecho, se creado el &lt;em&gt;daemonset&lt;/em&gt; &amp;ldquo;weave-net&amp;rdquo;. Un &lt;em&gt;daemonset&lt;/em&gt; es un &lt;em&gt;pod&lt;/em&gt; que se crea en cada uno de los nodos del clúster automáticamente. Kubernetes se encarga de descargar la imagen desde DockerHub y arrancar un contenedor. Los nodos en la red creada por Weave Net forman una red &lt;em&gt;mesh&lt;/em&gt; que se configura automáticamente, de manera que es posible agregar nodos adicionales sin necesidad de cambiar ninguna configuración.&lt;/p&gt;

&lt;p&gt;Pasados unos segundos la creación de los nodos se ha completado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$  kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS         RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running        0          5h
kube-system   kube-apiserver-k8s            1/1       Running        0          5h
kube-system   kube-controller-manager-k8s   1/1       Running        0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       ErrImagePull   0          5h
kube-system   kube-proxy-l02zn              1/1       Running        0          5h
kube-system   kube-scheduler-k8s            1/1       Running        0          5h
kube-system   weave-net-32ptg               2/2       Running        0          1m
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalmente, verificamos que el primer nodo del clúster ya es operativo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k8s       Ready     5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, una vez que tenemos la red instalada en el clúster, el &lt;em&gt;pod&lt;/em&gt; &lt;code&gt;kube-dns&lt;/code&gt; comienza la creación de los contenedores (quizás tengas que reiniciar):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             1          11d
kube-system   kube-apiserver-k8s            1/1       Running             1          11d
kube-system   kube-controller-manager-k8s   1/1       Running             1          11d
kube-system   kube-dns-3913472980-4nlg9     0/3       ContainerCreating   0          11d
kube-system   kube-proxy-l02zn              1/1       Running             1          11d
kube-system   kube-scheduler-k8s            1/1       Running             1          11d
kube-system   weave-net-32ptg               2/2       Running             3          10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tras unos segundos, tenemos todos los &lt;em&gt;pods&lt;/em&gt; del clúster funcionales:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   1          11d
kube-system   kube-apiserver-k8s            1/1       Running   1          11d
kube-system   kube-controller-manager-k8s   1/1       Running   1          11d
kube-system   kube-dns-3913472980-4nlg9     3/3       Running   0          11d
kube-system   kube-proxy-l02zn              1/1       Running   1          11d
kube-system   kube-scheduler-k8s            1/1       Running   1          11d
kube-system   weave-net-32ptg               2/2       Running   3          10d
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo tenemos que añadir nodos &lt;em&gt;worker&lt;/em&gt; y hacer crecer el clúster.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Errores sobre Orphaned pods en syslog</title>
      <link>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</link>
      <pubDate>Sun, 30 Apr 2017 12:55:44 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</guid>
      <description>&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; del clúster dejan de responder pasadas unas horas. La única manera de solucionarlo es reiniciar los nodos. Siguiendo con la revisión de logs, he encontrado que se genera una gran cantidad de entradas en &lt;em&gt;syslog&lt;/em&gt; en referencia a &lt;em&gt;orphaned pods&lt;/em&gt;. Además, el número de estos errores no para de crecer &lt;strong&gt;rápidamente&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
118938
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119022
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119170
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Revisando las últimas entradas del log:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-log&#34;&gt;Apr 30 10:57:57 k2 kubelet[3619]: E0430 10:57:57.186318    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;5064b9d9-2c9e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:01 k2 kubelet[3619]: E0430 10:58:01.759595    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;6c601e9c-2c9c-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.226372    3619 kubelet.go:1549] Unable to mount volumes for pod &amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]; skipping pod
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.238315    3619 pod_workers.go:182] Error syncing pod 4461d51d-2d93-11e7-a7ae-b827eb650fdb (&amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;), skipping: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]
Apr 30 10:58:05 k2 kubelet[3619]: E0430 10:58:05.830432    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;bb4d3ea6-2b80-11e7-9388-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:08 k2 kubelet[3619]: E0430 10:58:08.435567    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;cb23be0d-2d7e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Todas estas entradas se encuentran en los logs del nodo &lt;strong&gt;k2&lt;/strong&gt;, donde no hay ningún &lt;em&gt;pod&lt;/em&gt; en ejecución (a parte de los propios de  Kubernetes que el clúster planifica en los diferentes nodos).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>El nodo k3 del clúster colgado de nuevo</title>
      <link>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</link>
      <pubDate>Sun, 30 Apr 2017 11:39:20 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/&#34;&gt;Múltiples mensajes &amp;lsquo;action 17 suspended&amp;rsquo; en los logs&lt;/a&gt; comentaba que estaba a la espera de obtener resultados; después de apenas unas horas, ya los tengo: &lt;strong&gt;k3&lt;/strong&gt; se ha vuelto a &lt;em&gt;colgar&lt;/em&gt; mientras que &lt;strong&gt;k2&lt;/strong&gt; no.&lt;/p&gt;

&lt;p&gt;Este resultado parece demostrar que la mala configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la causante de los &lt;em&gt;cuelgues&lt;/em&gt; de las RPi 3 en el clúster de Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: El nodo &lt;strong&gt;k2&lt;/strong&gt; sobre RPi3 sigue colgándose :(&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización II&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Parece solucionado&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;A modo de recordatorio, los cambios realizados en los dos nodos sobre Raspberry Pi 3 han sido (incluyo el nodo &lt;strong&gt;k1&lt;/strong&gt; con RPi2):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                                |  k1  |  k2  |  k3  |
                                | RPi2 | RPi3 | RPi3 |
 -------------------------------|------|------|------|
| Modificada conf. de rsyslog   |  No  |  Sí  |  No  |
| Actualización a versión 1.6.2 |  Sí  |  Sí  |  Sí  |
 ----------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hasta ahora, los únicos nodos que se &lt;em&gt;colgaban&lt;/em&gt; eran el &lt;strong&gt;k2&lt;/strong&gt; y el &lt;strong&gt;k3&lt;/strong&gt; (sobre RPi3).&lt;/p&gt;

&lt;p&gt;Al modificar la configuración en de &lt;em&gt;rsyslog&lt;/em&gt; en &lt;strong&gt;k2&lt;/strong&gt; y pasadas unas horas, el único nodo que se sigue colgando es el &lt;strong&gt;k3&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k1        Ready      19d       v1.6.2
k2        Ready      14d       v1.6.2
k3        NotReady   14d       v1.6.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es decir, el fallo a la hora de redirigir los mensajes a &lt;code&gt;/dev/xconsole&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sólo afectan a las RPi3&lt;/li&gt;
&lt;li&gt;provoca que el sistema se acabe colgando&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para solucionarlo, como &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Abre &lt;code&gt;/etc/rsyslog.conf&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Modifica las últimas líneas (al final del fichero) y coméntalas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;# The named pipe /dev/xconsole is for the `xconsole&#39; utility.  To use it,
# you must invoke `xconsole&#39; with the `-file&#39; option:
#
#    $ xconsole -file /dev/xconsole [...]
#
# NOTE: adjust the list below, or you&#39;ll go crazy if you have a reasonably
#      busy site..
#
daemon.*;mail.*;\
    news.err;\
    *.=debug;*.=info;\
    *.=notice;*.=warn       |/dev/xconsole
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;deben quedar como:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;#daemon.*;mail.*;\
#        news.err;\
#        *.=debug;*.=info;\
#        *.=notice;*.=warn       |/dev/xconsole
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podrías comentar la redirección &lt;code&gt;|/dev/xconsole&lt;/code&gt;, pero en este caso el bloque no tendría ninguna funcionalidad, por lo que creo que es &lt;em&gt;más limpio&lt;/em&gt; comentar todo el bloque.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reinicia el equipo mediante &lt;code&gt;reboot&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Múltiples mensajes &#39;action 17 suspended&#39; en los logs</title>
      <link>https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/</link>
      <pubDate>Sun, 30 Apr 2017 08:44:27 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/</guid>
      <description>&lt;p&gt;Investigando las causas por las que los dos nodos con Raspberry Pi 3 se &lt;em&gt;cuelgan&lt;/em&gt;, he encontrado múltiples apariciones de este mensaje en &lt;code&gt;/var/log/messages&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-log&#34;&gt;Apr 30 06:40:42 k3 rsyslogd-2007: action &#39;action 17&#39; suspended, next retry is Sun Apr 30 06:41:12 2017 [try http://www.rsyslog.com/e/2007 ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;De hecho, revisando el origen del problema he encontrado este comando que cuenta las apariciones del mensaje:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo grep &amp;quot;action.*suspend&amp;quot; /var/log/messages | wc -l
1394
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además de &lt;em&gt;spamear&lt;/em&gt; los logs, provoca un montón de escrituras innecesarias sobre la tarjeta microSD, lo que puede acortar la vida útil de la misma.&lt;/p&gt;

&lt;p&gt;No tengo claro si este puede ser la causa que hace que las dos Raspberry Pi 3 se cuelguen pasado un tiempo y que dejen de responder, lo que hace que deba reiniciarlas (desconectando/conectando el cable) para recuperarlas. Sin embargo, en el nodo &lt;em&gt;master&lt;/em&gt; (Raspberry Pi 2 B+) no aparece el mensaje en los logs y no se cuelga (aunque la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la misma).&lt;/p&gt;

&lt;h1 id=&#34;solución-a-los-mensajes-de-rsyslog&#34;&gt;Solución a los mensajes de rsyslog&lt;/h1&gt;

&lt;p&gt;El mensaje de error , es un problema de configuración de la aplicación &lt;code&gt;rsyslog&lt;/code&gt;, que intenta mostrar mensajes en &lt;code&gt;/dev/xconsole&lt;/code&gt;, pero falla.&lt;/p&gt;

&lt;p&gt;La solución la explica Danny Tuppeny en su blog, en &lt;a href=&#34;https://blog.dantup.com/2016/04/removing-rsyslog-spam-on-raspberry-pi-raspbian-jessie/&#34;&gt;Removing [action &amp;lsquo;action 17&amp;rsquo; suspended] rsyslog Spam on Raspberry Pi (Raspian Jessie)&lt;/a&gt;. Él mismo abrió un &lt;em&gt;bug&lt;/em&gt; en RPI-Distro: &lt;a href=&#34;https://github.com/RPi-Distro/repo/issues/28&#34;&gt;Default Raspbian Jessie Lite install spams syslog with &amp;ldquo;rsyslogd-2007: action &amp;lsquo;action 17&amp;rsquo; suspended, next retry is #28&lt;/a&gt; en él explicaba cómo había eliminado la línea que hace referencia a &lt;code&gt;/dev/xconsole&lt;/code&gt; de la configuración de &lt;em&gt;rasyslog&lt;/em&gt; y que el mensaje desaparecía (después de reiniciar).&lt;/p&gt;

&lt;p&gt;Para comprobar si esta es la causa del &lt;em&gt;cuelgue&lt;/em&gt; de la RPi 3, he modificado la configuración de &lt;em&gt;rsyslog&lt;/em&gt; en el nodo &lt;strong&gt;k2&lt;/strong&gt; del clúster, pero no en el &lt;strong&gt;k3&lt;/strong&gt;. De esta forma podré averiguar si la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la causante del &lt;em&gt;cuelgue&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;También he actualizado el sistema (en todos los nodos) y &lt;em&gt;kubelet&lt;/em&gt;, &lt;em&gt;kubectl&lt;/em&gt; y &lt;em&gt;kubeadm&lt;/em&gt; se han actualizado a la versión 1.6.2:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
Setting up libldap-2.4-2:armhf (2.4.40+dfsg-1+deb8u2) ...
Setting up libicu52:armhf (52.1-8+deb8u5) ...
Setting up kubelet (1.6.2-00) ...
Setting up kubectl (1.6.2-00) ...
Setting up kubeadm (1.6.2-00) ...
Processing triggers for libc-bin (2.19-18+deb8u7)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k1        Ready     19d       v1.6.2
k2        Ready     14d       v1.6.2
k3        Ready     14d       v1.6.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo queda esperar -normalmente unas cuantas horas- a ver qué pasa: hay tres posibilidades:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Se cuelga &lt;strong&gt;k3&lt;/strong&gt; pero no &lt;strong&gt;k2&lt;/strong&gt;: La configuración de &lt;em&gt;rsyslog&lt;/em&gt; era la causa.&lt;/li&gt;
&lt;li&gt;Se cuelga &lt;strong&gt;k2&lt;/strong&gt; pero no &lt;strong&gt;k3&lt;/strong&gt;: ¿?&lt;/li&gt;
&lt;li&gt;No se cuelga ni &lt;strong&gt;k2&lt;/strong&gt; ni &lt;strong&gt;k3&lt;/strong&gt;: Era un problema de alguno de los componetes actualizados que sólo afecta a la RPi 3.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Informaré en cuanto tenga resultados.&lt;/p&gt;

&lt;h1 id=&#34;unas-horas-después&#34;&gt;Unas horas después&amp;hellip;&lt;/h1&gt;

&lt;p&gt;Ya tenngo resultados: la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la que causa el cuelgue del sistema en las RPi3.&lt;/p&gt;

&lt;p&gt;Échale un vistazo a cómo solucionar este error en la entrada: &lt;a href=&#34;https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/&#34;&gt;El nodo k3 del clúster colgado de nuevo&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Portainer para gestionar tus contenedores en Docker</title>
      <link>https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/</link>
      <pubDate>Sat, 29 Apr 2017 12:55:04 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://portainer.io/&#34;&gt;Portainer&lt;/a&gt; es una herramienta ligera y &lt;em&gt;open-source&lt;/em&gt; de gestión de contenedores sobre Docker (o Docker Swarm). Portainer ofrece una interfaz gráfica para gestionar el &lt;em&gt;host&lt;/em&gt; Docker desde cualquier navegador, tiene soporte para Raspberry Pi y se puede desplegar como un simple contenedor.&lt;/p&gt;

&lt;p&gt;Espero que este artículo ayude a todos aquellos que tengan ganas de probar Portainer y evitarles los problemas que me he encontrado yo.&lt;/p&gt;

&lt;p&gt;
&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/portainer-logo.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/portainer-logo.png&#34; width=1106 height=361 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;He estado buscando algún tipo de solución gráfica para monitorizar las Raspberry Pi ya que, por algún motivo, los nodos &lt;em&gt;worker&lt;/em&gt; del clúster de Kubernetes se &lt;em&gt;cuelgan&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Buscando alguna solución de monitorizado he encontrado Portainer referenciado en el blog de Hypriot: &lt;a href=&#34;https://blog.hypriot.com/post/new-docker-ui-portainer/&#34;&gt;Visualize your Raspberry Pi containers with Portainer or UI for Docker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Portainer no es una herramienta de monitorizado (a nivel de &lt;em&gt;host&lt;/em&gt;), sino que está enfocada a la &lt;em&gt;visualización&lt;/em&gt; básicamente del estado de los contenedores de uno (o varios) &lt;em&gt;endpoints&lt;/em&gt; Docker (o Docker Swarm). Sin embargo, ofreciendo soporte para ARM y estando disponible en forma de contenedor, no había motivo para no probarlo ;)&lt;/p&gt;

&lt;h1 id=&#34;soporte-para-arm&#34;&gt;Soporte para ARM&lt;/h1&gt;

&lt;p&gt;En el apartado para obtener Portainer de la web, sólo se indica el comando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d -p 9000:9000 portainer/portainer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Por muy minimalista que quiera ser la página, la verdad es que no les hubiera costado nada indicar que existen diferentes versiones disponibles de la imagen (como por ejemplo, la que proporciona soporte para ARM).&lt;/p&gt;

&lt;p&gt;Además, lanzando el comando &lt;em&gt;tal cual&lt;/em&gt;, si quieres configurar Portainer para monitorizar el nodo &lt;em&gt;local&lt;/em&gt;, &lt;strong&gt;no funcionará&lt;/strong&gt; (requiere montar &lt;code&gt;/var/run/docker.sock&lt;/code&gt; en el contenedor).&lt;/p&gt;

&lt;p&gt;El artículo de Hypriot apunta a una imagen llamada &lt;code&gt;portainer/portainer:arm&lt;/code&gt;, que ya no existe en DockerHub. Revisando las &lt;a href=&#34;https://hub.docker.com/r/portainer/portainer/tags/&#34;&gt;etiquetas disponibles para las imágenes de Portainer&lt;/a&gt;, encontramos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;TagName				Compressed Size 	Last Updated 
ppc64le				4 MB			16 days ago
demo				4 MB			23 days ago
latest				0 B 			23 days ago
1.12.4				0 B 			23 days ago
windows-amd64 			337 MB			23 days ago
windows-amd64-1.12.4	 	337 MB			23 days ago
linux-arm64 			4 MB			23 days ago
linux-arm64-1.12.4 		4 MB			23 days ago
linux-arm 			4 MB			23 days ago
linux-arm-1.12.4 		4 MB			23 days ago
linux-amd64 			4 MB			23 days ago
linux-amd64-1.12.4 		4 MB			23 days ago
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seleccionamos la versión adecuada para nuestra Raspberry Pi y la descargamos mediante:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker pull portainer/portainer:linux-arm-1.12.4
linux-arm-1.12.4: Pulling from portainer/portainer
a3ed95caeb02: Pull complete
802d894958a2: Pull complete
30fb5c96d238: Pull complete
Digest: sha256:5269fd824014fac1dee29e2cf74aa5c337cf5c0ceb7cae2634c1e054f5e2763f
Status: Downloaded newer image for portainer/portainer:linux-arm-1.12.4
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación he lanzado la creación del contenedor usando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker run -d -p 9000:9000 --name portainer portainer/portainer:linux-arm-1.12.4
d5ad5764788a932cd19942dcb0e70471101173c8d14801b0ce7c172ef9ac72ff
$
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;acceso-a-portainer&#34;&gt;Acceso a Portainer&lt;/h2&gt;

&lt;p&gt;Abre un navegador y accede a &lt;code&gt;http://IP-nodo:9000/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;La primera vez que accedes a la URL de Portainer debes introducir el password del usuario &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-1-define-admin-password.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-1-define-admin-password.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Una vez introducido, puedes acceder a la UI de gestión de Portainer.&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-2-first-login.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-2-first-login.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Para mostrar información sobre los contendores (imágenes, volúmenes, etc) en Docker, Portainer necesita conectarse -vía API- al &lt;em&gt;host&lt;/em&gt; en el que corre Docker. Tenemos dos opciones, un &lt;em&gt;endpoint remoto&lt;/em&gt; (opción por defecto) o conectar con el &lt;em&gt;host&lt;/em&gt; donde corre Portainer:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-3-remote_endpoint_by_default.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-3-remote_endpoint_by_default.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;El problema es que, como vemos, al seleccionar un &lt;em&gt;endpoint&lt;/em&gt; local, se indica que hay que lanzar el contenedor de Portainer dando acceso al contenedor sobre &lt;code&gt;/var/run/docker.sock&lt;/code&gt;:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-4-local_endpoint_require_docker.sock.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-4-local_endpoint_require_docker.sock.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Como este &lt;em&gt;detalle&lt;/em&gt; no se indica en ningún sitio hasta que estás intentando configurar Portainer, lo más probable es que no hayas lanzado el contenedor con el volumen necesario.&lt;/p&gt;

&lt;p&gt;Así que es necesario detener el contenedor -y eliminarlo, si quieres reusar el nombre- y volver a lanzar el proceso de configuración.&lt;/p&gt;

&lt;p&gt;No son más que unos pocos comandos en Linux (o en tu Mac), pero sin duda es una molestia que podría evitarse dando algo más de información. Mucho más grave es si el sistema operativo de tu &lt;em&gt;host&lt;/em&gt; es Windows, ya que &lt;strong&gt;esta opción no está disponible&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker stop portainer
portainer
$ docker rm portainer
portainer
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;acceso-a-portainer-segundo-intento&#34;&gt;Acceso a Portainer (segundo intento)&lt;/h2&gt;

&lt;p&gt;Lanzamos el contenedor de Portainer montando &lt;code&gt;docker.sock&lt;/code&gt; y pasamos por los mismos pasos que en intento anterior:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker run -d -p 9000:9000 --name portainer -v &amp;quot;/var/run/docker.sock:/var/run/docker.sock&amp;quot; portainer/portainer:linux-arm-1.12.4
3f0ad98393ed5c67cda864737d83fe098a13d1317e1f6c299419ab1a3c1d153c
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de validarnos, podemos conectar con el &lt;em&gt;docker-engine&lt;/em&gt; local y visualizar el &lt;em&gt;dashboard&lt;/em&gt;:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-5-dashboard.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-5-dashboard.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Desde la interfaz web podemos gestionar los contenedores, imágenes y volúmenes existentes:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-6-containers.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-6-containers.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Contenedores.
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-7-images.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-7-images.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Imágenes.
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-8-volumes.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-8-volumes.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Volúmenes.
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;En el próximo artículo me concentraré en usar &lt;a href=&#34;https://onthedock.github.io/tags/portainer/&#34;&gt;Portainer&lt;/a&gt; para realizar la gestión de Docker.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Solución al error de instalación de Kubernetes en Debian Jessie (Missing cgroups: memory)</title>
      <link>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</link>
      <pubDate>Sat, 22 Apr 2017 07:57:14 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</guid>
      <description>&lt;p&gt;Al lanzar la inicialización del clúster con &lt;code&gt;kubeadm init&lt;/code&gt; en Debian Jessie, las comprobaciones inciales indican que no se encuentran los &lt;em&gt;cgroups&lt;/em&gt; para la memoria (échale un vistazo al artículo &lt;a href=&#34;https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/&#34;&gt;La instalación de Kubernetes falla en Debian Jessie (missing cgroups: memory)&lt;/a&gt;). Los &lt;em&gt;cgroups&lt;/em&gt; son una de las piezas fundamentales en las que se basa Docker para &lt;em&gt;aislar&lt;/em&gt; los procesos de los contenedores, por lo que la inicialización del clúster de Kubernetes se detiene.&lt;/p&gt;

&lt;p&gt;La solución es tan sencilla como habilitar los &lt;em&gt;cgroups&lt;/em&gt; durante el arranque.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;En primer lugar, verificamos la versión del &lt;em&gt;kernel&lt;/em&gt; que tenemos instalada:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# uname -a
Linux k8s 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al lanzar &lt;code&gt;kubeadm init&lt;/code&gt; obtenemos el error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubeadm init
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
OS: Linux
KERNEL_VERSION: 3.16.0-4-amd64
CONFIG_NAMESPACES: enabled
CONFIG_NET_NS: enabled
CONFIG_PID_NS: enabled
CONFIG_IPC_NS: enabled
CONFIG_UTS_NS: enabled
CONFIG_CGROUPS: enabled
CONFIG_CGROUP_CPUACCT: enabled
CONFIG_CGROUP_DEVICE: enabled
CONFIG_CGROUP_FREEZER: enabled
CONFIG_CGROUP_SCHED: enabled
CONFIG_CPUSETS: enabled
CONFIG_MEMCG: enabled
CONFIG_INET: enabled
CONFIG_EXT4_FS: enabled (as module)
CONFIG_PROC_FS: enabled
CONFIG_NETFILTER_XT_TARGET_REDIRECT: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled (as module)
CONFIG_OVERLAYFS_FS: not set - Required for overlayfs.
CONFIG_AUFS_FS: enabled (as module)
CONFIG_BLK_DEV_DM: enabled (as module)
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: missing
DOCKER_VERSION: 17.04.0-ce
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[preflight] Some fatal errors occurred:
	missing cgroups: memory
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;solución&#34;&gt;Solución&lt;/h2&gt;

&lt;p&gt;La solución la he encontrado en &lt;a href=&#34;https://phabricator.wikimedia.org/T122734&#34;&gt;Enable memory cgroups for default Jessie image&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Nos convertimos en &lt;em&gt;root&lt;/em&gt;: &lt;code&gt;sudo su -&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Editamos el fichero &lt;code&gt;/etc/default/grup&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;En los parámetros para el arranque de linux (&lt;code&gt;GRUB_CMDLINE_LINUX&lt;/code&gt;) añadimos &lt;code&gt;cgroup_enable=memory&lt;/code&gt;. En mi caso, la línea queda: &lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet cgroup_enable=memory&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Actualizamos &lt;em&gt;grub&lt;/em&gt;: &lt;code&gt;update-grub2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Reiniciamos la máquina: &lt;code&gt;reboot&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[preflight] Some fatal errors occurred:
	missing cgroups: memory
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
root@k8s:~# nano /etc/default/grub
root@k8s:~# update-grub2
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.16.0-4-amd64
Found initrd image: /boot/initrd.img-3.16.0-4-amd64
done
root@k8s:~# reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al lanzar &lt;code&gt;kubeadm init&lt;/code&gt; de nuevo, el clúster arranca con normalidad:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;root@k8s:~# kubeadm init
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] API Server serving cert is signed for DNS names [k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.99]
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key and public key.
[certificates] Generated front-proxy CA certificate and key.
[certificates] Generated front-proxy client certificate and key.
[certificates] Valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/scheduler.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/controller-manager.conf&amp;quot;
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 39.363656 seconds
[apiclient] Waiting for at least one node to register
[apiclient] First node has registered after 1.518215 seconds
[token] Using token: fe9e91.7142118e712eb019
[apiconfig] Created RBAC rules
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  sudo cp /etc/kubernetes/admin.conf $HOME/
  sudo chown $(id -u):$(id -g) $HOME/admin.conf
  export KUBECONFIG=$HOME/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token fe9e91.7142118e712eb019 192.168.1.99:6443

root@k8s:~#
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Cómo agregar un nodo a un cluster Kubernetes</title>
      <link>https://onthedock.github.io/post/170417-como-agregar-un-nodo-a-un-cluster-kubernetes/</link>
      <pubDate>Sat, 15 Apr 2017 16:27:30 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170417-como-agregar-un-nodo-a-un-cluster-kubernetes/</guid>
      <description>&lt;p&gt;Después de realizar la instalación del nodo &lt;em&gt;master&lt;/em&gt; del clúster Kubernetes, el siguiente paso es agregar nodos adicionales al clúster. Es en estos nodos donde se van a planificar los &lt;em&gt;pods&lt;/em&gt; que realizan las funciones &lt;em&gt;productivas&lt;/em&gt; del clúster (en el nodo &lt;em&gt;master&lt;/em&gt; sólo realiza tareas de gestión del clúster).&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;cómo-agregar-un-nodo-a-un-cluster-kubernetes&#34;&gt;Cómo agregar un nodo a un cluster Kubernetes&lt;/h1&gt;

&lt;p&gt;En el nodo que vamos a añadir tenemos instalador HypriotOS (una distribución basada en Debian creada específicamente para ejecutar Docker en la Raspberry Pi).&lt;/p&gt;

&lt;p&gt;Hypriot OS tiene instalado Docker &lt;em&gt;de fábrica&lt;/em&gt; así que comprobamos la versión instalada:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker version
Client:
 Version:      17.04.0-ce
 API version:  1.28
 Go version:   go1.7.5
 Git commit:   4845c56
 Built:        Mon Apr  3 18:22:23 2017
 OS/Arch:      linux/arm

Server:
 Version:      17.04.0-ce
 API version:  1.28 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   4845c56
 Built:        Mon Apr  3 18:22:23 2017
 OS/Arch:      linux/arm
 Experimental: false
$
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tareas-previas&#34;&gt;Tareas previas&lt;/h2&gt;

&lt;p&gt;La instalación de HypriotOS define como nombre del &lt;em&gt;host&lt;/em&gt; &lt;code&gt;black-pearl&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Lo primero que haremos será cambiar el nombre del &lt;em&gt;host&lt;/em&gt;. Para ello modificamos el fichero &lt;code&gt;/boot/device-init.yaml&lt;/code&gt; especificando el nombre elegido para el nuevo nodo. En mi caso, &lt;code&gt;k2&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo nano /boot/device-init.yaml

# hostname for your HypriotOS device
hostname: k2

# optional wireless network settings
wifi:
  interfaces:
#     wlan0:
#       ssid: &amp;quot;MyNetwork&amp;quot;
#       password: &amp;quot;secret_password&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para que los cambios tenga efecto, es necesario reiniciar el equipo. Antes, sin embargo, vamos a establecer una IP fija.&lt;/p&gt;

&lt;p&gt;Creamos una copia del fichero &lt;code&gt;/etc/network/interfaces.d/eth0&lt;/code&gt; antes de editarlo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo cp /etc/network/interfaces.d/eth0 /etc/network/interfaces.d/eth0.original
$ sudo nano /etc/network/interfaces.d/eth0
allow-hotplug eth0
iface eth0 inet static
  address 192.168.1.12
  gateway 192.168.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Una vez realizadas las modificaciones del &lt;em&gt;hostname&lt;/em&gt; y de la dirección IP, reiniciamos el &lt;em&gt;host&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;instalación-de-kubernetes-kubeadm-kubectl-y-kubelet&#34;&gt;Instalación de Kubernetes (&lt;code&gt;kubeadm&lt;/code&gt;,  &lt;code&gt;kubectl&lt;/code&gt; y &lt;code&gt;kubelet&lt;/code&gt;)&lt;/h2&gt;

&lt;p&gt;Seguimos las instrucciones de la página oficial de Kubernetes: &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;Installing Kubernetes on Linux with kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Nos conectamos a la máquina vía &lt;em&gt;SSH&lt;/em&gt; y nos convertimos en &lt;code&gt;root&lt;/code&gt; mediante &lt;code&gt;sudo su -&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ apt-get update &amp;amp;&amp;amp; apt-get install -y apt-transport-https
...
apt-transport-https is already the newest version.0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El siguiente paso es obtener la clave GPG:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Añadimos el repositorio de Kubernetes y actualizamos la lista de paquetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
$ apt-get update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verificamos que tenemos Docker instalado (en nuestro caso, &lt;code&gt;docker-engine&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ apt-get install -y docker-engine
...
docker-engine is already the newest version.0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora es el momento de lanzar la instalación de los diferentes componentes de Kubernets:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni
...
The following extra packages will be installed:  ebtables socatThe following NEW packages will be installed:  ebtables kubeadm kubectl kubelet kubernetes-cni socat0 upgraded, 6 newly installed, 0 to remove and 0 not upgraded.Need to get 37.1 MB of archives.After this operation, 266 MB of additional disk space will be used.0% [Working]
...
Setting up kubernetes-cni (0.5.1-00) ...
Setting up socat (1.7.2.4-2) ...
Setting up kubelet (1.6.1-00) ...
Setting up kubectl (1.6.1-00) ...
Setting up kubeadm (1.6.1-00) ...
Processing triggers for systemd (215-17+deb8u6) ...
$
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;agregar-nodo-al-clúster&#34;&gt;Agregar nodo al clúster&lt;/h2&gt;

&lt;p&gt;Para añadir el &lt;em&gt;host&lt;/em&gt; como un nodo adicional del clúster de Kubernetes, usaremos el comando &lt;code&gt;kubeadm join --token {token} {IP-nodo-master}:puerto&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;El comando &lt;code&gt;kubeadm&lt;/code&gt; genera el token al inicializar el clúster, pero si no lo tenemos apuntado, podemos obtenerlo conectando al nodo &lt;em&gt;master&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ssh pirate@k1.local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para obtener el &lt;em&gt;token&lt;/em&gt;, nos convertimos en el usuario &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo su -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación, obtenemos la lista de &lt;em&gt;tokens&lt;/em&gt; generados en el clúster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubeadm token list
TOKEN                     TTL         EXPIRES   USAGES                 DESCRIPTION
5e6517.b9e07...293ff612   &amp;lt;forever&amp;gt;   &amp;lt;never&amp;gt;   authentication,signing   The default bootstrap token generated by &#39;kubeadm init&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copiamos el &lt;em&gt;token&lt;/em&gt; y cerramos la conexión con el nodo &lt;em&gt;master&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;En el &lt;em&gt;host&lt;/em&gt; que vamos a unir como nodo al clúster, ejecutamos (como &lt;code&gt;root&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubeadm join --token=5e6517.b9e07...293ff612 192.168.1.11:6443
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[discovery] Trying to connect to API Server &amp;quot;192.168.1.11:6443&amp;quot;
[discovery] Created cluster-info discovery client, requesting info from &amp;quot;https://192.168.1.11:6443&amp;quot;
[discovery] Cluster info signature and contents are valid, will use API Server &amp;quot;https://192.168.1.11:6443&amp;quot;
[discovery] Successfully established connection with API Server &amp;quot;192.168.1.11:6443&amp;quot;
[bootstrap] Detected server version: v1.6.0
[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)
[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request
[csr] Received signed certificate from the API server, generating KubeConfig...
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
Node join complete:
   * Certificate signing request sent to master and response  received.
   * Kubelet informed of new secure connection details.

   Run &#39;kubectl get nodes&#39; on the master to see this machine join.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;verificación&#34;&gt;Verificación&lt;/h2&gt;

&lt;p&gt;Para comprobar que el nodo &lt;code&gt;k2&lt;/code&gt; se ha añadido correctamente al clúster, nos conectamos al nodo &lt;em&gt;master&lt;/em&gt; y obtenemos la lista de nodos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME       STATUS     AGE       VERSION
k1         Ready      4d        v1.6.1
k2.local         NotReady   40s       v1.6.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El nodo &lt;code&gt;k2&lt;/code&gt; del clúster aparece como &lt;code&gt;NotReady&lt;/code&gt;. Esta situación debe ser temporal. Tras unos instantes, al ejecutar de nuevo el comando, el &lt;em&gt;status&lt;/em&gt; del nuevo nodo debería haber cambiado y mostrarse como &lt;code&gt;Ready&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME       STATUS    AGE       VERSION
k1         Ready     4d        v1.6.1
k2.local   Ready     1m        v1.6.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cambio-de-nombre-del-nodo&#34;&gt;Cambio de nombre del nodo&lt;/h2&gt;

&lt;p&gt;Incialmente he añadido el nodo al cúster como &lt;code&gt;k2.local&lt;/code&gt;, sin darme cuenta que el sufijo &lt;code&gt;.local&lt;/code&gt; lo añade el &lt;em&gt;daemon&lt;/em&gt; Avahi al publicar el nombre del &lt;em&gt;host&lt;/em&gt; en la red local.&lt;/p&gt;

&lt;p&gt;He modificado el nombre del nodo en el fichero &lt;code&gt;/boot/device-init.yaml&lt;/code&gt; y he reiniciado el nodo, pero a nivel del clúster, el nodo &lt;code&gt;k2.local&lt;/code&gt; sigue formando parte del mismo. Por eso aparece como &lt;code&gt;NotReady&lt;/code&gt; al ejecutar &lt;code&gt;get nodes&lt;/code&gt;. El &lt;em&gt;nuevo&lt;/em&gt;  nodo &lt;code&gt;k2&lt;/code&gt; sí que aparece en al ejecutar el comando &lt;code&gt;get nodes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME       STATUS     AGE       VERSION
k1         Ready      4d        v1.6.1
k2         Ready      37m       v1.6.1
k2.local   NotReady   1h        v1.6.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para evitar confusiones, lo más conveniente es eliminar el nodo del clúster mediante &lt;code&gt;kubectl delete node&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete node k2.local
node &amp;quot;k2.local&amp;quot; deleted

$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k1        Ready     4d        v1.6.1
k2        Ready     45m       v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Error: The connection to the server localhost:8080 was refused</title>
      <link>https://onthedock.github.io/post/170414-error_the-connection-to-the-server-was-refused/</link>
      <pubDate>Fri, 14 Apr 2017 18:10:34 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170414-error_the-connection-to-the-server-was-refused/</guid>
      <description>&lt;p&gt;Después de &lt;a href=&#34;https://onthedock.github.io/post/170410-k8s-en-rpi-teaser/&#34;&gt;conseguir arrancar Kubernetes tras la instalación&lt;/a&gt;, al intentar ejecutar comandos vía &lt;code&gt;kubectl&lt;/code&gt; obtengo el mensaje de error &lt;code&gt;The connection to the server localhost:8080 was refused - did you specify the right host or port?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A continuación explico cómo solucionar el error y evitar que vuelva a mostrarse.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;En la guía oficial para instalar Kubernetes en Linux con &lt;code&gt;kubeadm&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;Installing Kubernetes on Linux with kubeadm&lt;/a&gt;, en la salida del comando &lt;code&gt;kubeadm init&lt;/code&gt; en el punto &lt;em&gt;(&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;) - Initializing your master&lt;/em&gt;, se muestra:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular &amp;gt;user):

 sudo cp /etc/kubernetes/admin.conf $HOME/
 sudo chown $(id -u):$(id -g) $HOME/admin.conf
 export KUBECONFIG=$HOME/admin.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El problema es que la &lt;em&gt;exportación&lt;/em&gt; de la variable de entorno realizada mediante &lt;code&gt;export KUBECONFIG=$HOME/admin.conf&lt;/code&gt; &lt;strong&gt;se pierde en cuanto se cierra la sesión&lt;/strong&gt;.
Por tanto, cuando reconectamos más tarde, la variable &lt;code&gt;KUBECONFIG&lt;/code&gt; está vacía y el comando &lt;code&gt;kubectl&lt;/code&gt; intenta conectar con &lt;code&gt;localhost:8080&lt;/code&gt;. Como el API server no está escuchando en esta IP y puerto, lo que obtenemos el mensaje de error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The connection to the server localhost:8080 was refused - did you specify the right host or port?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si miramos el contenido del fichero &lt;code&gt;$HOME/admin.conf&lt;/code&gt; mediante &lt;code&gt;cat $HOME/admin.conf&lt;/code&gt; encontramos una línea que identifica el servidor: &lt;code&gt;server: https://192.168.1.11:6443&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Parece que lo único que tenemos que hacer es especificar el servidor como parámetro para &lt;code&gt;kubectl&lt;/code&gt;, pero&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes --server=https://192.168.1.11:6443
Please enter Username: pirate
Please enter Password: ********
  Unable to connect to the server: x509: certificate signed by unknown authority
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Si usamos el usuario &lt;code&gt;root&lt;/code&gt;, el resultado es el mismo.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Observando el contenido del fichero &lt;code&gt;admin.conf&lt;/code&gt; vemos que para el parámetro &lt;code&gt;user&lt;/code&gt; se especifican certificados (mediante &lt;code&gt;client-certificate-data&lt;/code&gt; y &lt;code&gt;client-key-data&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQ...
    client-key-data:   LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLQVdLN3JjWDIKY2DIKY2t1c...
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Así que no podemos autenticarnos en el API Server con los usuarios del sistema y tenemos que usar los certificados en el fichero &lt;code&gt;admin.conf&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Esto nos lleva de nuevo a la variable &lt;code&gt;KUBECONFIG&lt;/code&gt;. Si lanzamos el comando &lt;code&gt;export KUBECONFIG...&lt;/code&gt;, los comandos funcionarán durante la sesión en curso, pero tendremos que lanzar el comando &lt;code&gt;export&lt;/code&gt; en cada nueva sesión:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export KUBECONFIG=$HOME/admin.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La solución para que la variable se establezca automáticamente en cada inicio de sesión es añadiéndo el valor en el fichero &lt;code&gt;$HOME/.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ nano $HOME/.bashrc
export KUBECONFIG=$HOME/admin.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para verificar que funciona como debe, cierra sesión y vuelve a iniciarla.&lt;/p&gt;

&lt;p&gt;Comprueba que puedes lanzar comandos sin problemas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k1        Ready     3d        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;¡Problema solucionado!&lt;/p&gt;

&lt;p&gt;Otra solución alternativa, si no quieres modificar el fichero &lt;code&gt;$HOME/admin.conf&lt;/code&gt; es pasar la ubicación del fichero como parámetro a &lt;code&gt;kubectl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port?
$ kubectl --kubeconfig ./admin.conf get nodes
NAME      STATUS    AGE       VERSION
k1        Ready     3d        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Puedes usar también este método para conectar, por ejemplo, desde otro equipo al nodo master del clúster (debes copiar primero el fichero &lt;code&gt;admin.conf&lt;/code&gt; a tu equipo, desde su ubicación original &lt;code&gt;/etc/kubernetes/admin.conf&lt;/code&gt; o desde la carpeta &lt;code&gt;$HOME&lt;/code&gt; del usuario, si lo has copiado):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ scp pirate@k1.local:/home/pirate/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes en la Raspberry Pi (teaser)</title>
      <link>https://onthedock.github.io/post/170410-k8s-en-rpi-teaser/</link>
      <pubDate>Mon, 10 Apr 2017 22:45:28 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170410-k8s-en-rpi-teaser/</guid>
      <description>&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170410/itsalive.jpg&#34; alt=&#34;Kubernetes en la Raspberry Pi (teaser) images/170410/itsalive.jpg&#34; width=400 height=292 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170410/k8s-en-rpi.png&#34; alt=&#34;Kubernetes en la Raspberry Pi (teaser) images/170410/k8s-en-rpi.png&#34; width=640 height=259 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Todos los componentes necesarios en Running
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Acciones previas a la instalación de Kubernetes en Raspberry Pi</title>
      <link>https://onthedock.github.io/post/170409-acciones-previas-instalacion-rpi/</link>
      <pubDate>Sun, 09 Apr 2017 21:34:16 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170409-acciones-previas-instalacion-rpi/</guid>
      <description>&lt;p&gt;Uno de los objetivos motivadores de la existencia de este blog es instalar un clúster de Kubernetes sobre Raspberry Pi. Este artículo se centra en las tareas previas a la instalación en sí.&lt;/p&gt;

&lt;p&gt;Kubernetes requiere una instalación previa de Docker, una tarea simplificada gracias a HypriotOS, la &lt;em&gt;distro&lt;/em&gt; creada específicamente con este fin.&lt;/p&gt;

&lt;p&gt;El siguiente paso, la instalación de Kubernetes en la Raspberry será objeto de otra(s) entrada(s). Pero sin duda esta tarea sería mucho más complicada sin las contribuciones del joven finlandés &lt;a href=&#34;https://www.cncf.io/blog/2016/11/29/diversity-scholarship-series-programming-journey-becoming-kubernetes-maintainer/&#34;&gt;Lucas Käldström&lt;/a&gt; y su proyecto -ahora integrado la rama principal- &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes on ARM&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Descarga la imagen de &lt;a href=&#34;https://blog.hypriot.com/downloads/&#34;&gt;HypriotOS&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Traspásala a una tarjeta microSD usando, por ejemplo, &lt;a href=&#34;https://etcher.io/&#34;&gt;Etcher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Inserta la tarjeta microsSD en la Raspberry Pi y arranca la RPi.&lt;/li&gt;
&lt;li&gt;Comprueba que ha arrancado correctamente haciendo ping a &lt;code&gt;black-pearl.local&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Accede a la RPi mediante &lt;code&gt;ssh pirate@black-pearl.local&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Acepta el mensaje de seguridad (es la primera vez que conectas al equipo)&lt;/li&gt;
&lt;li&gt;Edita el fichero &lt;code&gt;/boot/device-init.yaml&lt;/code&gt; para modificar el nombre de la RPi. En mi caso, he cambiado el nombre a &lt;code&gt;k1&lt;/code&gt;: &lt;code&gt;hostname: k1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Crea un backup del fichero de configuración de la tarjeta de red: &lt;code&gt;sudo cp /etc/network/interfaces.d/eth0 /etc/network/interfaces.d/eth0.original&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Edita el fichero &lt;code&gt;/etc/network/interfaces.d/eth0&lt;/code&gt; para establecer una IP estática para la RPi:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  allow-hotplug eth0
  iface eth0 inet static
	address 192.168.1.11
	gateway 192.168.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Reinicia la RPi para que los cambios sean efectivos: &lt;code&gt;sudo reboot&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Comprueba que la RPi responde a ping con el nuevo nombre: &lt;code&gt;ping k1.local&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Accede a la RPi mediante &lt;code&gt;ssh pirate@k1.local&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Actualiza la RPi: &lt;code&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade -y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Verifica la versión de Docker instalada: &lt;code&gt;$ docker version&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Publica en Github Pages</title>
      <link>https://onthedock.github.io/post/170403-publica-en-github-pages/</link>
      <pubDate>Mon, 03 Apr 2017 22:38:35 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170403-publica-en-github-pages/</guid>
      <description>&lt;p&gt;Cómo publicar el sitio web generado con Hugo en GitHub Pages.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Siguiendo las instrucciones de la página de Hugo sobre &lt;a href=&#34;https://gohugo.io/tutorials/github-pages-blog/#hosting-personal-organization-pages&#34;&gt;cómo publicar en Github Pages&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Creo un repo llamado &lt;code&gt;onthedock.github.io&lt;/code&gt;: este albergará el sitio público.&lt;/li&gt;
&lt;li&gt;Creo un repo llamado &lt;code&gt;onthedock-hugo&lt;/code&gt; que contendrá todo el site: ficheros de hugo, el template, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Creo una carpeta local llamada &lt;code&gt;onthedock-githubpages&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Dentro de la carpeta, lanzo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/onthedock/onthedock-hugo.git`
Cloning into &#39;.&#39;...
warning: You appear to have cloned an empty repository.
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compruebo que tengo un repositorio local inicializado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git status
On branch master

Initial commit

nothing to commit (create/copy files and use &amp;quot;git add&amp;quot; to track)
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copio el contenido del &lt;em&gt;site&lt;/em&gt; de Hugo (que previamente he movido a otra carpeta):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git status
On branch master
Initial commit
Untracked files:
  (use &amp;quot;git add &amp;lt;file&amp;gt;...&amp;quot; to include in what will be committed)
	config.toml
	content/
	static/
	themes/
nothing added to commit but untracked files present (use &amp;quot;git add&amp;quot; to track)
$
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Las carpetas vacías no se añaden a Git.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Eliminamos la carpeta &lt;code&gt;$HUGO/public&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Añadimos un &lt;a href=&#34;https://git-scm.com/book/es/v1/Las-herramientas-de-Git-Subm%C3%B3dulos&#34;&gt;&lt;em&gt;submodulo&lt;/em&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git submodule add -b master https://github.com/onthedock/onthedock.github.io.git public
Cloning into &#39;/Users/xavi/Dropbox/dev/hugo/onthedock-githubpages/public&#39;...
remote: Counting objects: 3, done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (3/3), done.
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Comprobamos el estado del repositorio:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git status
On branch master

Initial commit

Changes to be committed:
  (use &amp;quot;git rm --cached &amp;lt;file&amp;gt;...&amp;quot; to unstage)

	new file:   .gitmodules
	new file:   public

Untracked files:
  (use &amp;quot;git add &amp;lt;file&amp;gt;...&amp;quot; to include in what will be committed)

	config.toml
	content/
	static/
	themes/

$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Añadimos los ficheros del &lt;em&gt;andamiaje&lt;/em&gt; de Hugo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git add .
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verifico que el repositorio &lt;em&gt;remoto&lt;/em&gt; es el correcto:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git remote -v
origin	https://github.com/onthedock/onthedock-hugo.git (fetch)
origin	https://github.com/onthedock/onthedock-hugo.git (push)
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Y subo el sitio al &lt;em&gt;repo&lt;/em&gt; remoto: &lt;code&gt;onthedock-hugo&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$  git push origin master
error: src refspec master does not match any.
error: failed to push some refs to &#39;https://github.com/onthedock/onthedock-hugo.git&#39;
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Oopps.&lt;/p&gt;

&lt;p&gt;El problema era que no había guardado ningún cambio, por lo que no existía la rama &lt;code&gt;master&lt;/code&gt;. Aunque he interpretado correctamente el mensaje, he corregido el problema en el extremo opuesto (en el repositorio remoto); he creado un fichero &lt;code&gt;License.md&lt;/code&gt; y he lanzado &lt;code&gt;git pull&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git pull origin
remote: Counting objects: 3, done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (3/3), done.
From https://github.com/onthedock/onthedock-hugo
 * [new branch]      master     -&amp;gt; origin/master
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;git pull&lt;/code&gt; hace un &lt;code&gt;git fetch&lt;/code&gt; y un &lt;code&gt;git merge&lt;/code&gt;, lo que crea un &lt;em&gt;commit&lt;/em&gt; (que era lo que me faltaba por hacer):&lt;/p&gt;

&lt;p&gt;Vuelvo a intentarlo y esta vez sí:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git commit

(había añadido los cambios al _staging area_ pero no los había guardado con _commit_)

$ git push origin master
Username for &#39;https://github.com&#39;: onthedock
Password for &#39;https://onthedock@github.com&#39;:
Counting objects: 64, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (56/56), done.
Writing objects: 100% (64/64), 3.28 MiB | 547.00 KiB/s, done.
Total 64 (delta 2), reused 0 (delta 0)
remote: Resolving deltas: 100% (2/2), done.
To https://github.com/onthedock/onthedock-hugo.git
   92f53f2..5ecc4bc  master -&amp;gt; master
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora voy a generar el sitio (después de actualizar el fichero &lt;code&gt;config.toml&lt;/code&gt; para que el parámetro &lt;code&gt;baseURL&lt;/code&gt; apunte a la dirección &lt;em&gt;pública&lt;/em&gt; del sitio en GitHub):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ hugo
Started building sites ...
Built site for language en:
0 draft content
0 future content
0 expired content
6 regular pages created
14 other pages created
0 non-page files copied
12 paginator pages created
8 tags created
2 categories created
total in 68 ms
$ git status
On branch master
Your branch is up-to-date with &#39;origin/master&#39;.
Changes not staged for commit:
  (use &amp;quot;git add &amp;lt;file&amp;gt;...&amp;quot; to update what will be committed)
  (use &amp;quot;git checkout -- &amp;lt;file&amp;gt;...&amp;quot; to discard changes in working directory)
  (commit or discard the untracked or modified content in submodules)

	modified:   public (untracked content)

no changes added to commit (use &amp;quot;git add&amp;quot; and/or &amp;quot;git commit -a&amp;quot;)
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El contenido de la carpeta &lt;code&gt;$HUGO/public&lt;/code&gt;  está contenida en un &lt;em&gt;submódulo&lt;/em&gt; de Git.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd public/
$ git status
On branch master
Your branch is up-to-date with &#39;origin/master&#39;.
Untracked files:
  (use &amp;quot;git add &amp;lt;file&amp;gt;...&amp;quot; to include in what will be committed)

	404.html
	categories/
	images/
	index.html
	index.xml
	page/
	post/
	sitemap.xml
	tags/

nothing added to commit but untracked files present (use &amp;quot;git add&amp;quot; to track)
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora, desde este &lt;em&gt;sub-repositorio&lt;/em&gt;, lanzo &lt;code&gt;git add&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git add .
$ git status
On branch master
Your branch is up-to-date with &#39;origin/master&#39;.
Changes to be committed:
  (use &amp;quot;git reset HEAD &amp;lt;file&amp;gt;...&amp;quot; to unstage)

	new file:   404.html
	new file:   categories/dev/index.html
	new file:   categories/dev/index.xml
	new file:   categories/dev/page/1/index.html
	new file:   categories/index.html
	new file:   categories/ops/index.html
	new file:   categories/ops/index.xml
	new file:   categories/ops/page/1/index.html
.
.
.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lanzo un &lt;em&gt;commit&lt;/em&gt; para guardar los cambios:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git commit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verifico que el repositorio remoto es &lt;code&gt;onthedock.github.io&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git remote -v
origin	https://github.com/onthedock/onthedock.github.io.git (fetch)
origin	https://github.com/onthedock/onthedock.github.io.git (push)
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora, subo los cambios al repositorio de GitHub Pages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git push origin master
Username for &#39;https://github.com&#39;: onthedock
Password for &#39;https://onthedock@github.com&#39;:
Counting objects: 104, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (74/74), done.
Writing objects: 100% (104/104), 911.20 KiB | 0 bytes/s, done.
Total 104 (delta 41), reused 0 (delta 0)
remote: Resolving deltas: 100% (41/41), done.
To https://github.com/onthedock/onthedock.git
   49c08af..5432d12  master -&amp;gt; master
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La web estará accesible en los próximos diez minutos, aproximadamente, en &lt;code&gt;http://onthedock.github.io&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;El nombre del repositorio debe ser &lt;code&gt;onthedock.github.io&lt;/code&gt;, y no sólo  &lt;code&gt;onthedock&lt;/code&gt;. Si te pasa como a mi y debes cambiar el nombre del &lt;em&gt;repo&lt;/em&gt; , recuerda que ¡puedes hacerlo!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Se puede renombrar el repositorio desde GitHub, pero eso supone que también hay que actualizar el nombre del repositorio en la configuración del  &lt;em&gt;remote&lt;/em&gt; en el repositorio local.&lt;/p&gt;

&lt;p&gt;Para ello, usa el comando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git remote set-url origin https://github.com/onthedock/onthedock.github.io.git
$ git remote -v
origin	https://github.com/onthedock/onthedock.github.io.git (fetch)
origin	https://github.com/onthedock/onthedock.github.io.git (push)
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Una vez cambiado el nombre del repositorio, tras una corta espera, el sitio ya es accesible a través de &lt;code&gt;https://onthedock.github.io&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>De Blogger a Hugo</title>
      <link>https://onthedock.github.io/post/170401-de-blogger-a-hugo/</link>
      <pubDate>Sat, 01 Apr 2017 18:10:12 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170401-de-blogger-a-hugo/</guid>
      <description>&lt;p&gt;Porqué me estoy planteando dejar Blogger y pasar a un sitio estático gracias a Hugo.&lt;/p&gt;

&lt;p&gt;Hugo es un &lt;em&gt;generador de sitios estáticos&lt;/em&gt; a partir de ficheros en formato &lt;em&gt;markdown&lt;/em&gt;. Hugo aplica una plantilla al contenido de los ficheros en formato &lt;em&gt;markdon&lt;/em&gt; y crea los ficheros HTML.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;motivación&#34;&gt;Motivación&lt;/h2&gt;

&lt;p&gt;Aunque llevo &lt;em&gt;toda la vida&lt;/em&gt; con un blog personal en &lt;a href=&#34;https://www.blogger.com/&#34;&gt;Blogger&lt;/a&gt;, Google ha desatendido la plataforma y poco a poco se ha ido quedando atrás en prestaciones.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ghost.org&#34;&gt;Ghost&lt;/a&gt; es la platforma que cada vez más desarrolladores y escritores &lt;em&gt;técnicos&lt;/em&gt; usan, tanto en la versión alojada como en sus propias instalaciones. Es la que me gustaría usar para mis blogs: soporta &lt;em&gt;markdown&lt;/em&gt; y no se entromete en el proceso ni de escribir ni de publicar los artículos.&lt;/p&gt;

&lt;p&gt;Mi objetivo era ejecutar Ghost en la Raspberry Pi, pero al no existir soporte de SQLite para la arquitectura ARM, las imágenes para &lt;a href=&#34;https://github.com/alexellis/ghost-on-docker&#34;&gt;&lt;em&gt;contenedores&lt;/em&gt; Docker&lt;/a&gt; están desactualizadas y no siempre son fáciles de &lt;em&gt;construir&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Por otro lado, el objetivo del blog en la Raspberry Pi es documentar el proceso de aprendizaje sobre  Docker y Kubernetes (además de Linux). A diferencia de lo que pasaba en mi anterior trabajo, donde estuve usando Hugo de forma &lt;em&gt;experimental&lt;/em&gt;, ahora estas notas no contienen ningún tipo de información privada, por lo que publicaré también los artículos en internet.&lt;/p&gt;

&lt;p&gt;En mis pruebas Hugo se integró en el flujo de trabajo diario sin interferir lo más mínimo, por lo que resultó una experiencia muy positiva.&lt;/p&gt;

&lt;p&gt;Quiero combinar este blog (orientado al avance, a las pruebas, es decir, al proceso) con Dokuwiki (como almacén de conocimiento y documentación). Sin embargo, con Dokuwiki la  &lt;em&gt;dualidad&lt;/em&gt; entre en entorno &lt;em&gt;local&lt;/em&gt; (en casa) y en internet es más difícil de conseguir de forma directa (usando recursos gratuitos). Tengo un &lt;a href=&#34;http://wiki-ameisin.rhcloud.com/&#34;&gt;contenedor en OpenShift&lt;/a&gt; con notas sobre diferentes temas, pero en esta instancia de Dokuwiki en OpenShift las carpetas de datos tienen una estructura diferente a la estándar, lo que dificulta mantener &lt;em&gt;sincronizadas&lt;/em&gt; la versión &lt;em&gt;local&lt;/em&gt; y la alojada en el &lt;em&gt;cloud&lt;/em&gt; de Red Hat.&lt;/p&gt;

&lt;h2 id=&#34;hugo&#34;&gt;Hugo&lt;/h2&gt;

&lt;p&gt;La idea detrás de un generador de sitios estáticos es que, en muchas ocasiones, no es necesario disponer de toda la potencia que ofrecen las plataformas de &lt;em&gt;blogging&lt;/em&gt; modernas como &lt;a href=&#34;https://wordpress.org&#34;&gt;Wordpress&lt;/a&gt;, etc. Además, estas plataformas no son siempre fáciles de instalar, configurar y mantener en tu propio entorno local.&lt;/p&gt;

&lt;p&gt;La alternativa es mantener un sitio web a partir de ficheros HTML independientes, pero resulta muy costoso en tiempo y esfuerzo.&lt;/p&gt;

&lt;p&gt;A medio camino se encuentran los generadores de sitios como &lt;a href=&#34;https://jekyllrb.com&#34;&gt;Jekill&lt;/a&gt; o &lt;a href=&#34;https://gohugo.io&#34;&gt;Hugo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Estos &lt;em&gt;generadores de sitios estáticos&lt;/em&gt; parten de ficheros en formato &lt;em&gt;markdown&lt;/em&gt; -que son sencillos de escribir- y se encargan de combinarlos con unas plantillas, generar los enlaces entre los diferentes artículos, crear nubes de etiquetas, etc (la parte tediosa) hasta generar los ficheros HTML.&lt;/p&gt;

&lt;p&gt;Al final del proceso, tenemos un conjunto de ficheros &lt;em&gt;web&lt;/em&gt; (HTML, javascript, css) que podemos alojar en cualquier servidor (o en servicios como &lt;a href=&#34;https://pages.github.com&#34;&gt;GitHub Pages&lt;/a&gt; o &lt;a href=&#34;https://confluence.atlassian.com/bitbucket/publishing-a-website-on-bitbucket-cloud-221449776.html&#34;&gt;Bitbucket&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;siguientes-pasos&#34;&gt;Siguientes pasos&lt;/h2&gt;

&lt;p&gt;En estas fase inicial, únicamente tengo un contenedor con un servidor web (Nginx) sirviendo el sitio estático generado por Hugo (en un portátil).&lt;/p&gt;

&lt;p&gt;Más adelante quiero incluir también un contenedor con Hugo (como el proporcionado por &lt;a href=&#34;https://hub.docker.com/r/hypriot/rpi-hugo/&#34;&gt;Hypriot&lt;/a&gt;) e ir añadiendo poco a poco todas las herramientas del proceso de Integración Continua -en forma de contenedores- desde el &lt;em&gt;código fuente&lt;/em&gt; al sitio web publicado automáticamente con cada cambio. Como se apunta en la entrada &lt;a href=&#34;https://blog.hypriot.com/post/static-website-generation-on-steriods-with-docker/&#34;&gt;Static Website Generation on Steriods with Docker&lt;/a&gt;, la idea es montar una cadena de &lt;a href=&#34;https://es.wikipedia.org/wiki/Integración_continua&#34;&gt;CI&lt;/a&gt;: GoGS (repositorio de código &lt;em&gt;a lo Github&lt;/em&gt;), &lt;a href=&#34;https://github.com/drone/drone&#34;&gt;Drone&lt;/a&gt; (el &lt;em&gt;motor&lt;/em&gt; de Integración Continua: como &lt;a href=&#34;https://es.wikipedia.org/wiki/Jenkins&#34;&gt;Jenkins&lt;/a&gt;, pero escrito en Go) y para el &lt;em&gt;deployment&lt;/em&gt;, una mezcla de &lt;a href=&#34;https://hub.docker.com/r/xaviaznar/rpi-alpine-nginx/&#34;&gt;Nginx&lt;/a&gt; (publicación local) y &lt;a href=&#34;https://bitbucket.org/product&#34;&gt;Bitbucket&lt;/a&gt; (publicación en internet).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Configura curl para usar un proxy</title>
      <link>https://onthedock.github.io/post/170111-configura-curl-para-usar-proxy/</link>
      <pubDate>Wed, 11 Jan 2017 08:22:56 +0100</pubDate>
      
      <guid>https://onthedock.github.io/post/170111-configura-curl-para-usar-proxy/</guid>
      <description>&lt;p&gt;Cómo configurar &lt;code&gt;curl&lt;/code&gt; para salir a internet a través de un &lt;em&gt;proxy&lt;/em&gt; que requiere autenticación.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Como la VM está detrás de un &lt;em&gt;proxy&lt;/em&gt;, primero tienes que indicar a &lt;code&gt;curl&lt;/code&gt; la dirección del mismo. La manera más sencilla de solucionar el problema de una vez por todas es indicar la URL del &lt;em&gt;proxy&lt;/em&gt; en el fichero &lt;code&gt;.curlrc&lt;/code&gt;, en la carpeta &lt;em&gt;home&lt;/em&gt; del usuario.&lt;/p&gt;

&lt;p&gt;Si estás trabajando con el usuario &lt;code&gt;root&lt;/code&gt;, coloca el fichero en &lt;code&gt;/root/.curlrc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Edita el fichero y añade la dirección del &lt;em&gt;proxy&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;proxy = https://${USERNAME}:${PASSWORD}@proxy.ameisin.com:8080/proxy.pac
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Referencia: &lt;a href=&#34;http://stackoverflow.com/questions/7559103/how-to-setup-curl-to-permanently-use-a-proxy&#34;&gt;How to setup curl to permanently use a proxy? [closed]&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instala Docker en Ubuntu Server 16.04</title>
      <link>https://onthedock.github.io/post/170110-instala-docker-en-ubuntu-server-16.04/</link>
      <pubDate>Tue, 10 Jan 2017 15:12:46 +0100</pubDate>
      
      <guid>https://onthedock.github.io/post/170110-instala-docker-en-ubuntu-server-16.04/</guid>
      <description>&lt;p&gt;Cómo instalar Docker en Ubuntu Server 16.04.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Para instalar la última versión de Docker, usamos las instrucciones &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/como-instalar-y-usar-docker-en-ubuntu-16-04-es&#34;&gt;¿Cómo instalar y usar Docker en Ubuntu 16.04?&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;No funciona por algún motivo, probablemente por el &lt;em&gt;proxy&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Para instalar la clave GPG de Docker, el método que funciona es (&lt;small&gt;ref: &lt;a href=&#34;https://github.com/docker/docker/issues/17436#issuecomment-151870782&#34;&gt;Docker website encourages users to import GPG key for apt repository in unsafe ways #17436&lt;/a&gt;&lt;/small&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# curl -s  https://get.docker.com/gpg | apt-key add -
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Agregamos el repositorio de Docker a APT&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-add-repository &#39;deb https://apt.dockerproject.org/repo ubuntu-xenial main&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Actualizamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Una vez añadido, comprobamos mediante:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-cache policy docker-engine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalmente, instalamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install -y docker-engine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si ha habido problemas para validar la autenticidad del paquete de Docker, la instalación debe hacerse sin la aceptación automática (es decir, sin el parámetro &lt;code&gt;-y&lt;/code&gt;) o añadiendo &lt;code&gt;--allow-authenticate&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Verificamos que tenemos docker funcionando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# docker version
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>