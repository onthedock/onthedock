<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on On The Dock</title>
    <link>https://onthedock.github.io/post/index.xml</link>
    <description>Recent content in Posts on On The Dock</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Handmade with &amp;#9829; by Xavi Aznar</copyright>
    <lastBuildDate>Sat, 20 May 2017 19:59:44 +0200</lastBuildDate>
    <atom:link href="https://onthedock.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Normas para estructurar ficheros implicados en la creación de contenedores</title>
      <link>https://onthedock.github.io/post/170520-normas-para-estructurar-ficheros-implicados-en-la-creacion-de-contenedores/</link>
      <pubDate>Sat, 20 May 2017 19:59:44 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170520-normas-para-estructurar-ficheros-implicados-en-la-creacion-de-contenedores/</guid>
      <description>&lt;p&gt;El proceso desde la creación a la ejecución del contenedor se puede separar en varias fases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Creación de la imagen (mediante la redacción de un fichero &lt;code&gt;Dockerfile&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Construcción de la imagen&lt;/li&gt;
&lt;li&gt;Ejecución del contenedores&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Para tener los diferentes ficheros implicados en el proceso organizados de forma homogénea, me he autoimpuesto las siguientes reglas a la hora de estructurar los repositorios.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h2&gt;

&lt;p&gt;El primero paso para ejecutar un contenedor es crear la imagen en la que está basado. Para ello debes crear un fichero &lt;code&gt;Dockerfile&lt;/code&gt; en el que se indica la imagen base usada y los diferentes pasos de instalación de paquetes, configuración de usuarios, volúmenes y puertos expuestos.&lt;/p&gt;

&lt;p&gt;En la creación de la imagen intervienen, además del fichero &lt;code&gt;Dockerfile&lt;/code&gt;, ficheros de configuración, etc que se copian a la imagen desde la carpeta donde se encuentra el fichero &lt;code&gt;Dockerfile&lt;/code&gt; (el llamado &lt;em&gt;contexto&lt;/em&gt;, ver &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/build/#options&#34;&gt;Documentación oficial de &lt;code&gt;docker build&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Para gestionar los cambios sobre estos ficheros, lo más sencillo es guardarlos en un repositorio y tener un registro de todos los cambios que se van introduciendo a lo largo del tiempo.&lt;/p&gt;

&lt;p&gt;Todos los ficheros relacionados con la &lt;em&gt;creación&lt;/em&gt; de la imagen se colocan en una carpeta llamada &lt;code&gt;build&lt;/code&gt;, con el &lt;code&gt;Dockerfile&lt;/code&gt; y los ficheros de configuración, etc, agrupados en sus correspondientes carpetas.&lt;/p&gt;

&lt;p&gt;En esta carpeta también se incluyen un fichero con instrucciones para la creación de la imagen (condiciones en las que reutilizar la cache, puntos a tener en cuenta, etc) y un &lt;em&gt;script&lt;/em&gt; para lanzar la creación de la imagen de forma siempre igual (quizás el script borra ficheros temporales o descargados en ejecuciones anteriores, por ejemplo).&lt;/p&gt;

&lt;h2 id=&#34;construcción-de-la-imagen&#34;&gt;Construcción de la imagen&lt;/h2&gt;

&lt;p&gt;Una vez creado el &lt;code&gt;Dockerfile&lt;/code&gt;, &lt;em&gt;construyes&lt;/em&gt; la imagen mediante &lt;code&gt;docker build&lt;/code&gt;. Aunque en general la construcción se realiza mediante un sólo comando de la forma &lt;code&gt;docker build -t {repositorio/etiqueta} .&lt;/code&gt;, puede ser interesante disponer de documentación con indicaciones sobre las reglas de etiquetado de la imagen definidas por la empresa o similar.&lt;/p&gt;

&lt;h2 id=&#34;ejecución-del-contenedor&#34;&gt;Ejecución del contenedor&lt;/h2&gt;

&lt;p&gt;Finalmente la creación de contenedores basados en la imagen se realiza mediante un comando &lt;code&gt;docker run&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A la hora de ejecutar el contenedor la instrucción puede incluir el nombre del contenedor final, la relación entre puertos del &lt;em&gt;host&lt;/em&gt; y el contenedor, el montaje de volúmenes, etc. En algunos casos, el contenedor admite parámetros que se pasan al comando definido en la instrucción &lt;code&gt;CMD&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Para evitar errores o simplemente para no teclear una y otra vez comandos larguísimos para ejecutar el contenedor, podemos crear un &lt;em&gt;script&lt;/em&gt; que lance el contenedor con los parámetros necesarios, así como documentación de la funcionalidad proporcionada por el contenedor, etc.&lt;/p&gt;

&lt;p&gt;Estos ficheros se guardan en el carpeta llamada &lt;code&gt;run&lt;/code&gt;; básicamente el comando para lanzar la creación del contenedor de forma homogénea y las instrucciones con información sobre el uso del contenedor, volúmenes, etc.&lt;/p&gt;

&lt;h2 id=&#34;carpetas&#34;&gt;Carpetas&lt;/h2&gt;

&lt;p&gt;Para estructurar todos los ficheros implicados en el proceso de creación de un contenedor he definido la siguiente estructura de carpetas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./nombre-contenedor/
 |
 ├─Readme.md
 ├─build/
 | ├─Dockerfile
 | ├─build.sh
 | ├─Build-Instructions.md
 | ├─{context-files}/
 | ├─...
 | ├─{context-files}/
 ├─run/
 | ├─run.sh
 | ├─Run-Instructions.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;motivación&#34;&gt;Motivación&lt;/h2&gt;

&lt;p&gt;No he encontrado ningún artículo sobre la organización de los ficheros implicados en el creación de imágenes o de los flujos de trabajo asociados a estos procesos. Tampoco sobre las normas a la hora de etiquetar las imágenes o si se realizan validaciones a la hora de obtener/subir imágenes de repositorios públicos.&lt;/p&gt;

&lt;p&gt;Incluso en una empresa en la que el proceso de desarrollo y operación de las aplicaciones gire alrededor del concepto &lt;em&gt;DevOp&lt;/em&gt;, puede haber otros implicados en el proceso &lt;em&gt;administrativo&lt;/em&gt; del ciclo de vida de la aplicación: decisiones estratégicas, a nivel de seguridad, de &lt;em&gt;compliance&lt;/em&gt; con leyes como la protección de datos, etc.&lt;/p&gt;

&lt;p&gt;En los artículos/conferencias lo habitual es explicar soluciones técnicas sin entrar nunca en estos procesos que relacionan IT con el resto de departamentos de la empresa.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>El nodo k3 sigue colgandose por culpa de Flannel</title>
      <link>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</link>
      <pubDate>Wed, 17 May 2017 21:02:21 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</guid>
      <description>&lt;p&gt;En la entrada &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt; encontré restos de la instalación de &lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt; en la Raspberry Pi. Eliminé los &lt;em&gt;pods&lt;/em&gt; que hacían referencia a Flannel y conseguí que el nodo &lt;strong&gt;k2&lt;/strong&gt; no se volviera a colgar.&lt;/p&gt;

&lt;p&gt;Sin embargo, el problema sigue dándose en el nodo &lt;strong&gt;k3&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Revisando el contenido de &lt;code&gt;/var/lib/kubernetes/pods/&lt;/code&gt; he visto que algunos &lt;em&gt;pods&lt;/em&gt; hacían referencia, todavía, a Flannel.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~439642480.deleting~747926470.deleting~067946013.deleting~791070092.deleting~964331938.deleting~717873461.deleting~755129373.deleting~499171027
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~504683568.deleting~184413472.deleting~138413964.deleting~985160408.deleting~943143520.deleting~459558341.deleting~578589077.deleting~501462031.deleting~769373718
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~613861491.deleting~841547526.deleting~012178845.deleting~177797190.deleting~192052322.deleting~958792988.deleting~338401309.deleting~623810479.deleting~369130424
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~288881360.deleting~534630955.deleting~520377076.deleting~598159984.deleting~426698803.deleting~142931759.deleting~872800923.deleting~808586860
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~439642480.deleting~747926470.deleting~067946013.deleting~791070092.deleting~622848191.deleting~646325460.deleting~868409130.deleting~824166496
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~668636622.deleting~334825066.deleting~737147422.deleting~055159245.deleting~572255670.deleting~485248219.deleting~690855316.deleting~753094008.deleting~457647557
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~756292309.deleting~273222811.deleting~039503494.deleting~182629307.deleting~984614903.deleting~081831640.deleting~628560452.deleting~303652395.deleting~450650534
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~883601122.deleting~535739903.deleting~385002935.deleting~558075878.deleting~174007749.deleting~757820208.deleting~194356513.deleting~813327027.deleting~485662152
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta vez he detectado el problema al intentar calcular el espacio usado por esta carpeta, ya que la Raspberry Pi se ha quedado como &amp;ldquo;colgada&amp;rdquo;, aunque al lanzar &lt;em&gt;htop&lt;/em&gt; no se observaba un uso excesivo de CPU.&lt;/p&gt;

&lt;p&gt;Finalmente, he usado el mismo sistema que la otra vez: eliminar todas las subcarpetas de cada uno de los &lt;em&gt;pods&lt;/em&gt; (dejando únicamente los que no se pueden borrar al estar en uso).&lt;/p&gt;

&lt;p&gt;Después de la purga masiva de &lt;code&gt;rm -rf /var/lib/kubelet/pods/&lt;/code&gt; sólo han quedado dos carpetas &lt;em&gt;en uso&lt;/em&gt;; el número corresponde con el número de &lt;em&gt;pods&lt;/em&gt; planificados sobre el nodo &lt;strong&gt;k3&lt;/strong&gt; desde &lt;code&gt;kubectl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                         READY     STATUS    RESTARTS   AGE       IP             NODE
kube-system   etcd-k1                      1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-apiserver-k1            1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-controller-manager-k1   1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-dns-279829092-1b27r     3/3       Running   12         36d       10.32.0.2      k1
kube-system   kube-proxy-20t3b             1/1       Running   0          25m       192.168.1.13   k3
kube-system   kube-proxy-3dggd             1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-proxy-5b8k3             1/1       Running   2          12d       192.168.1.12   k2
kube-system   kube-scheduler-k1            1/1       Running   4          36d       192.168.1.11   k1
kube-system   weave-net-6qr0l              2/2       Running   8          36d       192.168.1.11   k1
kube-system   weave-net-mxp2w              2/2       Running   0          25m       192.168.1.13   k3
kube-system   weave-net-tmmdj              2/2       Running   4          12d       192.168.1.12   k2
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Un reinicio y ¡listo!, problema -espero- resuelto de forma definitiva.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Protege el acceso remoto via API a Docker</title>
      <link>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sun, 07 May 2017 18:33:16 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; explicaba cómo configurar el acceso remoto al API de Docker. El problema es que de esta forma no hay manera de restringir el acceso.&lt;/p&gt;

&lt;p&gt;En este artículo protegemos el acceso usando TLS de manera que sólo se permitan conexiones que presenten un certificado firmado por una CA de confianza.
&lt;/p&gt;

&lt;p&gt;Seguiremos las instrucciones oficiales de Docker &lt;a href=&#34;https://docs.docker.com/engine/security/https/&#34;&gt;Protect the Docker daemon socket&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;creamos-una-ca-claves-para-el-cliente-y-el-servidor-con-openssl&#34;&gt;Creamos una CA, claves para el cliente y el servidor con OpenSSL&lt;/h2&gt;

&lt;p&gt;Primero, en la máquina &lt;em&gt;host&lt;/em&gt; del Docker &lt;em&gt;daemon&lt;/em&gt;, generamos las claves públicas y privadas de la CA (&lt;em&gt;Certification Authority&lt;/em&gt;, la entidad certificadora):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -aes256 -out ca-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................++
..........................++
e is 65537 (0x10001)
Enter pass phrase for ca-key.pem:
Verifying - Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Y a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem
Enter pass phrase for ca-key.pem:
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &#39;.&#39;, the field will be left blank.
-----
Country Name (2 letter code) [AU]:ES
State or Province Name (full name) [Some-State]:Barcelona
Locality Name (eg, city) []:Barcelona
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Ameisin
Organizational Unit Name (eg, section) []:DevOps
Common Name (e.g. server FQDN or YOUR name) []:192.168.1.20
Email Address []: {REDACTED}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora que tenemos una CA, podemos crear la clave para el servidor y la petición de firmado del certificado (&lt;em&gt;certificate signing request&lt;/em&gt;, CSR). Por favor, verifica que &lt;code&gt;Common Name&lt;/code&gt; (es decir, el &lt;em&gt;FQDN&lt;/em&gt; o &lt;em&gt;YOUR Name&lt;/em&gt;) coincide con el nombre del &lt;em&gt;host&lt;/em&gt; que vas a usar para conectar a Docker.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out server-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................................................................................................++
...............................................................++
e is 65537 (0x10001)
# openssl req -subj &amp;quot;/CN=192.168.1.20&amp;quot; -sha256 -new -key server-key.pem -out server.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación vamos a firmar la clave pública con nuestra CA.&lt;/p&gt;

&lt;p&gt;Como las conexiones TLS pueden realizarse usando la dirección IP o un nombre DNS, deben especificarse durante la creación del certificado.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo subjectAltName = IP:192.168.1.20,IP:127.0.0.1 &amp;gt; extfile.cnf
# openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out server-cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=192.168.1.20
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;autenticación-del-cliente&#34;&gt;Autenticación del cliente&lt;/h2&gt;

&lt;p&gt;Para autenticar al cliente, crearemos una clave de cliente y una petición de firmado del certificado.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Para simplificar, los siguientes dos pasos pueden realizarse desde la máquina donde se encuentra el Docker &lt;em&gt;daemon&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out key.pem 4096
Generating RSA private key, 4096 bit long modulus
...............................................................................++
................................................................................................................................................................................++
e is 65537 (0x10001)
# openssl req -subj &#39;/CN=client&#39; -new -key key.pem -out client.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para que la clave permita autenticar al cliente, creamos un fichero de configuración de extensiones:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo extendedKeyUsage = clientAuth &amp;gt; extfile.cnf
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora firmamos la clave:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=client
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de haber generado &lt;code&gt;cert.pem&lt;/code&gt; y &lt;code&gt;server-cert.pem&lt;/code&gt; podemos eliminar las peticiones de firmado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ls
ca-key.pem  ca.srl    client.csr  extfile.cnf  server-cert.pem	server-key.pem
ca.pem	    cert.pem  key.pem     server.csr
# rm -v client.csr server.csr
removed ‘client.csr’
removed ‘server.csr’
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;protección-de-las-claves&#34;&gt;Protección de las claves&lt;/h2&gt;

&lt;p&gt;Con una máscara &lt;code&gt;umask&lt;/code&gt; por defecto de &lt;code&gt;022&lt;/code&gt; las claves secretas que hemos generado dan a todo el mundo acceso de lectura y de escritura a tu usuario y tu grupo.&lt;/p&gt;

&lt;p&gt;Para proteger las claves de daños accidentales, vamos a eliminar los permisos de escritura sobre ellas. Para hacerlas de sólo lectura para tu usuario, usamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0400 ca-key.pem key.pem server-key.pem
mode of ‘ca-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘server-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Los certificados pueden ser leídos por todo el mundo, pero para evitar daños accidentales, mejor eliminamos los permisos de escritura:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0444 ca.pem server-cert.pem cert.pem
mode of ‘ca.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘server-cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configurando-el-api-de-acceso-remoto-de-forma-segura&#34;&gt;Configurando el API de acceso remoto de forma segura&lt;/h2&gt;

&lt;p&gt;Para hacer que el Docker &lt;em&gt;daemon&lt;/em&gt; sólo acepte conexiones de clientes que proporcionen un certificado de confianza de tu CA.&lt;/p&gt;

&lt;p&gt;Para ello, modificamos las opciones de arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H=0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De manera que quede como (lo he dividido en varias líneas por claridad):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ExecStart=/usr/bin/dockerd --tlsverify 		\
         --tlscacert=/root/ca.pem 		\
         --tlscert=/root/server-cert.pem 	\
         --tlskey=/root/server-key.pem 		\
         -H=0.0.0.0:2376 			\
         -H fd://
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación, recargamos la configuración y reinciamos el servicio:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Los primeros intentos de arrancar el &lt;em&gt;daemon&lt;/em&gt; han fallado; ha sido necesario especificar la ruta completa a los certificados y las claves para conseguir que el servicio arrancara.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finalmente, comprobamos que podemos acceder usando el certificado con &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# curl https://192.168.1.20:2376/version --cert /root/cert.pem --key /root/key.pem --cacert /root/ca.pem
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A diferencia de lo que pasaba antes, cuando se intenta acceder a &lt;code&gt;https://192.168.1.9:2376/version&lt;/code&gt; desde otro equipo (sin usar el certificado), obtenemos un error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-url&#34;&gt;This site can’t be reached
192.168.1.9 refused to connect.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Configura un endpoint remoto en Portainer</title>
      <link>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</link>
      <pubDate>Sat, 06 May 2017 17:38:20 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/&#34;&gt;Portainer para gestionar tus contenedores en Docker&lt;/a&gt; usamos &lt;strong&gt;Portainer&lt;/strong&gt; para gestionar el Docker Engine local.&lt;/p&gt;

&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; habilitamos el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;En este artículo configuramos &lt;strong&gt;Portainer&lt;/strong&gt; para conectar con un &lt;em&gt;endpoint&lt;/em&gt; remoto (el API expuesta de un Docker Engine).
&lt;/p&gt;

&lt;p&gt;Accede a &lt;strong&gt;Portainer&lt;/strong&gt; y selecciona &lt;em&gt;Endpoints&lt;/em&gt; en el panel izquierdo.&lt;/p&gt;

&lt;p&gt;Para configurar el &lt;em&gt;endopoint&lt;/em&gt; remoto (no seguro) sólo necesitas proporcionar un nombre para el &lt;em&gt;endpoint&lt;/em&gt; y la URL de acceso:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/1-configure-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/1-configure-endpoint.png&#34; width=935 height=660 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Configura un nuevo endpoint
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;Para identificar qué Docker Engine estoy viendo en cada momento, indico la IP de la máquina, seguido de la plataforma y el &lt;em&gt;host&lt;/em&gt; en el que se encuentra.&lt;/p&gt;

&lt;p&gt;Para cambiar entre los diferentes &lt;em&gt;endpoints&lt;/em&gt; definidos en &lt;strong&gt;Portainer&lt;/strong&gt;, selecciona el que quieres gestionar en el desplegable de la parte superior del panel lateral:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/2-change-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/2-change-endpoint.png&#34; width=450 height=168 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Cambia entre los diferentes endpoints definidos
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Habilita el acceso remoto vía API a Docker</title>
      <link>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sat, 06 May 2017 15:23:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;Portainer permite gestionar &lt;em&gt;endpoints&lt;/em&gt; remotos para Docker (y Docker Swarm) mediante el API REST de Docker Engine. El problema es que el API está desactivado por defecto.&lt;/p&gt;

&lt;p&gt;A continuación indico cómo activar y verificar el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Buscando en Google cómo habilitar el API remoto de Docker Engine probablemente encuentres el artículo
&lt;a href=&#34;https://www.ivankrizsan.se/2016/05/18/enabling-docker-remote-api-on-ubuntu-16-04/&#34;&gt;Enabling Docker Remote API on Ubuntu 16.04&lt;/a&gt;. Como bien dice en el párrafo inicial, no es fácil encontrar unas instrucciones claras sobre cómo configurar el API de principio a fin.&lt;/p&gt;

&lt;p&gt;Lanzando &lt;code&gt;docker man&lt;/code&gt;, vemos que la opción que buscamos es:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-H, --host=[unix:///var/run/docker.sock]: tcp://[host]:[port][path] to bind or
       unix://[/path/to/socket] to use.
         The socket(s) to bind to in daemon mode specified using one or more
         tcp://host:port/path, unix:///path/to/socket, fd://* or fd://socketfd.
         If the tcp port is not specified, then it will default to either 2375 when
         --tls is off, or 2376 when --tls is on, or --tlsverify is specified.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta opción debe pasarse en el arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker. Para configurar esta opción durante el arranque de Docker Engine tenemos dos opciones:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;modificar el arranque del &lt;em&gt;daemon&lt;/em&gt; modificando la configuración de &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;añadiendo las opciones en el fichero de configuración de Docker Engine. Para sistemas Linux con &lt;em&gt;systemd&lt;/em&gt;, la &lt;a href=&#34;https://docs.docker.com/engine/admin/systemd/#start-automatically-at-system-boot&#34;&gt;configuración del &lt;em&gt;daemon&lt;/em&gt; de Docker&lt;/a&gt; se realiza a través del fichero &lt;code&gt;daemon.json&lt;/code&gt; ubicado en &lt;code&gt;/etc/docker/&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;He intentado configurar Docker Engine mediante el segundo método &lt;em&gt;daemon.json&lt;/em&gt; pero no he sido capaz de activar el API.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Primero, hacemos una copia de seguridad del fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# cp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.original
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editamos el fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target docker.socket firewalld.service
Wants=network-online.target
Requires=docker.socket

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H fd:// 
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=1048576
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process
# restart the docker process if it exits prematurely
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea &lt;code&gt;ExecStart=/usr/bin/dockerd -H fd://&lt;/code&gt; y añadimos: &lt;code&gt;-H tcp://0.0.0.0:2375&lt;/code&gt; de manera que quede:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Esto hace que &lt;em&gt;dockerd&lt;/em&gt; escuche en todas las interfaces disponibles. En el caso de la máquina virtual en la que estoy probando, sólo tengo una, pero lo correcto sería especificar la dirección IP donde quieres que escuche &lt;em&gt;dockerd&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Guardamos los cambios.&lt;/p&gt;

&lt;p&gt;Recargamos la configuración y reiniciamos el servicio.&lt;/p&gt;

&lt;p&gt;Para comprobar que hemos el API funciona, lanzamos una consulta usando &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
# curl http://localhost:2375/version
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Debes tener en cuenta que esta configuración &lt;strong&gt;supone un riesgo de seguridad&lt;/strong&gt; al permitir el acceso al API de Docker Engine sin ningún tipo de control.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Tags, categorias y organización en Hugo</title>
      <link>https://onthedock.github.io/post/170506-tags-categorias-archetypes-en-hugo/</link>
      <pubDate>Sat, 06 May 2017 06:23:50 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-tags-categorias-archetypes-en-hugo/</guid>
      <description>&lt;p&gt;A medida que aumenta el número de artículos me he dado cuenta de que es necesario tener algún conjunto de reglas para organizar los ficheros que componen el blog.&lt;/p&gt;

&lt;p&gt;El problema no está en los ficheros de Hugo, sino en los ficheros generados por mi: artículos, imágenes, etc.
&lt;/p&gt;

&lt;p&gt;Hugo genera sitios web a partir de un conjunto de ficheros organizados en carpetas: el contenido se encuentra en &lt;code&gt;$HUGO/content&lt;/code&gt;, las imágenes en &lt;code&gt;$HUGO/static/images&lt;/code&gt; (o dentro de la carpeta equivalente del &lt;em&gt;theme&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Con unos pocos artículos, es fácil identificar qué fichero corresponde a un artículo concreto (simplemente a partir del título). Pero cuando el número de ficheros aumenta, la cosa se complica; el sistama operativo ordena los ficheros por orden alfabético, mientras que en el blog están organizados por fecha de creación. Para complicar más las cosas, el nombre del fichero &lt;em&gt;puede&lt;/em&gt; no ser el mismo que el título del artículo.&lt;/p&gt;

&lt;p&gt;Pasa algo parecido con las imágenes; con unas pocas no hay problema, pero cuando tenga un montón, será complicado identificar qué imagen corresponde a cada artículo.&lt;/p&gt;

&lt;p&gt;Para evitar complicaciones, creo que lo mejor es definir unas reglas sobre cómo organizar el contenido y definir &lt;em&gt;consistentemente&lt;/em&gt; etiquetas, categorías, etc.&lt;/p&gt;

&lt;h2 id=&#34;categorías&#34;&gt;Categorías&lt;/h2&gt;

&lt;p&gt;El blog está orientado al &lt;em&gt;DIY tecnológico&lt;/em&gt; en el aprendizaje sobre contenedores y tecnologías relacionadas.&lt;/p&gt;

&lt;p&gt;En este sentido, para simplificar, he decidido limitarme a dos categorías básicas &lt;code&gt;Dev&lt;/code&gt; y &lt;code&gt;Ops&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;De momento estoy montando el clúster de Kubernetes, realizando troubleshooting, etc, por lo que la mayoría de artículos son de la categoría &lt;code&gt;Ops&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Cuando tenga el clúster montado y pueda crear &lt;em&gt;pods&lt;/em&gt; (&lt;em&gt;replication controllers&lt;/em&gt;, etc), empezaré con la parte más &lt;code&gt;Dev&lt;/code&gt;; de momento, los únicos artículos &lt;code&gt;Dev&lt;/code&gt; son los relacionados con Hugo, su configuración, etc.&lt;/p&gt;

&lt;h2 id=&#34;etiquetas&#34;&gt;Etiquetas&lt;/h2&gt;

&lt;p&gt;El objetivo de las etiquetas es permitir organizar de forma flexible los artículos en conjuntos relacionados. Esta flexibilidad puede degenerar rápidamente en un montón de etiquetas que se usan sólo una vez y que no aportan nada.&lt;/p&gt;

&lt;p&gt;Para evitar definir un conjunto de etiquetas estricto -y perder flexibilidad- he pensado que lo mejor es definir unas &lt;em&gt;reglas&lt;/em&gt; sobre qué etiquetas son necesarias en cada artículo.&lt;/p&gt;

&lt;p&gt;Ya he utilizado este sistema en otra ocasión y ha funcionado mucho mejor que otras alternativas que intenté en el pasado.&lt;/p&gt;

&lt;p&gt;La primera etiqueta se refiere a la arquitectura; puede ser &lt;code&gt;x64&lt;/code&gt; o &lt;code&gt;arm&lt;/code&gt;, básicamente. En el primer caso entran tanto equipos físicos como máquinas virtuales; en este caso, no uso ninguna etiqueta concreta (esta es la arquitectura &lt;em&gt;default&lt;/em&gt; para contenedores).&lt;/p&gt;

&lt;p&gt;En el segundo tenemos las Raspberry Pi, para las que uso la etiqueta &lt;code&gt;RASPBERRY PI&lt;/code&gt;, aunque igual me planteo usar &lt;code&gt;ARM&lt;/code&gt; (o las dos). Esto es porque no descarto &lt;em&gt;ampliar la familia&lt;/em&gt; e incorporar alguna Orange Pi al clúster.&lt;/p&gt;

&lt;p&gt;El siguiente nivel sería identificar el sistema operativo: hasta ahora sólo estoy usando Debian (en máquinas virtuales) o Hypriot OS (en las Raspberry Pi). Si en algún momento empiezo a probar contenedores sobre Windows, sólo tengo que añadir esta etiqueta.&lt;/p&gt;

&lt;p&gt;Por encima del sistema operativo tendría la capa de producto: Docker o Kubernetes, por el momento. También  Hugo, por ejemplo, aunque no sea el objetivo principal del blog.&lt;/p&gt;

&lt;p&gt;Finalmente, alguna etiqueta específica sobre el tema del artículo.&lt;/p&gt;

&lt;p&gt;Creo que esta organización de etiquetas permite identificar todo el &lt;em&gt;stack&lt;/em&gt; usado y así distinguir, especialmente pasado un tiempo, qué componentes estaba usando en cada momento (aunque no se expliciten en el artículo).&lt;/p&gt;

&lt;p&gt;Sería interesante poder incluir el número de versión de &lt;em&gt;cada capa&lt;/em&gt; (RPi 1,2,3, versión del SO, de Docker y Kubernetes&amp;hellip;) pero no se me ocurre cómo hacerlo de manera que sea a la vez útil para agrupar artículos y sin provocar &lt;em&gt;ruido&lt;/em&gt; (mogollón de etiquetas similares, como con &lt;code&gt;raspberry pi&lt;/code&gt;, &lt;code&gt;raspberry pi 2&lt;/code&gt; para poder agrupar por RPi, pero también sobre sólo los artículos sobre RPi2 y no los RPi 3, por ejemplo).&lt;/p&gt;

&lt;h2 id=&#34;fecha-en-el-nombre-de-fichero&#34;&gt;Fecha en el nombre de fichero&lt;/h2&gt;

&lt;p&gt;Para que los ficheros se muestren en un orden similar en el blog y en el sistema de ficheros del portátil, el truco es sencillo: los prefijo con la fecha: &lt;code&gt;yymmdd-nombre-articulo.md&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;No es habitual que escriba varios artículos el mismo día, pero incluso cuando lo hago, diferenciar entre dos o tres artículos no supone un problema.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;$ ls -la content/post/
...
2.6K Apr 30 15:18 170430-k3-colgado-de-nuevo.md
3.5K Apr 30 12:23 170430-multiples-mensajes-action-17-suspended.md
 12K May  6 05:23 170430-troubleshooting-kubernetes-i.md
6.1K May  5 22:51 170505-instala-weave-net-en-kubernetes-1.6.md
4.6K May  6 08:33 170506-tags-categorias-archetypes-en-hugo.md
6.0K May  6 06:11 170506-troubleshooting-kubernetes-ii.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En cuanto al nombre del fichero, uso el guión &lt;code&gt;-&lt;/code&gt; como sustituyo del espacio en el nombre del fichero. Si cambio el título del artículo, renombro el fichero para que &lt;strong&gt;siempre&lt;/strong&gt; el nombre del fichero y el artículo coincidan el máximo posible.&lt;/p&gt;

&lt;h2 id=&#34;agrupar-imágenes&#34;&gt;Agrupar imágenes&lt;/h2&gt;

&lt;p&gt;Al estar conectado por consola a las máquinas virtuales o las Raspberry Pi, no suelo hacer muchas capturas de pantalla.&lt;/p&gt;

&lt;p&gt;En un blog o un wiki, la propia plataforma se encarga de gestionar las imágenes y nunca he tenido problema porque dos imágenes tuvieran el mismo nombre. La inclusión de la imagen en el artículo se realiza de forma gráfica, por lo que el nombre de la imagen tampoco era importante.&lt;/p&gt;

&lt;p&gt;Sin embargo, al escribir los artículos en markdown y tener que enlazar las imágenes manualmente, el nombre del fichero de la imagen &lt;strong&gt;es relevante&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;He decidido crear una carpeta para evitar problemas de &lt;em&gt;colisión de nombres&lt;/em&gt; y tener organizadas todas las imágenes de un mismo artículo. El nombre de la carpeta corresponde a la fecha del artículo (de nuevo, en formato &lt;code&gt;yymmdd&lt;/code&gt;). Dentro de cada carpeta las imágenes se llaman como se tengan que llamar (de manera que tengan un nombre descriptivo) pero sin preocuparme de si ya existe otra imagen con el mismo nombre de fichero.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Troubleshooting Kubernetes (II)</title>
      <link>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</link>
      <pubDate>Sat, 06 May 2017 05:21:09 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</guid>
      <description>&lt;p&gt;Sigo con el &lt;em&gt;troubleshooting&lt;/em&gt; del &lt;em&gt;cuelgue&lt;/em&gt; de los nodos sobre Raspberry Pi 3 del clúster.&lt;/p&gt;

&lt;p&gt;Ayer estuve &lt;em&gt;haciendo limpieza&lt;/em&gt; siguiendo &lt;em&gt;vagamente&lt;/em&gt; la recomendación de &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;esta respuesta&lt;/a&gt; en el hilo &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;Kubernetes memory consumption explosion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;La solución de &lt;code&gt;RenaudWasTaken&lt;/code&gt; al problema de consumo excesivo de memoria (32GB) fue la realizar limpieza de las carpetas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/etcd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Antes de empezar a borrar &lt;em&gt;a lo loco&lt;/em&gt;, revisé el contenido de estas carpetas.&lt;/p&gt;

&lt;h2 id=&#34;var-run-kubernetes&#34;&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;En &lt;code&gt;/var/run/kubernetes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/run/kubernetes/
total 8
drwxr-xr-x  2 root root   80 May  5 18:43 .
drwxr-xr-x 18 root root  600 May  5 18:43 ..
-rw-r--r--  1 root root 1082 May  5 18:43 kubelet.crt
-rw-------  1 root root 1679 May  5 18:43 kubelet.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos ficheros son certificados, por lo que no parecen implicados en el problema y decido no borrarlos.&lt;/p&gt;

&lt;h2 id=&#34;var-lib-kubelet&#34;&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/h2&gt;

&lt;h3 id=&#34;nodo-k2&#34;&gt;Nodo &lt;strong&gt;k2&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Al intentar listar el contenido de la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt;, la Raspberry Pi 3 ha tardado una eternidad (en los primeros intentos he creído que había dejado de responder).&lt;/p&gt;

&lt;p&gt;Finalmente, el resultado del comando ha mostrado una gran cantidad de carpetas dentro de esta carpeta:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods
...
drwx------    2 root root    4096 May  4 23:11 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~207819844.deleting~559419937.deleting~142152710.deleting~494766199.deleting~952339001
drwx------    2 root root    4096 May  5 18:02 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~274250693.deleting~987962315.deleting~680794233.deleting~917929467
drwx------    2 root root    4096 May  4 23:37 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~292131322.deleting~049606881.deleting~105942520.deleting~463246644
drwx------    2 root root    4096 May  4 22:46 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~119491600.deleting~962328406.deleting~220005477.deleting~309794961.deleting~392355244.deleting~378832104.deleting~159122214.deleting~324365539
drwx------    2 root root    4096 Apr 15 20:39 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~162549394.deleting~296869341.deleting~353223099.deleting~018715754.deleting~526835026.deleting~320404022.deleting~453576282.deleting~001809150
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parece como si algo no hubiera funcionado correctamente y hubiera entrado en un bucle, creando carpetas y más carpetas. Además, en el nombre de alguna de estas carpetas aparece &lt;code&gt;..._flannel-cfg...&lt;/code&gt;. Esta ha sido la pista que me ha convencido; al intentar instalar el &lt;em&gt;dashboard&lt;/em&gt; de Kubernetes, tuve problemas precisamente porque no tengo instalado Flannel. Eliminé el &lt;em&gt;pod&lt;/em&gt; y no le di más vueltas.&lt;/p&gt;

&lt;p&gt;Sin embargo, la existencia de estas carpetas parece indicar que la eliminación no fue tan limpia como pensaba y que &lt;em&gt;algo&lt;/em&gt; se quedó atrapado en un bucle.&lt;/p&gt;

&lt;p&gt;He lanzado &lt;code&gt;rm -rf /var/lib/kubelet/pods/&lt;/code&gt; y el comando ha fallado indicando que uno de los &lt;em&gt;pods&lt;/em&gt; estaba en uso. Así que he eliminado poco a poco los &lt;em&gt;pods&lt;/em&gt; hasta que al final:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls /var/lib/kubelet/pods/ -la
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos &lt;em&gt;pods&lt;/em&gt;, sean los que sean, están en uso (no tengo nada desplegado en el clúster, así que deben ser &lt;em&gt;de sistema&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Tras la limpieza, he reiniciado el nodo.&lt;/p&gt;

&lt;h3 id=&#34;nodo-k3&#34;&gt;Nodo &lt;strong&gt;k3&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;El nodo &lt;strong&gt;k3&lt;/strong&gt; no presentaba estas &lt;em&gt;carpetas sospechosas&lt;/em&gt;, pero también he realizado limpieza:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ rm -rf /var/lib/kubelet/pods/
rm: cannot remove ‘/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap’: Directory not empty
rm: cannot remove ‘/var/lib/kubelet/pods/71290201-31bb-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/kube-proxy-token-7zk2k’: Device or resource busy
rm: cannot remove ‘/var/lib/kubelet/pods/ef887c6a-31ba-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/weave-net-token-61scv’: Device or resource busy
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Igual que en el nodo &lt;strong&gt;k2&lt;/strong&gt;, he reiniciado.&lt;/p&gt;

&lt;h2 id=&#34;resultados&#34;&gt;Resultados&lt;/h2&gt;

&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; siguen en estado &lt;code&gt;Ready&lt;/code&gt; tras unas siete y ocho horas, que es bastante más de lo que &lt;em&gt;aguantaban&lt;/em&gt; antes.&lt;/p&gt;

&lt;p&gt;He comprobado que en la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt; sólo aparecen dos &lt;em&gt;pods&lt;/em&gt; (en el nodo &lt;strong&gt;k2&lt;/strong&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En el nodo &lt;strong&gt;k3&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 28
drwxr-x--- 5 root root 12288 May  5 19:44 .
drwxr-x--- 4 root root  4096 Apr 15 14:10 ..
drwxr-x--- 3 root root  4096 May  5 19:33 3a5e2819-21e5-11e7-bcfd-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 514d4c93-31c9-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 c0b9753a-31c9-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Más adelante actualizaré el artículo para verificar si los nodos siguen activos y sin problemas.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instala Weave Net en Kubernetes 1.6</title>
      <link>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</link>
      <pubDate>Fri, 05 May 2017 22:14:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</guid>
      <description>&lt;p&gt;Una de las cosas que más me sorprenden de Kubernetes es que es necesario instalar una &lt;em&gt;capa de red&lt;/em&gt; sobre el clúster.&lt;/p&gt;

&lt;p&gt;En el caso concreto del que he obtenido las &lt;em&gt;capturas de pantalla&lt;/em&gt;, el clúster corre sobre máquinas virtuales con Debian Jessie.
&lt;/p&gt;

&lt;p&gt;La instalación de Weave Net en Kubernetes consiste únicamente en una línea, como explica el artículo: &lt;a href=&#34;https://www.weave.works/weave-net-kubernetes-integration/&#34;&gt;Run Weave Net with Kubernetes in Just One Line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Antes de instalar la &lt;em&gt;red&lt;/em&gt; en el clúster (de momento, de un solo nodo), &lt;em&gt;kubectl&lt;/em&gt; indica que el estado del nodo es &lt;code&gt;NotReady&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k8s       NotReady   5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la salida del comando tenemos que la versión de Kubernetes es la 1.6.1. Este dato será importante más adelante a la hora de escoger el comando de instalación de Weave Net.&lt;/p&gt;

&lt;p&gt;Si obtenemos la lista de &lt;em&gt;pods&lt;/em&gt;, comprobamos que no tenemos ningún &lt;em&gt;pod de red&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   0          5h
kube-system   kube-apiserver-k8s            1/1       Running   0          5h
kube-system   kube-controller-manager-k8s   1/1       Running   0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending   0          5h
kube-system   kube-proxy-l02zn              1/1       Running   0          5h
kube-system   kube-scheduler-k8s            1/1       Running   0          5h
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, los &lt;em&gt;pods&lt;/em&gt; de &lt;em&gt;DNS&lt;/em&gt; &lt;code&gt;kube-dns-*&lt;/code&gt; están en estado &lt;code&gt;Pending&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Siguiendo las instrucciones del artículo de Weave Net, lanzamos el comando de instalación para versiones 1.6 (o superior):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl apply -f https://git.io/weave-kube-1.6
clusterrole &amp;quot;weave-net&amp;quot; created
serviceaccount &amp;quot;weave-net&amp;quot; created
clusterrolebinding &amp;quot;weave-net&amp;quot; created
daemonset &amp;quot;weave-net&amp;quot; created
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obtenemos la lista de &lt;em&gt;pods&lt;/em&gt; de nuevo y observamos que se están creando dos nuevos contenedores:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;operador@k8s:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             0          5h
kube-system   kube-apiserver-k8s            1/1       Running             0          5h
kube-system   kube-controller-manager-k8s   1/1       Running             0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending             0          5h
kube-system   kube-proxy-l02zn              1/1       Running             0          5h
kube-system   kube-scheduler-k8s            1/1       Running             0          5h
kube-system   weave-net-32ptg               0/2       ContainerCreating   0          12s
operador@k8s:~$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De hecho, se creado el &lt;em&gt;daemonset&lt;/em&gt; &amp;ldquo;weave-net&amp;rdquo;. Un &lt;em&gt;daemonset&lt;/em&gt; es un &lt;em&gt;pod&lt;/em&gt; que se crea en cada uno de los nodos del clúster automáticamente. Kubernetes se encarga de descargar la imagen desde DockerHub y arrancar un contenedor. Los nodos en la red creada por Weave Net forman una red &lt;em&gt;mesh&lt;/em&gt; que se configura automáticamente, de manera que es posible agregar nodos adicionales sin necesidad de cambiar ninguna configuración.&lt;/p&gt;

&lt;p&gt;Pasados unos segundos la creación de los nodos se ha completado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$  kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS         RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running        0          5h
kube-system   kube-apiserver-k8s            1/1       Running        0          5h
kube-system   kube-controller-manager-k8s   1/1       Running        0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       ErrImagePull   0          5h
kube-system   kube-proxy-l02zn              1/1       Running        0          5h
kube-system   kube-scheduler-k8s            1/1       Running        0          5h
kube-system   weave-net-32ptg               2/2       Running        0          1m
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalmente, verificamos que el primer nodo del clúster ya es operativo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k8s       Ready     5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, una vez que tenemos la red instalada en el clúster, el &lt;em&gt;pod&lt;/em&gt; &lt;code&gt;kube-dns&lt;/code&gt; comienza la creación de los contenedores (quizás tengas que reiniciar):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             1          11d
kube-system   kube-apiserver-k8s            1/1       Running             1          11d
kube-system   kube-controller-manager-k8s   1/1       Running             1          11d
kube-system   kube-dns-3913472980-4nlg9     0/3       ContainerCreating   0          11d
kube-system   kube-proxy-l02zn              1/1       Running             1          11d
kube-system   kube-scheduler-k8s            1/1       Running             1          11d
kube-system   weave-net-32ptg               2/2       Running             3          10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tras unos segundos, tenemos todos los &lt;em&gt;pods&lt;/em&gt; del clúster funcionales:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   1          11d
kube-system   kube-apiserver-k8s            1/1       Running   1          11d
kube-system   kube-controller-manager-k8s   1/1       Running   1          11d
kube-system   kube-dns-3913472980-4nlg9     3/3       Running   0          11d
kube-system   kube-proxy-l02zn              1/1       Running   1          11d
kube-system   kube-scheduler-k8s            1/1       Running   1          11d
kube-system   weave-net-32ptg               2/2       Running   3          10d
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo tenemos que añadir nodos &lt;em&gt;worker&lt;/em&gt; y hacer crecer el clúster.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Troubleshooting Kubernetes (I)</title>
      <link>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</link>
      <pubDate>Sun, 30 Apr 2017 15:24:35 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</guid>
      <description>&lt;p&gt;Tras la alegría inicial pensando que la configuración de &lt;em&gt;rsyslog&lt;/em&gt; era la causante de los cuelgues de las dos RPi 3 (&lt;a href=&#34;https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/&#34;&gt;El nodo k3 del clúster colgado de nuevo&lt;/a&gt;), pasadas unas horas los dos nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; han dejado de responder de nuevo.&lt;/p&gt;

&lt;p&gt;Así que es el momento de atacar el problema de forma algo más sistemática. Para ello seguiré las instrucciones que proporcina la página de Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/&#34;&gt;Troubleshooting Clusters&lt;/a&gt;.
&lt;/p&gt;

&lt;h2 id=&#34;descripción-del-problema&#34;&gt;Descripción del problema&lt;/h2&gt;

&lt;p&gt;Tras unas horas activos y formando parte del clúster, los dos nodos que corren sobre Raspberry Pi 3 dejan de responder y el clúster los muestra como &lt;em&gt;NotReady&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;El clúster está formado por tres Raspberry Pi; el nodo &lt;em&gt;master&lt;/em&gt; es una Raspberry Pi 2 B mientras que los dos nodos &lt;em&gt;worker&lt;/em&gt; son Raspberry Pi 3 B.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes -o wide
NAME      STATUS     AGE       VERSION   EXTERNAL-IP   OS-IMAGE                        KERNEL-VERSION
k1        Ready      19d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
k2        NotReady   15d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
k3        NotReady   14d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ping&#34;&gt;Ping&lt;/h2&gt;

&lt;h3 id=&#34;ping-al-nombre-del-nodo&#34;&gt;Ping al nombre del nodo&lt;/h3&gt;

&lt;p&gt;La prueba más sencilla para ver si los nodos están colgados, es lanzar un ping desde el portátil:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ping -c5 k2.local
ping: cannot resolve k2.local: Unknown host
$ ping -c5 k3.local
ping: cannot resolve k3.local: Unknown host
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En esta prueba vemos que ninguno de los nodos responde al nombre que &lt;em&gt;publica&lt;/em&gt; el servicio &lt;a href=&#34;https://en.wikipedia.org/wiki/Avahi_(software)&#34;&gt;Avahi&lt;/a&gt; en el sistema.&lt;/p&gt;

&lt;h3 id=&#34;ping-a-la-ip-del-nodo&#34;&gt;Ping a la IP del nodo&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ping -c5 192.168.1.12
PING 192.168.1.12 (192.168.1.12): 56 data bytes
64 bytes from 192.168.1.12: icmp_seq=0 ttl=64 time=3.842 ms
64 bytes from 192.168.1.12: icmp_seq=1 ttl=64 time=6.678 ms
64 bytes from 192.168.1.12: icmp_seq=2 ttl=64 time=10.789 ms
64 bytes from 192.168.1.12: icmp_seq=3 ttl=64 time=7.411 ms
64 bytes from 192.168.1.12: icmp_seq=4 ttl=64 time=10.518 ms

--- 192.168.1.12 ping statistics ---
5 packets transmitted, 5 packets received, 0.0% packet loss
round-trip min/avg/max/stddev = 3.842/7.848/10.789/2.584 ms
$ ping -c5 192.168.1.13
PING 192.168.1.13 (192.168.1.13): 56 data bytes
Request timeout for icmp_seq 0
Request timeout for icmp_seq 1
Request timeout for icmp_seq 2
Request timeout for icmp_seq 3

--- 192.168.1.13 ping statistics ---
5 packets transmitted, 0 packets received, 100.0% packet loss
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En este caso, el nodo &lt;strong&gt;k2&lt;/strong&gt; sí que responde a ping a la IP, mientras que el nodo &lt;strong&gt;k3&lt;/strong&gt; actúa como si estuviera apagado o con la red deshabilitada.&lt;/p&gt;

&lt;h3 id=&#34;ssh&#34;&gt;SSH&lt;/h3&gt;

&lt;p&gt;Aunque el nodo &lt;strong&gt;k2&lt;/strong&gt; responde a ping, no es posible conectar vía SSH; el intento de conectar no tiene éxito, pero tampoco falla (por &lt;em&gt;timeout&lt;/em&gt;, por ejemplo). He probado a conectar tanto desde el portátil como desde el nodo &lt;strong&gt;k1&lt;/strong&gt;, con el mismo resulado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ssh pirate@192.168.1.12

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubelet-describe-node&#34;&gt;kubelet describe node&lt;/h2&gt;

&lt;p&gt;Usamos el comando &lt;code&gt;kubelet describe node&lt;/code&gt; para los dos nodos colgados.&lt;/p&gt;

&lt;h3 id=&#34;nodo-k2&#34;&gt;Nodo &lt;strong&gt;k2&lt;/strong&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k2
Name:			k2
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k2
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			&amp;lt;none&amp;gt;
CreationTimestamp:	Sat, 15 Apr 2017 10:25:31 +0000
Phase:
Conditions:
  Type			Status		LastHeartbeatTime			LastTransitionTime			Reason			Message
  ----			------		-----------------			------------------			------			-------
  OutOfDisk 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  MemoryPressure 	Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  DiskPressure 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  Ready 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
Addresses:		192.168.1.12,192.168.1.12,k2
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			84bf8a2b-b83f-445b-a4b3-250dc6e5db40
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.2.0/24
ExternalID:			k2
Non-terminated Pods:		(2 in total)
  Namespace			Name				CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----				------------	----------	---------------	-------------
  kube-system			kube-proxy-g580s		0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-kxpk6			20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  20m (0%)	0 (0%)		0 (0%)		0 (0%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodo-k3&#34;&gt;Nodo &lt;strong&gt;k3&lt;/strong&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k3
Name:			k3
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k3
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			&amp;lt;none&amp;gt;
CreationTimestamp:	Sat, 15 Apr 2017 14:10:06 +0000
Phase:
Conditions:
  Type			Status		LastHeartbeatTime			LastTransitionTime			Reason			Message
  ----			------		-----------------			------------------			------			-------
  OutOfDisk 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  MemoryPressure 	Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  DiskPressure 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  Ready 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
Addresses:		192.168.1.13,192.168.1.13,k3
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			23bf96e4-ec65-489c-be00-d0fa848265f3
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.3.0/24
ExternalID:			k3
Non-terminated Pods:		(2 in total)
  Namespace			Name				CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----				------------	----------	---------------	-------------
  kube-system			kube-proxy-bkl4g		0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-3bf40			20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  20m (0%)	0 (0%)		0 (0%)		0 (0%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El nodo &lt;strong&gt;k2&lt;/strong&gt; deja de responder a las 11:56:26, mientras que el &lt;strong&gt;k3&lt;/strong&gt; lo hace a las 10:33:45.&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Cuando reinicie los dos nodos lo haré a la misma hora, para comprobar si hay diferencias en el tiempo que tarda en dejar de responder cada nodo.&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Actualizar la zona horaria de las Raspberry Pi.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;nodo-k1-ready&#34;&gt;Nodo &lt;strong&gt;k1&lt;/strong&gt; (&lt;code&gt;Ready&lt;/code&gt;)&lt;/h3&gt;

&lt;p&gt;Como referencia, incluimos el mismo comando para el nodo &lt;em&gt;master&lt;/em&gt; &lt;strong&gt;k1&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k1
Name:			k1
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k1
			node-role.kubernetes.io/master=
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			node-role.kubernetes.io/master:NoSchedule
CreationTimestamp:	Mon, 10 Apr 2017 20:22:32 +0000
Phase:
Conditions:
  Type			Status	LastHeartbeatTime			LastTransitionTime			Reason				Message
  ----			------	-----------------			------------------			------				-------
  OutOfDisk 		False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasSufficientDisk 	kubelet has sufficient disk space available
  MemoryPressure 	False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasSufficientMemory 	kubelet has sufficient memory available
  DiskPressure 		False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasNoDiskPressure 	kubelet has no disk pressure
  Ready 		True 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:20 +0000 	KubeletReady 			kubelet is posting ready status
Addresses:		192.168.1.11,192.168.1.11,k1
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			55e1fad0-d40c-480b-b039-5586ff728d2c
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.0.0/24
ExternalID:			k1
Non-terminated Pods:		(7 in total)
  Namespace			Name					CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----					------------	----------	---------------	-------------
  kube-system			etcd-k1					0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-apiserver-k1			250m (6%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-controller-manager-k1		200m (5%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-dns-279829092-1b27r		260m (6%)	0 (0%)		110Mi (14%)	170Mi (22%)
  kube-system			kube-proxy-3dggd			0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-scheduler-k1			100m (2%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-6qr0l				20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  830m (20%)	0 (0%)		110Mi (14%)	170Mi (22%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;revisando-los-logs-en-el-nodo-master&#34;&gt;Revisando los logs en el nodo &lt;em&gt;master&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;En la guía de &lt;em&gt;Troubleshooting&lt;/em&gt; de Kubernetes, el siguiente paso es revisar los logs. En el caso del nodo &lt;em&gt;master&lt;/em&gt;, los logs relevantes se encuentran en:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-apiserver.log&lt;/code&gt; - El &lt;em&gt;API Server&lt;/em&gt;, encargado de servir la API&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-scheduler.log&lt;/code&gt; - El &lt;em&gt;Scheduler&lt;/em&gt;, encargado de las decisiones de planificar los &lt;em&gt;pods&lt;/em&gt; en los nodos&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-controller-manager.log&lt;/code&gt; - El responsable de gestionar los &lt;em&gt;replication controllers&lt;/em&gt; encargados de mantener el &lt;strong&gt;estado deseado&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sin embargo, los logs indicados &lt;strong&gt;no existen en la ruta indicada&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls /var/log/kube*
ls: cannot access /var/log/kube*: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es probable que la documentación no esté actualizada, así que continuaré en cuanto encuentre los logs para poder revisarlos.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Errores sobre Orphaned pods en syslog</title>
      <link>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</link>
      <pubDate>Sun, 30 Apr 2017 12:55:44 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</guid>
      <description>&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; del clúster dejan de responder pasadas unas horas. La única manera de solucionarlo es reiniciar los nodos. Siguiendo con la revisión de logs, he encontrado que se genera una gran cantidad de entradas en &lt;em&gt;syslog&lt;/em&gt; en referencia a &lt;em&gt;orphaned pods&lt;/em&gt;. Además, el número de estos errores no para de crecer &lt;strong&gt;rápidamente&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
118938
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119022
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119170
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Revisando las últimas entradas del log:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-log&#34;&gt;Apr 30 10:57:57 k2 kubelet[3619]: E0430 10:57:57.186318    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;5064b9d9-2c9e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:01 k2 kubelet[3619]: E0430 10:58:01.759595    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;6c601e9c-2c9c-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.226372    3619 kubelet.go:1549] Unable to mount volumes for pod &amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]; skipping pod
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.238315    3619 pod_workers.go:182] Error syncing pod 4461d51d-2d93-11e7-a7ae-b827eb650fdb (&amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;), skipping: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]
Apr 30 10:58:05 k2 kubelet[3619]: E0430 10:58:05.830432    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;bb4d3ea6-2b80-11e7-9388-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:08 k2 kubelet[3619]: E0430 10:58:08.435567    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;cb23be0d-2d7e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Todas estas entradas se encuentran en los logs del nodo &lt;strong&gt;k2&lt;/strong&gt;, donde no hay ningún &lt;em&gt;pod&lt;/em&gt; en ejecución (a parte de los propios de  Kubernetes que el clúster planifica en los diferentes nodos).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>El nodo k3 del clúster colgado de nuevo</title>
      <link>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</link>
      <pubDate>Sun, 30 Apr 2017 11:39:20 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/&#34;&gt;Múltiples mensajes &amp;lsquo;action 17 suspended&amp;rsquo; en los logs&lt;/a&gt; comentaba que estaba a la espera de obtener resultados; después de apenas unas horas, ya los tengo: &lt;strong&gt;k3&lt;/strong&gt; se ha vuelto a &lt;em&gt;colgar&lt;/em&gt; mientras que &lt;strong&gt;k2&lt;/strong&gt; no.&lt;/p&gt;

&lt;p&gt;Este resultado parece demostrar que la mala configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la causante de los &lt;em&gt;cuelgues&lt;/em&gt; de las RPi 3 en el clúster de Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: El nodo &lt;strong&gt;k2&lt;/strong&gt; sobre RPi3 sigue colgándose :(&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización II&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Parece solucionado&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;A modo de recordatorio, los cambios realizados en los dos nodos sobre Raspberry Pi 3 han sido (incluyo el nodo &lt;strong&gt;k1&lt;/strong&gt; con RPi2):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                                |  k1  |  k2  |  k3  |
                                | RPi2 | RPi3 | RPi3 |
 -------------------------------|------|------|------|
| Modificada conf. de rsyslog   |  No  |  Sí  |  No  |
| Actualización a versión 1.6.2 |  Sí  |  Sí  |  Sí  |
 ----------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hasta ahora, los únicos nodos que se &lt;em&gt;colgaban&lt;/em&gt; eran el &lt;strong&gt;k2&lt;/strong&gt; y el &lt;strong&gt;k3&lt;/strong&gt; (sobre RPi3).&lt;/p&gt;

&lt;p&gt;Al modificar la configuración en de &lt;em&gt;rsyslog&lt;/em&gt; en &lt;strong&gt;k2&lt;/strong&gt; y pasadas unas horas, el único nodo que se sigue colgando es el &lt;strong&gt;k3&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k1        Ready      19d       v1.6.2
k2        Ready      14d       v1.6.2
k3        NotReady   14d       v1.6.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es decir, el fallo a la hora de redirigir los mensajes a &lt;code&gt;/dev/xconsole&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sólo afectan a las RPi3&lt;/li&gt;
&lt;li&gt;provoca que el sistema se acabe colgando&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para solucionarlo, como &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Abre &lt;code&gt;/etc/rsyslog.conf&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Modifica las últimas líneas (al final del fichero) y coméntalas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;# The named pipe /dev/xconsole is for the `xconsole&#39; utility.  To use it,
# you must invoke `xconsole&#39; with the `-file&#39; option:
#
#    $ xconsole -file /dev/xconsole [...]
#
# NOTE: adjust the list below, or you&#39;ll go crazy if you have a reasonably
#      busy site..
#
daemon.*;mail.*;\
    news.err;\
    *.=debug;*.=info;\
    *.=notice;*.=warn       |/dev/xconsole
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;deben quedar como:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;#daemon.*;mail.*;\
#        news.err;\
#        *.=debug;*.=info;\
#        *.=notice;*.=warn       |/dev/xconsole
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podrías comentar la redirección &lt;code&gt;|/dev/xconsole&lt;/code&gt;, pero en este caso el bloque no tendría ninguna funcionalidad, por lo que creo que es &lt;em&gt;más limpio&lt;/em&gt; comentar todo el bloque.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reinicia el equipo mediante &lt;code&gt;reboot&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Múltiples mensajes &#39;action 17 suspended&#39; en los logs</title>
      <link>https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/</link>
      <pubDate>Sun, 30 Apr 2017 08:44:27 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/</guid>
      <description>&lt;p&gt;Investigando las causas por las que los dos nodos con Raspberry Pi 3 se &lt;em&gt;cuelgan&lt;/em&gt;, he encontrado múltiples apariciones de este mensaje en &lt;code&gt;/var/log/messages&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-log&#34;&gt;Apr 30 06:40:42 k3 rsyslogd-2007: action &#39;action 17&#39; suspended, next retry is Sun Apr 30 06:41:12 2017 [try http://www.rsyslog.com/e/2007 ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;De hecho, revisando el origen del problema he encontrado este comando que cuenta las apariciones del mensaje:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo grep &amp;quot;action.*suspend&amp;quot; /var/log/messages | wc -l
1394
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además de &lt;em&gt;spamear&lt;/em&gt; los logs, provoca un montón de escrituras innecesarias sobre la tarjeta microSD, lo que puede acortar la vida útil de la misma.&lt;/p&gt;

&lt;p&gt;No tengo claro si este puede ser la causa que hace que las dos Raspberry Pi 3 se cuelguen pasado un tiempo y que dejen de responder, lo que hace que deba reiniciarlas (desconectando/conectando el cable) para recuperarlas. Sin embargo, en el nodo &lt;em&gt;master&lt;/em&gt; (Raspberry Pi 2 B+) no aparece el mensaje en los logs y no se cuelga (aunque la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la misma).&lt;/p&gt;

&lt;h1 id=&#34;solución-a-los-mensajes-de-rsyslog&#34;&gt;Solución a los mensajes de rsyslog&lt;/h1&gt;

&lt;p&gt;El mensaje de error , es un problema de configuración de la aplicación &lt;code&gt;rsyslog&lt;/code&gt;, que intenta mostrar mensajes en &lt;code&gt;/dev/xconsole&lt;/code&gt;, pero falla.&lt;/p&gt;

&lt;p&gt;La solución la explica Danny Tuppeny en su blog, en &lt;a href=&#34;https://blog.dantup.com/2016/04/removing-rsyslog-spam-on-raspberry-pi-raspbian-jessie/&#34;&gt;Removing [action &amp;lsquo;action 17&amp;rsquo; suspended] rsyslog Spam on Raspberry Pi (Raspian Jessie)&lt;/a&gt;. Él mismo abrió un &lt;em&gt;bug&lt;/em&gt; en RPI-Distro: &lt;a href=&#34;https://github.com/RPi-Distro/repo/issues/28&#34;&gt;Default Raspbian Jessie Lite install spams syslog with &amp;ldquo;rsyslogd-2007: action &amp;lsquo;action 17&amp;rsquo; suspended, next retry is #28&lt;/a&gt; en él explicaba cómo había eliminado la línea que hace referencia a &lt;code&gt;/dev/xconsole&lt;/code&gt; de la configuración de &lt;em&gt;rasyslog&lt;/em&gt; y que el mensaje desaparecía (después de reiniciar).&lt;/p&gt;

&lt;p&gt;Para comprobar si esta es la causa del &lt;em&gt;cuelgue&lt;/em&gt; de la RPi 3, he modificado la configuración de &lt;em&gt;rsyslog&lt;/em&gt; en el nodo &lt;strong&gt;k2&lt;/strong&gt; del clúster, pero no en el &lt;strong&gt;k3&lt;/strong&gt;. De esta forma podré averiguar si la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la causante del &lt;em&gt;cuelgue&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;También he actualizado el sistema (en todos los nodos) y &lt;em&gt;kubelet&lt;/em&gt;, &lt;em&gt;kubectl&lt;/em&gt; y &lt;em&gt;kubeadm&lt;/em&gt; se han actualizado a la versión 1.6.2:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
Setting up libldap-2.4-2:armhf (2.4.40+dfsg-1+deb8u2) ...
Setting up libicu52:armhf (52.1-8+deb8u5) ...
Setting up kubelet (1.6.2-00) ...
Setting up kubectl (1.6.2-00) ...
Setting up kubeadm (1.6.2-00) ...
Processing triggers for libc-bin (2.19-18+deb8u7)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k1        Ready     19d       v1.6.2
k2        Ready     14d       v1.6.2
k3        Ready     14d       v1.6.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo queda esperar -normalmente unas cuantas horas- a ver qué pasa: hay tres posibilidades:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Se cuelga &lt;strong&gt;k3&lt;/strong&gt; pero no &lt;strong&gt;k2&lt;/strong&gt;: La configuración de &lt;em&gt;rsyslog&lt;/em&gt; era la causa.&lt;/li&gt;
&lt;li&gt;Se cuelga &lt;strong&gt;k2&lt;/strong&gt; pero no &lt;strong&gt;k3&lt;/strong&gt;: ¿?&lt;/li&gt;
&lt;li&gt;No se cuelga ni &lt;strong&gt;k2&lt;/strong&gt; ni &lt;strong&gt;k3&lt;/strong&gt;: Era un problema de alguno de los componetes actualizados que sólo afecta a la RPi 3.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Informaré en cuanto tenga resultados.&lt;/p&gt;

&lt;h1 id=&#34;unas-horas-después&#34;&gt;Unas horas después&amp;hellip;&lt;/h1&gt;

&lt;p&gt;Ya tenngo resultados: la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la que causa el cuelgue del sistema en las RPi3.&lt;/p&gt;

&lt;p&gt;Échale un vistazo a cómo solucionar este error en la entrada: &lt;a href=&#34;https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/&#34;&gt;El nodo k3 del clúster colgado de nuevo&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Portainer para gestionar tus contenedores en Docker</title>
      <link>https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/</link>
      <pubDate>Sat, 29 Apr 2017 12:55:04 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://portainer.io/&#34;&gt;Portainer&lt;/a&gt; es una herramienta ligera y &lt;em&gt;open-source&lt;/em&gt; de gestión de contenedores sobre Docker (o Docker Swarm). Portainer ofrece una interfaz gráfica para gestionar el &lt;em&gt;host&lt;/em&gt; Docker desde cualquier navegador, tiene soporte para Raspberry Pi y se puede desplegar como un simple contenedor.&lt;/p&gt;

&lt;p&gt;Espero que este artículo ayude a todos aquellos que tengan ganas de probar Portainer y evitarles los problemas que me he encontrado yo.&lt;/p&gt;

&lt;p&gt;
&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/portainer-logo.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/portainer-logo.png&#34; width=1106 height=361 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;He estado buscando algún tipo de solución gráfica para monitorizar las Raspberry Pi ya que, por algún motivo, los nodos &lt;em&gt;worker&lt;/em&gt; del clúster de Kubernetes se &lt;em&gt;cuelgan&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Buscando alguna solución de monitorizado he encontrado Portainer referenciado en el blog de Hypriot: &lt;a href=&#34;https://blog.hypriot.com/post/new-docker-ui-portainer/&#34;&gt;Visualize your Raspberry Pi containers with Portainer or UI for Docker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Portainer no es una herramienta de monitorizado (a nivel de &lt;em&gt;host&lt;/em&gt;), sino que está enfocada a la &lt;em&gt;visualización&lt;/em&gt; básicamente del estado de los contenedores de uno (o varios) &lt;em&gt;endpoints&lt;/em&gt; Docker (o Docker Swarm). Sin embargo, ofreciendo soporte para ARM y estando disponible en forma de contenedor, no había motivo para no probarlo ;)&lt;/p&gt;

&lt;h1 id=&#34;soporte-para-arm&#34;&gt;Soporte para ARM&lt;/h1&gt;

&lt;p&gt;En el apartado para obtener Portainer de la web, sólo se indica el comando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d -p 9000:9000 portainer/portainer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Por muy minimalista que quiera ser la página, la verdad es que no les hubiera costado nada indicar que existen diferentes versiones disponibles de la imagen (como por ejemplo, la que proporciona soporte para ARM).&lt;/p&gt;

&lt;p&gt;Además, lanzando el comando &lt;em&gt;tal cual&lt;/em&gt;, si quieres configurar Portainer para monitorizar el nodo &lt;em&gt;local&lt;/em&gt;, &lt;strong&gt;no funcionará&lt;/strong&gt; (requiere montar &lt;code&gt;/var/run/docker.sock&lt;/code&gt; en el contenedor).&lt;/p&gt;

&lt;p&gt;El artículo de Hypriot apunta a una imagen llamada &lt;code&gt;portainer/portainer:arm&lt;/code&gt;, que ya no existe en DockerHub. Revisando las &lt;a href=&#34;https://hub.docker.com/r/portainer/portainer/tags/&#34;&gt;etiquetas disponibles para las imágenes de Portainer&lt;/a&gt;, encontramos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;TagName				Compressed Size 	Last Updated 
ppc64le				4 MB			16 days ago
demo				4 MB			23 days ago
latest				0 B 			23 days ago
1.12.4				0 B 			23 days ago
windows-amd64 			337 MB			23 days ago
windows-amd64-1.12.4	 	337 MB			23 days ago
linux-arm64 			4 MB			23 days ago
linux-arm64-1.12.4 		4 MB			23 days ago
linux-arm 			4 MB			23 days ago
linux-arm-1.12.4 		4 MB			23 days ago
linux-amd64 			4 MB			23 days ago
linux-amd64-1.12.4 		4 MB			23 days ago
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seleccionamos la versión adecuada para nuestra Raspberry Pi y la descargamos mediante:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker pull portainer/portainer:linux-arm-1.12.4
linux-arm-1.12.4: Pulling from portainer/portainer
a3ed95caeb02: Pull complete
802d894958a2: Pull complete
30fb5c96d238: Pull complete
Digest: sha256:5269fd824014fac1dee29e2cf74aa5c337cf5c0ceb7cae2634c1e054f5e2763f
Status: Downloaded newer image for portainer/portainer:linux-arm-1.12.4
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación he lanzado la creación del contenedor usando:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker run -d -p 9000:9000 --name portainer portainer/portainer:linux-arm-1.12.4
d5ad5764788a932cd19942dcb0e70471101173c8d14801b0ce7c172ef9ac72ff
$
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;acceso-a-portainer&#34;&gt;Acceso a Portainer&lt;/h2&gt;

&lt;p&gt;Abre un navegador y accede a &lt;code&gt;http://IP-nodo:9000/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;La primera vez que accedes a la URL de Portainer debes introducir el password del usuario &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-1-define-admin-password.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-1-define-admin-password.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Una vez introducido, puedes acceder a la UI de gestión de Portainer.&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-2-first-login.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-2-first-login.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Para mostrar información sobre los contendores (imágenes, volúmenes, etc) en Docker, Portainer necesita conectarse -vía API- al &lt;em&gt;host&lt;/em&gt; en el que corre Docker. Tenemos dos opciones, un &lt;em&gt;endpoint remoto&lt;/em&gt; (opción por defecto) o conectar con el &lt;em&gt;host&lt;/em&gt; donde corre Portainer:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-3-remote_endpoint_by_default.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-3-remote_endpoint_by_default.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;El problema es que, como vemos, al seleccionar un &lt;em&gt;endpoint&lt;/em&gt; local, se indica que hay que lanzar el contenedor de Portainer dando acceso al contenedor sobre &lt;code&gt;/var/run/docker.sock&lt;/code&gt;:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-4-local_endpoint_require_docker.sock.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-4-local_endpoint_require_docker.sock.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Como este &lt;em&gt;detalle&lt;/em&gt; no se indica en ningún sitio hasta que estás intentando configurar Portainer, lo más probable es que no hayas lanzado el contenedor con el volumen necesario.&lt;/p&gt;

&lt;p&gt;Así que es necesario detener el contenedor -y eliminarlo, si quieres reusar el nombre- y volver a lanzar el proceso de configuración.&lt;/p&gt;

&lt;p&gt;No son más que unos pocos comandos en Linux (o en tu Mac), pero sin duda es una molestia que podría evitarse dando algo más de información. Mucho más grave es si el sistema operativo de tu &lt;em&gt;host&lt;/em&gt; es Windows, ya que &lt;strong&gt;esta opción no está disponible&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker stop portainer
portainer
$ docker rm portainer
portainer
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;acceso-a-portainer-segundo-intento&#34;&gt;Acceso a Portainer (segundo intento)&lt;/h2&gt;

&lt;p&gt;Lanzamos el contenedor de Portainer montando &lt;code&gt;docker.sock&lt;/code&gt; y pasamos por los mismos pasos que en intento anterior:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ docker run -d -p 9000:9000 --name portainer -v &amp;quot;/var/run/docker.sock:/var/run/docker.sock&amp;quot; portainer/portainer:linux-arm-1.12.4
3f0ad98393ed5c67cda864737d83fe098a13d1317e1f6c299419ab1a3c1d153c
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de validarnos, podemos conectar con el &lt;em&gt;docker-engine&lt;/em&gt; local y visualizar el &lt;em&gt;dashboard&lt;/em&gt;:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-5-dashboard.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-5-dashboard.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
&lt;/figure&gt;


&lt;p&gt;Desde la interfaz web podemos gestionar los contenedores, imágenes y volúmenes existentes:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-6-containers.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-6-containers.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Contenedores.
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-7-images.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-7-images.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Imágenes.
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170429/portainer-8-volumes.png&#34; alt=&#34;Portainer para gestionar tus contenedores en Docker images/170429/portainer-8-volumes.png&#34; width=640 height=400 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Volúmenes.
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;En el próximo artículo me concentraré en usar &lt;a href=&#34;https://onthedock.github.io/tags/portainer/&#34;&gt;Portainer&lt;/a&gt; para realizar la gestión de Docker.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Solución al error de instalación de Kubernetes en Debian Jessie (Missing cgroups: memory)</title>
      <link>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</link>
      <pubDate>Sat, 22 Apr 2017 07:57:14 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</guid>
      <description>&lt;p&gt;Al lanzar la inicialización del clúster con &lt;code&gt;kubeadm init&lt;/code&gt; en Debian Jessie, las comprobaciones inciales indican que no se encuentran los &lt;em&gt;cgroups&lt;/em&gt; para la memoria (échale un vistazo al artículo &lt;a href=&#34;https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/&#34;&gt;La instalación de Kubernetes falla en Debian Jessie (missing cgroups: memory)&lt;/a&gt;). Los &lt;em&gt;cgroups&lt;/em&gt; son una de las piezas fundamentales en las que se basa Docker para &lt;em&gt;aislar&lt;/em&gt; los procesos de los contenedores, por lo que la inicialización del clúster de Kubernetes se detiene.&lt;/p&gt;

&lt;p&gt;La solución es tan sencilla como habilitar los &lt;em&gt;cgroups&lt;/em&gt; durante el arranque.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;En primer lugar, verificamos la versión del &lt;em&gt;kernel&lt;/em&gt; que tenemos instalada:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# uname -a
Linux k8s 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al lanzar &lt;code&gt;kubeadm init&lt;/code&gt; obtenemos el error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubeadm init
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
OS: Linux
KERNEL_VERSION: 3.16.0-4-amd64
CONFIG_NAMESPACES: enabled
CONFIG_NET_NS: enabled
CONFIG_PID_NS: enabled
CONFIG_IPC_NS: enabled
CONFIG_UTS_NS: enabled
CONFIG_CGROUPS: enabled
CONFIG_CGROUP_CPUACCT: enabled
CONFIG_CGROUP_DEVICE: enabled
CONFIG_CGROUP_FREEZER: enabled
CONFIG_CGROUP_SCHED: enabled
CONFIG_CPUSETS: enabled
CONFIG_MEMCG: enabled
CONFIG_INET: enabled
CONFIG_EXT4_FS: enabled (as module)
CONFIG_PROC_FS: enabled
CONFIG_NETFILTER_XT_TARGET_REDIRECT: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled (as module)
CONFIG_OVERLAYFS_FS: not set - Required for overlayfs.
CONFIG_AUFS_FS: enabled (as module)
CONFIG_BLK_DEV_DM: enabled (as module)
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: missing
DOCKER_VERSION: 17.04.0-ce
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[preflight] Some fatal errors occurred:
	missing cgroups: memory
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;solución&#34;&gt;Solución&lt;/h2&gt;

&lt;p&gt;La solución la he encontrado en &lt;a href=&#34;https://phabricator.wikimedia.org/T122734&#34;&gt;Enable memory cgroups for default Jessie image&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Nos convertimos en &lt;em&gt;root&lt;/em&gt;: &lt;code&gt;sudo su -&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Editamos el fichero &lt;code&gt;/etc/default/grup&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;En los parámetros para el arranque de linux (&lt;code&gt;GRUB_CMDLINE_LINUX&lt;/code&gt;) añadimos &lt;code&gt;cgroup_enable=memory&lt;/code&gt;. En mi caso, la línea queda: &lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet cgroup_enable=memory&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Actualizamos &lt;em&gt;grub&lt;/em&gt;: &lt;code&gt;update-grub2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Reiniciamos la máquina: &lt;code&gt;reboot&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[preflight] Some fatal errors occurred:
	missing cgroups: memory
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
root@k8s:~# nano /etc/default/grub
root@k8s:~# update-grub2
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.16.0-4-amd64
Found initrd image: /boot/initrd.img-3.16.0-4-amd64
done
root@k8s:~# reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Al lanzar &lt;code&gt;kubeadm init&lt;/code&gt; de nuevo, el clúster arranca con normalidad:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;root@k8s:~# kubeadm init
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] API Server serving cert is signed for DNS names [k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.99]
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key and public key.
[certificates] Generated front-proxy CA certificate and key.
[certificates] Generated front-proxy client certificate and key.
[certificates] Valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/scheduler.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;/etc/kubernetes/controller-manager.conf&amp;quot;
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 39.363656 seconds
[apiclient] Waiting for at least one node to register
[apiclient] First node has registered after 1.518215 seconds
[token] Using token: fe9e91.7142118e712eb019
[apiconfig] Created RBAC rules
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  sudo cp /etc/kubernetes/admin.conf $HOME/
  sudo chown $(id -u):$(id -g) $HOME/admin.conf
  export KUBECONFIG=$HOME/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token fe9e91.7142118e712eb019 192.168.1.99:6443

root@k8s:~#
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>La instalación de Kubernetes falla en Debian Jessie (Missing cgroups: memory)</title>
      <link>https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/</link>
      <pubDate>Mon, 17 Apr 2017 19:38:11 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/</guid>
      <description>&lt;p&gt;La instalación de Kubernetes se realiza de forma casi automática gracias al &lt;em&gt;script&lt;/em&gt; &lt;code&gt;kubeadm&lt;/code&gt;. Sólo hay que seguir las instrucciones de &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;Installing Kubernetes on Linux with kubeadm&lt;/a&gt; y la salida por pantalla del propio &lt;em&gt;script&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Para no &lt;em&gt;deshacer&lt;/em&gt; la instalación de Kubernetes sobre Raspberry Pi, he creado una máquina virtual con Debian 3 (&lt;em&gt;Jessie&lt;/em&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# uname -a
Linux k8s 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En primer lugar, me he convertido en &lt;code&gt;root&lt;/code&gt; mediante &lt;code&gt;su -&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Uno de los requisitos para instalar Kubernetes es tener Docker instalado. En mi caso, tengo instalado &lt;code&gt;docker-engine&lt;/code&gt;, el paquete de Docker mantenido por Docker Inc. Este paquete y &lt;code&gt;docker.io&lt;/code&gt; (el mantenido por Ubuntu) son equivalentes, aunque tienen numeración de versión diferente.&lt;/p&gt;

&lt;p&gt;Verifico que Docker está instalado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# docker version
Client:
 Version:      17.04.0-ce
 API version:  1.28
 Go version:   go1.7.5
 Git commit:   4845c56
 Built:        Mon Apr  3 17:45:49 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.04.0-ce
 API version:  1.28 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   4845c56
 Built:        Mon Apr  3 17:45:49 2017
 OS/Arch:      linux/amd64
 Experimental: false
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Antes de empezar la instalación, he actualizado el sistema mediante:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# apt-get update &amp;amp;&amp;amp; apt-get upgrade
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A partir de aquí, sigo las instrucciones de la guía oficial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# apt-get update &amp;amp;&amp;amp; apt-get install -y apt-transport-https
...
Reading state information... Done
apt-transport-https is already the newest version.
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obtenemos la clave GPG de los paquetes de Kubernetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Añadimos el repositorio de Kubernetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Y ya sólo nos queda actualizar la información e instalar los componentes de Kubernetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# apt-get update
...
Fetched 22.1 kB in 3s (6,784 B/s)
Reading package lists... Done
#
# apt-get install -y kubelet kubeadm kubectl kubernetes-cni
...
Setting up ebtables (2.0.10.4-3) ...
update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults
Setting up ethtool (1:3.16-1) ...
Setting up kubernetes-cni (0.5.1-00) ...
Setting up socat (1.7.2.4-2) ...
Setting up kubelet (1.6.1-00) ...
Setting up kubectl (1.6.1-00) ...
Setting up kubeadm (1.6.1-00) ...
Processing triggers for systemd (215-17+deb8u6) ...
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ya tenemos instalado Kubernetes en nuestro sistema.&lt;/p&gt;

&lt;p&gt;El siguiente paso es inicializar el clúster con &lt;code&gt;kubeadm init&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# kubeadm init
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
OS: Linux
KERNEL_VERSION: 3.16.0-4-amd64
CONFIG_NAMESPACES: enabled
CONFIG_NET_NS: enabled
CONFIG_PID_NS: enabled
CONFIG_IPC_NS: enabled
CONFIG_UTS_NS: enabled
CONFIG_CGROUPS: enabled
CONFIG_CGROUP_CPUACCT: enabled
CONFIG_CGROUP_DEVICE: enabled
CONFIG_CGROUP_FREEZER: enabled
CONFIG_CGROUP_SCHED: enabled
CONFIG_CPUSETS: enabled
CONFIG_MEMCG: enabled
CONFIG_INET: enabled
CONFIG_EXT4_FS: enabled (as module)
CONFIG_PROC_FS: enabled
CONFIG_NETFILTER_XT_TARGET_REDIRECT: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled (as module)
CONFIG_OVERLAYFS_FS: not set - Required for overlayfs.
CONFIG_AUFS_FS: enabled (as module)
CONFIG_BLK_DEV_DM: enabled (as module)
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: missing
DOCKER_VERSION: 17.04.0-ce
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.04.0-ce. Max validated version: 1.12
[preflight] WARNING: hostname &amp;quot;k8s&amp;quot; could not be reached
[preflight] WARNING: hostname &amp;quot;k8s&amp;quot; lookup k8s on 80.58.61.254:53: no such host
[preflight] Some fatal errors occurred:
	missing cgroups: memory
[preflight] If you know what you are doing, you can skip pre-flight checks with `--skip-preflight-checks`
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La instalación de Kubernetes falla porque no están habilitados los &lt;em&gt;cgroups&lt;/em&gt; para la memoria. :(&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>