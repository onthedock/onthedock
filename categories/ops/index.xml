<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ops on On The Dock</title>
    <link>https://onthedock.github.io/categories/ops/index.xml</link>
    <description>Recent content in Ops on On The Dock</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Handmade with &amp;#9829; by Xavi Aznar</copyright>
    <atom:link href="https://onthedock.github.io/categories/ops/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>IP en mensaje prelogin</title>
      <link>https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/</link>
      <pubDate>Sun, 02 Jul 2017 22:07:18 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/</guid>
      <description>&lt;p&gt;En la pantalla de &lt;em&gt;login&lt;/em&gt; en modo consola de los sistemas Linux se muestra un mensaje de bienvenida.&lt;/p&gt;

&lt;p&gt;En este artículo se muestra cómo hacer que se muestre la IP del equipo.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Cuando tenemos una máquina (virtual) configurada para obtener la IP de forma dinámica mediante DHCP, al desconocer por adelantado la IP que se le ha asignado, es necesario conectarse al hipervisor, hacer login en la máquina virtual para, finalmente, obtener la IP &lt;em&gt;actual&lt;/em&gt; de la VM.&lt;/p&gt;

&lt;p&gt;Entonces podemos conectarnos &lt;em&gt;remotamente&lt;/em&gt; usando PuTTY (desde Windows) o un emulador de terminal desde Linux/OSX.&lt;/p&gt;

&lt;p&gt;Sin embargo, hay una manera de agilizar este proceso, aprovechando el mensaje que se muestra antes del login.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Aunque en este caso he usado Alpine Linux, las instrucciones son igualmente válidas para la mayoría de distribuciones.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En primer lugar necesitamos ejecutar un &lt;em&gt;script&lt;/em&gt; durante el arranque del sistema operativo. En el caso concreto de Alpine Linux, he encontrado la solución en &lt;a href=&#34;https://forum.alpinelinux.org/forum/general-discussion/run-script-boot&#34;&gt;run script on boot&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Ya que no vamos a escribir nuestro propio servicio, usaremos el servicio &lt;em&gt;local&lt;/em&gt;. Para ello, hay que añadir nuestro &lt;em&gt;script&lt;/em&gt; en la carpeta &lt;code&gt;/etc/local.d/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;rc-update add local default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El &lt;code&gt;README&lt;/code&gt; ubicado en &lt;code&gt;/etc/local.d/README&lt;/code&gt; indica que cualquier fichero ejecutable con extensión &lt;code&gt;.start&lt;/code&gt; se lanza al arrancar el servicio, mientras que si la extensión es &lt;code&gt;.stop&lt;/code&gt;, se ejecuta al parar el servicio.&lt;/p&gt;

&lt;p&gt;En mi caso, he usado el &lt;em&gt;script&lt;/em&gt; para obtener la IP de la máquina como se indica en &lt;a href=&#34;http://offbytwo.com/2008/05/09/show-ip-address-of-vm-as-console-pre-login-message.html&#34;&gt;Show IP address of VM as console pre-login message&lt;/a&gt; y la he escrito en el fichero &lt;code&gt;/etc/issue&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/sbin/ifconfig | grep &amp;quot;inet addr&amp;quot; | grep -v &amp;quot;127.0.0.1&amp;quot; | awk &#39;{ print $2 }&#39; | awk -F: &#39;{ print $2 }&#39; &amp;gt; /etc/issue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de convertir el &lt;em&gt;script&lt;/em&gt; en ejecutable, he reinciado la máquina para probar que todo funcionaba como esperaba.&lt;/p&gt;

&lt;p&gt;Tras los mensajes de arranque, se muestra:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.1.208
alpine login:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Así no hace falta hacer &lt;em&gt;login&lt;/em&gt; en la máquina para obtener la IP.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instalación de Alpine linux</title>
      <link>https://onthedock.github.io/post/170604-instalacion-de-alpine-linux/</link>
      <pubDate>Sun, 04 Jun 2017 18:26:48 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170604-instalacion-de-alpine-linux/</guid>
      <description>&lt;p&gt;Alpine Linux se ha convertido en la distribución por defecto con la que construir contenedores.&lt;/p&gt;

&lt;p&gt;Alpine tiene sus propias particularidades, ya que no deriva de otra distribución, de manera que he pensado que sería una buena idea tener una máquina virtual con la que entrenarme.&lt;/p&gt;

&lt;p&gt;En este artículo explico qué diferencias he encontrado en Alpine.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;descargando-alpine-linux&#34;&gt;Descargando Alpine Linux&lt;/h2&gt;

&lt;p&gt;La primera diferencia respecto al resto de distribuciones es el tamaño de la ISO de instalación. En la &lt;a href=&#34;https://alpinelinux.org/downloads/&#34;&gt;página de descarga&lt;/a&gt; de Alpine Linux, tienes varias versiones para descargar. Además de las habituales, en función de la arquitectura (x86, x86-64, Raspberry Pi, Generic ARM), tienes disponibles versiones orientadas a entornos virtuales, para Xen, etc.&lt;/p&gt;

&lt;p&gt;En mi caso he descargado la versión &lt;code&gt;Virtual&lt;/code&gt;, orientada a sistemas virtuales y la imagen de instalación ocupa 35MB!.&lt;/p&gt;

&lt;h2 id=&#34;máquina-virtual&#34;&gt;Máquina virtual&lt;/h2&gt;

&lt;p&gt;He creado una máquina virtual y he conectado la ISO.&lt;/p&gt;

&lt;p&gt;Al arrancar la máquina, el sistema arranca en modo &lt;em&gt;live-CD&lt;/em&gt;, ejecutándose completamente en memoria.&lt;/p&gt;

&lt;p&gt;Para acceder al sistema, teclea &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mi primera sorpresa ha sido que no se ha solicitado la contraseña.&lt;/p&gt;

&lt;p&gt;Una vez dentro, ara configurar el sistema, lanza la utilidad &lt;code&gt;setup-alpine&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Welcome to Alpine!

The Alpine Wiki contains a large amount of how-to guides and general
information about administrating Alpine systems.
See &amp;lt;http://wiki.alpinelinux.org&amp;gt;.

You can setup the system with the command: setup-alpine

You may change this message by editing /etc/motd.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El teclado, por defecto, está en inglés, por lo que el &lt;code&gt;-&lt;/code&gt; se encuentra bajo la tecla &lt;code&gt;?&lt;/code&gt; en un teclado en castellano.&lt;/p&gt;

&lt;p&gt;El script de instalación pasa por los diferentes pasos de configuración:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;teclado: &lt;code&gt;es&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;variacion de teclado: &lt;code&gt;es-cat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;nombre del equipo: &lt;code&gt;alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;inicializar interfaz: &lt;code&gt;eth0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;configuración de IP: &lt;code&gt;dhcp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;¿quieres realizar alguna configuración de red manual?: &lt;code&gt;no&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;establecer el password del &lt;code&gt;root&lt;/code&gt;:&lt;/li&gt;
&lt;li&gt;zona horaria: &lt;code&gt;CET&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Proxy: &lt;code&gt;none&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Elección del &lt;em&gt;mirror&lt;/em&gt;: &lt;code&gt;f&lt;/code&gt; (se selecciona el más rápido)&lt;/li&gt;
&lt;li&gt;instalación de servidor de SSH: &lt;code&gt;openssh&lt;/code&gt; (opción por defecto)&lt;/li&gt;
&lt;li&gt;cliente NTP: &lt;code&gt;chrony&lt;/code&gt; (opción por defecto)&lt;/li&gt;
&lt;li&gt;selección de disco: &lt;code&gt;sda&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;uso del disco: &lt;code&gt;sys&lt;/code&gt; (selecciona &lt;code&gt;?&lt;/code&gt; para ver las diferencias entre las opciones presentadas)&lt;/li&gt;
&lt;li&gt;confirmar el borrado del disco: &lt;code&gt;y&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Y ¡ya está! Sólo queda reiniciar.&lt;/p&gt;

&lt;p&gt;En mi caso, he escrito &lt;code&gt;reboot&lt;/code&gt; y el sistema se ha reiniciado al cabo de unos pocos segundos.&lt;/p&gt;

&lt;p&gt;Al hacer login de nuevo, me ha sorprendido que no se me haya solicitado el password y que se haya perdido la configuración introducida :(&lt;/p&gt;

&lt;p&gt;Alpine Linux es una distribución tan ligera -y en la máquina de laboratorio tengo un SSD- que me ha costado un momento darme cuenta de que, al reiniciar, la máquina virtual no ha perdido la configuración, sino que ha arrancado de nuevo la versión del &lt;em&gt;live-CD&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Una vez expulsada la ISO, he reiniciado de nuevo y he accedido al sistema ya instalado en la VM ;)&lt;/p&gt;

&lt;h2 id=&#34;acceso-remoto-vía-ssh&#34;&gt;Acceso remoto vía SSH&lt;/h2&gt;

&lt;p&gt;Por comodidad, prefiero trabajar desde la consola del Mac, pero no quiero crear un nuevo usuario.&lt;/p&gt;

&lt;p&gt;Por defecto, OpenSSH no permite la conexión remota del usuario &lt;code&gt;root&lt;/code&gt;, así que el siguiente paso es modificar el fichero de configuración.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# vi /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Desplázate hasta la línea &lt;code&gt;PermitRootLogin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pulsa &lt;code&gt;i&lt;/code&gt; para entrar en el modo interactivo de Vi&lt;/li&gt;
&lt;li&gt;Escribe en una nueva línea: &lt;code&gt;PermitRootLogin yes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pulsa &lt;code&gt;ESC&lt;/code&gt; para volver al modo de comandos&lt;/li&gt;
&lt;li&gt;Escribe &lt;code&gt;:wq&lt;/code&gt; (&lt;em&gt;write&lt;/em&gt;, &lt;em&gt;quit&lt;/em&gt;) para guardar los cambios y salir de Vi.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para que los cambios tengan efecto, reinicia el servicio SSH:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# service sshd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;siguientes-pasos&#34;&gt;Siguientes pasos&lt;/h2&gt;

&lt;p&gt;A continuación realizaré la instalación de algunos paquetes.&lt;/p&gt;

&lt;p&gt;El objetivo es probar el proceso que se realiza durante la creación de una imagen en Docker, pero en un entorno donde poder observar la salida de los comandos ejecutados, etc.&lt;/p&gt;

&lt;h2 id=&#34;resumen&#34;&gt;Resumen&lt;/h2&gt;

&lt;p&gt;En este artículo hemos instalado Alpine Linux en una máquina virtual.&lt;/p&gt;

&lt;p&gt;También hemos modificado el servidor SSH para poder conectar remotamente como &lt;code&gt;root&lt;/code&gt; (por comodidad, en un entorno seguro de laboratorio).&lt;/p&gt;

&lt;p&gt;En los próximos artículos seguiremos familiarizándonos con Alpine Linux.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Revisión de conceptos</title>
      <link>https://onthedock.github.io/post/170528-revision-de-conceptos/</link>
      <pubDate>Sun, 28 May 2017 07:59:31 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170528-revision-de-conceptos/</guid>
      <description>&lt;p&gt;Después de estabilizar el clúster, el siguiente paso es poner en marcha aplicaciones. Pero ¿qué es exactamente lo que hay que desplegar?: ¿&lt;em&gt;pods&lt;/em&gt;?, ¿&lt;em&gt;replication controllers&lt;/em&gt;?, ¿&lt;em&gt;deployments&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Muchos artículos empiezan creando el fichero YAML para un &lt;em&gt;pod&lt;/em&gt;, después construyen el &lt;em&gt;replication controller&lt;/em&gt;, etc&amp;hellip; Sin embargo, revisando la documentación oficial, crear &lt;em&gt;pods&lt;/em&gt; directamente en Kubernetes no tiene mucho sentido.&lt;/p&gt;

&lt;p&gt;En este artículo intento determinar qué objetos son los que deben crearse en un clúster Kubernetes.
&lt;/p&gt;

&lt;h2 id=&#34;pod&#34;&gt;Pod&lt;/h2&gt;

&lt;p&gt;La unidad fundamental de despliegue en Kubernetes es el &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/&#34;&gt;&lt;strong&gt;Pod&lt;/strong&gt;&lt;/a&gt;. Un &lt;em&gt;pod&lt;/em&gt; sería el equivalente a la mínima unidad funcional de la aplicación.&lt;/p&gt;

&lt;p&gt;En general, un &lt;em&gt;pod&lt;/em&gt; contendrá únicamente un contenedor, aunque no tiene que ser así: si tenemos dos contenedores que actúan de forma conjunta, podemos desplegarlos dentro de un solo &lt;em&gt;pod&lt;/em&gt;. Dentro de un &lt;em&gt;pod&lt;/em&gt; todos los contenedores se pueden comunicar entre ellos usando &lt;code&gt;localhost&lt;/code&gt;, por lo que es una manera sencilla de desplegar en Kubernetes aplicaciones que, aunque hayan sido &lt;em&gt;containerizadas&lt;/em&gt;, no puedan modificarse para comunicarse con otras partes de la aplicación usando una IP o un nombre DNS (porque la aplicación espera que el resto de &lt;em&gt;partes&lt;/em&gt; de la aplicación estén en el mismo equipo).&lt;/p&gt;

&lt;p&gt;En este sentido, todos los contenedores dentro de un &lt;em&gt;pod&lt;/em&gt; se podría decir que están instaladas en una mismo equipo (como un &lt;em&gt;stack&lt;/em&gt; LAMP).&lt;/p&gt;

&lt;p&gt;Sin embargo, un &lt;em&gt;pod&lt;/em&gt; es un elemento &lt;strong&gt;no-durable&lt;/strong&gt;, es decir, que puede fallar o ser eliminado en cualquier momento. Por tanto, &lt;strong&gt;no es una buena idea desplegar &lt;em&gt;pods&lt;/em&gt; individuales en Kubernetes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Como indica la &lt;a href=&#34;https://kubernetes.io/docs/api-reference/v1.6/#pod-v1-core&#34;&gt;documentación para los &lt;em&gt;pods&lt;/em&gt; de la API para la versión 1.6 de Kubernetes&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It is recommended that users create Pods only through a Controller, and not directly.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;replicaset-y-replication-controller&#34;&gt;ReplicaSet y Replication Controller&lt;/h2&gt;

&lt;p&gt;El &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;&lt;strong&gt;Replication Controller&lt;/strong&gt;&lt;/a&gt; o la versión &lt;em&gt;mejorada&lt;/em&gt;, el &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;strong&gt;ReplicaSet&lt;/strong&gt;&lt;/a&gt; se encarga de mantener un determinado número de réplicas del &lt;em&gt;pod&lt;/em&gt; en el clúster.&lt;/p&gt;

&lt;p&gt;El &lt;em&gt;ReplicaSet&lt;/em&gt; asegura que un determinado número de copias -&lt;strong&gt;réplicas&lt;/strong&gt;- del &lt;em&gt;pod&lt;/em&gt; se encuentran en ejecución en el clúster en todo momento. Por tanto, si alguno de los &lt;em&gt;pods&lt;/em&gt; es eliminado, el &lt;em&gt;ReplicaSet&lt;/em&gt; se encarga de crear un nuevo &lt;em&gt;pod&lt;/em&gt;. Para ello, el &lt;em&gt;ReplicaSet&lt;/em&gt; incluye una plantilla con la que crear nuevos &lt;em&gt;pods&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Así, el &lt;em&gt;ReplicaSet&lt;/em&gt; define el &lt;strong&gt;estado deseado&lt;/strong&gt; de la aplicación: cuántas copias de mi aplicación quiero tener en todo momento en ejecución en el clúster.
Modificando el número de réplicas para el &lt;em&gt;ReplicaSet&lt;/em&gt; podemos &lt;strong&gt;escalar&lt;/strong&gt; (incrementar o reducir) el número de copias en ejecución en función de las necesidades.&lt;/p&gt;

&lt;p&gt;Por tanto, parece que el mejor candidato para ponerse a definir ficheros &lt;code&gt;YAML&lt;/code&gt; y desplegar aplicaciones en el clúster de Kubernetes sería un &lt;em&gt;ReplicaSet&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Si embargo, la &lt;a href=&#34;https://kubernetes.io/docs/api-reference/v1.6/#replicaset-v1beta1-extensions&#34;&gt;documentación oficial&lt;/a&gt; nos ofrece otra opción:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In many cases it is recommended to create a Deployment instead of ReplicaSet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Es decir, tenemos una opción mejor: el &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;El &lt;em&gt;Deployment&lt;/em&gt; añade la capacidad de poder actualizar la aplicación definida en un &lt;em&gt;ReplicaSet&lt;/em&gt; sin pérdida de servicio, mediante &lt;strong&gt;actualización continua&lt;/strong&gt; (&lt;em&gt;rolling update&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Si el estado deseado de la aplicación son tres réplicas de un pod basado en &lt;code&gt;yomismo/app-1.0&lt;/code&gt; y queremos actualizar a &lt;code&gt;yomismo/app-2.0&lt;/code&gt;, el &lt;em&gt;Deployment&lt;/em&gt; se encarga de realizar la transición de la versión 1.0 a la 2.0 de forma que no haya interrupción del servicio. La estrategia de actualización puede definirse manualmente, pero sin entrar en detalles, Kubernetes se encarga de ir eliminado progresivamente las réplicas de la aplicación v1.0 y sustituirlas por las de la v2.0.&lt;/p&gt;

&lt;p&gt;El proceso se hace de forma controlada, por lo que si surgen problemas con la nueva versión de la aplicación, la actualización se detiene y es posible realizar &lt;em&gt;marcha atrás&lt;/em&gt; hacia la versión estable.&lt;/p&gt;

&lt;h2 id=&#34;resumiendo&#34;&gt;Resumiendo&lt;/h2&gt;

&lt;p&gt;Así pues, después de leer la sección de &lt;a href=&#34;https://kubernetes.io/docs/concepts/&#34;&gt;Concepts&lt;/a&gt; de la documentación de Kubernetes, parece que ya tengo claro cuál es el proceso para desplegar aplicaciones en Kubernetes.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;En Docker

&lt;ol&gt;
&lt;li&gt;Crear fichero &lt;code&gt;Dockerfile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Construir imagen personalizada&lt;/li&gt;
&lt;li&gt;Subir imagen a un &lt;em&gt;Registry&lt;/em&gt; (de momento, DockerHub)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;En Kubernetes

&lt;ol&gt;
&lt;li&gt;Crear fichero &lt;code&gt;YAML&lt;/code&gt; definiendo el &lt;em&gt;Deployment&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Crear &lt;em&gt;Deployment&lt;/em&gt; en el clúster&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hay otros objetos específicos que pueden ser más adecuados para tus necesidades:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;DaemonSets&lt;/em&gt; : despliegan una copia de un &lt;em&gt;pod&lt;/em&gt; en cada nodo del clúster. Por ejemplo, un antivirus, o una herramienta de gestión de logs, etc&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Jobs&lt;/em&gt; y &lt;em&gt;CronJobs&lt;/em&gt;: crean &lt;em&gt;pods&lt;/em&gt; hasta asegurar que un número determinado finaliza con éxito, lo que completa el &lt;em&gt;job&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;StatefulSets&lt;/em&gt; : todavía en Beta, asignan una identidad única a los &lt;em&gt;pods&lt;/em&gt;, lo que garantiza que se creen o escalen en un orden determinado.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;siguientes-pasos&#34;&gt;Siguientes pasos&lt;/h2&gt;

&lt;p&gt;Al final de este proceso tendré una aplicación &lt;em&gt;simple&lt;/em&gt; desplegada en el clúster. Con &amp;ldquo;sencilla&amp;rdquo; quiero decir que las diferentes instancias de la aplicación actuan de forma independiente. Un ejemplo sería un servidor web: con el &lt;em&gt;deployment&lt;/em&gt; sería posible escalar la aplicación para dar respuesta a la demanda en todo momento y actualizar el contenido de la web sin interrupciones.&lt;/p&gt;

&lt;p&gt;El siguiente paso es crear una aplicación &lt;em&gt;compleja&lt;/em&gt; en la que tengamos, por ejemplo, un &lt;em&gt;frontend&lt;/em&gt; y un &lt;em&gt;backend&lt;/em&gt;. Para estas situaciones, necesitaremos definir un &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34;&gt;&lt;strong&gt;servicio&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introduccion a YAML</title>
      <link>https://onthedock.github.io/post/170525-introduccion-a-yaml/</link>
      <pubDate>Thu, 25 May 2017 18:34:11 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170525-introduccion-a-yaml/</guid>
      <description>&lt;p&gt;YAML es el lenguaje en el que se definen los &lt;em&gt;pods&lt;/em&gt;, los &lt;em&gt;deployments&lt;/em&gt; y demás estructuras en Kubernetes. Todos los artículos que he leído sobre cómo crear un fichero de definición del &lt;em&gt;pod&lt;/em&gt; (&lt;em&gt;deployment&lt;/em&gt;, etc) se centran en el &lt;strong&gt;contenido&lt;/strong&gt; del fichero.&lt;/p&gt;

&lt;p&gt;Pero en mi caso, echaba de menos una explicación de &lt;strong&gt;cómo&lt;/strong&gt; se crea el fichero, qué reglas se siguen a la hora de &lt;em&gt;describir&lt;/em&gt; la configuración en formato YAML.&lt;/p&gt;

&lt;p&gt;Afortunadamente el lenguaje YAML es muy sencillo y basta con conocer un par de estructuras para crear los ficheros de configuración de Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;YAML es un lenguaje de marcado muy simple, basado en ficheros de texto plano legible por los humanos. Este formato se utiliza dentro del mundillo del software para almacenar información de tipo configuración.&lt;/p&gt;

&lt;p&gt;YAML son las siglas de &lt;em&gt;Yet Another Markup Language&lt;/em&gt; (Otro lenguaje de marcado más) o &lt;em&gt;YAML Ain&amp;rsquo;t Markup Language&lt;/em&gt; (YAML no es un lenguaje de marcado), depende de a quién preguntes.&lt;/p&gt;

&lt;p&gt;Usar YAML para las definiciones de Kubernetes proporciona las siguientes ventajas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conveniencia&lt;/strong&gt; No es necesario especificar todos los parámetros en la línea de comandos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mantenimiento&lt;/strong&gt; Los ficheros YAML puede ser gestionados por un sistema de control de versiones, de manera que se pueden registrar los cambios.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibilidad&lt;/strong&gt; Es posible crear estructuras mucho más complejas usando YAML de lo que puede conseguirse desde la línea de comandos.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;YAML es un superconjunto de JSON, lo que significa que cualquier fichero JSON válido también es un fichero YAML válido.&lt;/p&gt;

&lt;p&gt;Como consejos generales a la hora de crear un fichero YAML:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Usa siempre la codificación UTF-8 para evitar errores.&lt;/li&gt;
&lt;li&gt;No uses &lt;strong&gt;nunca&lt;/strong&gt; tabulaciones&lt;/li&gt;
&lt;li&gt;Usa una fuente monoespaciada para visualizar/editar el contenido de los ficheros YAML.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sólo necesitas conocer dos tipos de estructuras en YAML:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Listas&lt;/li&gt;
&lt;li&gt;Mapas&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A parte de los mapas y las listas, también te puede resultar útil saber que cualquier línea que comience con un &lt;code&gt;#&lt;/code&gt; se considera un comentario y es ignorada.&lt;/p&gt;

&lt;h2 id=&#34;mapas-yaml&#34;&gt;Mapas YAML&lt;/h2&gt;

&lt;p&gt;Los mapas te permiten asociar parejas de nombres y valores, lo que es conveniente cuando estás tratando con información relativa a configuraciones. Por ejemplo, puedes tener una configuración que empiece como:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;---
apiVersion: v1
kind: Pod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La primera línea es un separador, y es opcional a no ser que trates de definir múltiples estructuras en un solo fichero. En el fichero puedes ver que tenemos dos valores, &lt;code&gt;v1&lt;/code&gt; y &lt;code&gt;Pod&lt;/code&gt;, asociados a dos claves, &lt;code&gt;apiVersion&lt;/code&gt; y &lt;code&gt;kind&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;No es necesario que los valores estén entrecomillados (con comillas simples o dobles), excepto para asegurarte de que no se interpreta algún caracter especial con un significado diferente a su valor literal.&lt;/p&gt;

&lt;p&gt;Las parejas clave-valor contenidas en un mapa se almacenan sin orden, por lo que puedes especificarlas en el orden que quieras.&lt;/p&gt;

&lt;p&gt;El fichero YAML anterior es equivalente a:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;---
kind: Pod
apiVersion: v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podemos anidar mapas dentro de mapas para crear estructuras más complejas, como:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;---
apiVersion: v1
kind: Pod
metadata:
   name: rss-site
   labels:
      app: web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En este caso tenemos un mapa llamado &lt;code&gt;metadata&lt;/code&gt; que contiene otros dos mapas; el primero &lt;code&gt;name: rss-site&lt;/code&gt; y el segundo, &lt;code&gt;labels&lt;/code&gt;, contiene como valor otro mapa &lt;code&gt;app: web&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Puedes anidar tantos mapas dentro de mapas como quieras.&lt;/p&gt;

&lt;p&gt;Para indicar que un mapa está contenido en otro, se usa la indentación. En el ejemplo anterior hemos usado una indentación de 3 espacios, pero el número de espacios no importa, siempre que sea &lt;strong&gt;consistente&lt;/strong&gt; en el fichero. El procesador de YAML interpreta las claves y valores en la misma profundidad de indentación como al mismo nivel (por ejemplo, &lt;code&gt;name&lt;/code&gt; y &lt;code&gt;labels&lt;/code&gt;), mientras que si están indentadas, interpreta que están &lt;em&gt;contenidas&lt;/em&gt; unos en otros (como en el caso de &lt;code&gt;labels&lt;/code&gt; y &lt;code&gt;app: web&lt;/code&gt;).&lt;/p&gt;

&lt;h2 id=&#34;listas-en-yaml&#34;&gt;Listas en YAML&lt;/h2&gt;

&lt;p&gt;Una lista, en YAML, es una secuencia de objetos, o lo que es lo mismo, una colección ordenada de valores. En este caso los valores no están asociados con una clave, sino con un índice posicional obtenido del orden en el que están especificados en la lista. Por ejemplo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;args
   - sleep
   - 1000
   - message
   - &amp;quot;Hello World!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Puede haber cualquier número de elementos en una lista.&lt;/p&gt;

&lt;p&gt;Como en el caso de las parejas clave-valor, los elementos de una lista se encuentran indentados con el mismo número de espacios bajo el identificador (la clave) de la lista; cada elemento de la lista va precedido por un &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Del mismo modo que podemos anidar mapas en mapas, podemos anidar listas en listas, mapas en listas y cualquier combinación imaginable. Un ejemplo de mapas y listas anidados:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;# Configuracion ficticia
spec:
   containers:
      - name: front-end
        image: nginx
        ports:
           - containerPort: 80
      - name: rss-reader
        image: xavi/rss-reader
        ports:
           - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En resumen, tenemos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mapas&lt;/strong&gt;, que son grupos no ordenados de parejas de clave y valor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Listas&lt;/strong&gt;, que son colecciones ordenadas de elementos individuales&lt;/li&gt;
&lt;li&gt;Mapas de mapas&lt;/li&gt;
&lt;li&gt;Mapas de listas&lt;/li&gt;
&lt;li&gt;Listas de mapas&lt;/li&gt;
&lt;li&gt;Listas de listas&lt;/li&gt;
&lt;li&gt;etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Básicamente, cualquier estructura que puedas imaginar, se puede construir a partir de estos dos elementos.&lt;/p&gt;

&lt;p&gt;Con estos conocimientos básicos, espero que ahora te resulte mucho más sencillo interpretar los ficheros de configuración de &lt;em&gt;pods&lt;/em&gt;, &lt;em&gt;deployments&lt;/em&gt;, etc. Y no sólo en Kubernetes; las configuraciones en formato YAML son usadas por un montón de productos diferentes.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Normas para estructurar ficheros implicados en la creación de contenedores</title>
      <link>https://onthedock.github.io/post/170520-normas-para-estructurar-ficheros-implicados-en-la-creacion-de-contenedores/</link>
      <pubDate>Sat, 20 May 2017 19:59:44 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170520-normas-para-estructurar-ficheros-implicados-en-la-creacion-de-contenedores/</guid>
      <description>&lt;p&gt;El proceso desde la creación a la ejecución del contenedor se puede separar en varias fases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Creación de la imagen (mediante la redacción de un fichero &lt;code&gt;Dockerfile&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Construcción de la imagen&lt;/li&gt;
&lt;li&gt;Ejecución del contenedores&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Para tener los diferentes ficheros implicados en el proceso organizados de forma homogénea, me he autoimpuesto las siguientes reglas a la hora de estructurar los repositorios.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h2&gt;

&lt;p&gt;El primero paso para ejecutar un contenedor es crear la imagen en la que está basado. Para ello debes crear un fichero &lt;code&gt;Dockerfile&lt;/code&gt; en el que se indica la imagen base usada y los diferentes pasos de instalación de paquetes, configuración de usuarios, volúmenes y puertos expuestos.&lt;/p&gt;

&lt;p&gt;En la creación de la imagen intervienen, además del fichero &lt;code&gt;Dockerfile&lt;/code&gt;, ficheros de configuración, etc que se copian a la imagen desde la carpeta donde se encuentra el fichero &lt;code&gt;Dockerfile&lt;/code&gt; (el llamado &lt;em&gt;contexto&lt;/em&gt;, ver &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/build/#options&#34;&gt;Documentación oficial de &lt;code&gt;docker build&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Para gestionar los cambios sobre estos ficheros, lo más sencillo es guardarlos en un repositorio y tener un registro de todos los cambios que se van introduciendo a lo largo del tiempo.&lt;/p&gt;

&lt;p&gt;Todos los ficheros relacionados con la &lt;em&gt;creación&lt;/em&gt; de la imagen se colocan en una carpeta llamada &lt;code&gt;build&lt;/code&gt;, con el &lt;code&gt;Dockerfile&lt;/code&gt; y los ficheros de configuración, etc, agrupados en sus correspondientes carpetas.&lt;/p&gt;

&lt;p&gt;En esta carpeta también se incluyen un fichero con instrucciones para la creación de la imagen (condiciones en las que reutilizar la cache, puntos a tener en cuenta, etc) y un &lt;em&gt;script&lt;/em&gt; para lanzar la creación de la imagen de forma siempre igual (quizás el script borra ficheros temporales o descargados en ejecuciones anteriores, por ejemplo).&lt;/p&gt;

&lt;h2 id=&#34;construcción-de-la-imagen&#34;&gt;Construcción de la imagen&lt;/h2&gt;

&lt;p&gt;Una vez creado el &lt;code&gt;Dockerfile&lt;/code&gt;, &lt;em&gt;construyes&lt;/em&gt; la imagen mediante &lt;code&gt;docker build&lt;/code&gt;. Aunque en general la construcción se realiza mediante un sólo comando de la forma &lt;code&gt;docker build -t {repositorio/etiqueta} .&lt;/code&gt;, puede ser interesante disponer de documentación con indicaciones sobre las reglas de etiquetado de la imagen definidas por la empresa o similar.&lt;/p&gt;

&lt;h2 id=&#34;ejecución-del-contenedor&#34;&gt;Ejecución del contenedor&lt;/h2&gt;

&lt;p&gt;Finalmente la creación de contenedores basados en la imagen se realiza mediante un comando &lt;code&gt;docker run&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A la hora de ejecutar el contenedor la instrucción puede incluir el nombre del contenedor final, la relación entre puertos del &lt;em&gt;host&lt;/em&gt; y el contenedor, el montaje de volúmenes, etc. En algunos casos, el contenedor admite parámetros que se pasan al comando definido en la instrucción &lt;code&gt;CMD&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Para evitar errores o simplemente para no teclear una y otra vez comandos larguísimos para ejecutar el contenedor, podemos crear un &lt;em&gt;script&lt;/em&gt; que lance el contenedor con los parámetros necesarios, así como documentación de la funcionalidad proporcionada por el contenedor, etc.&lt;/p&gt;

&lt;p&gt;Estos ficheros se guardan en el carpeta llamada &lt;code&gt;run&lt;/code&gt;; básicamente el comando para lanzar la creación del contenedor de forma homogénea y las instrucciones con información sobre el uso del contenedor, volúmenes, etc.&lt;/p&gt;

&lt;h2 id=&#34;carpetas&#34;&gt;Carpetas&lt;/h2&gt;

&lt;p&gt;Para estructurar todos los ficheros implicados en el proceso de creación de un contenedor he definido la siguiente estructura de carpetas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./nombre-contenedor/
 |
 ├─Readme.md
 ├─build/
 | ├─Dockerfile
 | ├─build.sh
 | ├─Build-Instructions.md
 | ├─{context-files}/
 | ├─...
 | ├─{context-files}/
 ├─run/
 | ├─run.sh
 | ├─Run-Instructions.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;motivación&#34;&gt;Motivación&lt;/h2&gt;

&lt;p&gt;No he encontrado ningún artículo sobre la organización de los ficheros implicados en el creación de imágenes o de los flujos de trabajo asociados a estos procesos. Tampoco sobre las normas a la hora de etiquetar las imágenes o si se realizan validaciones a la hora de obtener/subir imágenes de repositorios públicos.&lt;/p&gt;

&lt;p&gt;Incluso en una empresa en la que el proceso de desarrollo y operación de las aplicaciones gire alrededor del concepto &lt;em&gt;DevOp&lt;/em&gt;, puede haber otros implicados en el proceso &lt;em&gt;administrativo&lt;/em&gt; del ciclo de vida de la aplicación: decisiones estratégicas, a nivel de seguridad, de &lt;em&gt;compliance&lt;/em&gt; con leyes como la protección de datos, etc.&lt;/p&gt;

&lt;p&gt;En los artículos/conferencias lo habitual es explicar soluciones técnicas sin entrar nunca en estos procesos que relacionan IT con el resto de departamentos de la empresa.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>El nodo k3 sigue colgandose por culpa de Flannel</title>
      <link>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</link>
      <pubDate>Wed, 17 May 2017 21:02:21 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</guid>
      <description>&lt;p&gt;En la entrada &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt; encontré restos de la instalación de &lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt; en la Raspberry Pi. Eliminé los &lt;em&gt;pods&lt;/em&gt; que hacían referencia a Flannel y conseguí que el nodo &lt;strong&gt;k2&lt;/strong&gt; no se volviera a colgar.&lt;/p&gt;

&lt;p&gt;Sin embargo, el problema sigue dándose en el nodo &lt;strong&gt;k3&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Revisando el contenido de &lt;code&gt;/var/lib/kubernetes/pods/&lt;/code&gt; he visto que algunos &lt;em&gt;pods&lt;/em&gt; hacían referencia, todavía, a Flannel.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~439642480.deleting~747926470.deleting~067946013.deleting~791070092.deleting~964331938.deleting~717873461.deleting~755129373.deleting~499171027
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~504683568.deleting~184413472.deleting~138413964.deleting~985160408.deleting~943143520.deleting~459558341.deleting~578589077.deleting~501462031.deleting~769373718
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~613861491.deleting~841547526.deleting~012178845.deleting~177797190.deleting~192052322.deleting~958792988.deleting~338401309.deleting~623810479.deleting~369130424
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~288881360.deleting~534630955.deleting~520377076.deleting~598159984.deleting~426698803.deleting~142931759.deleting~872800923.deleting~808586860
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~439642480.deleting~747926470.deleting~067946013.deleting~791070092.deleting~622848191.deleting~646325460.deleting~868409130.deleting~824166496
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~668636622.deleting~334825066.deleting~737147422.deleting~055159245.deleting~572255670.deleting~485248219.deleting~690855316.deleting~753094008.deleting~457647557
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~756292309.deleting~273222811.deleting~039503494.deleting~182629307.deleting~984614903.deleting~081831640.deleting~628560452.deleting~303652395.deleting~450650534
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~883601122.deleting~535739903.deleting~385002935.deleting~558075878.deleting~174007749.deleting~757820208.deleting~194356513.deleting~813327027.deleting~485662152
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta vez he detectado el problema al intentar calcular el espacio usado por esta carpeta, ya que la Raspberry Pi se ha quedado como &amp;ldquo;colgada&amp;rdquo;, aunque al lanzar &lt;em&gt;htop&lt;/em&gt; no se observaba un uso excesivo de CPU.&lt;/p&gt;

&lt;p&gt;Finalmente, he usado el mismo sistema que la otra vez: eliminar todas las subcarpetas de cada uno de los &lt;em&gt;pods&lt;/em&gt; (dejando únicamente los que no se pueden borrar al estar en uso).&lt;/p&gt;

&lt;p&gt;Después de la purga masiva de &lt;code&gt;rm -rf /var/lib/kubelet/pods/&lt;/code&gt; sólo han quedado dos carpetas &lt;em&gt;en uso&lt;/em&gt;; el número corresponde con el número de &lt;em&gt;pods&lt;/em&gt; planificados sobre el nodo &lt;strong&gt;k3&lt;/strong&gt; desde &lt;code&gt;kubectl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                         READY     STATUS    RESTARTS   AGE       IP             NODE
kube-system   etcd-k1                      1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-apiserver-k1            1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-controller-manager-k1   1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-dns-279829092-1b27r     3/3       Running   12         36d       10.32.0.2      k1
kube-system   kube-proxy-20t3b             1/1       Running   0          25m       192.168.1.13   k3
kube-system   kube-proxy-3dggd             1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-proxy-5b8k3             1/1       Running   2          12d       192.168.1.12   k2
kube-system   kube-scheduler-k1            1/1       Running   4          36d       192.168.1.11   k1
kube-system   weave-net-6qr0l              2/2       Running   8          36d       192.168.1.11   k1
kube-system   weave-net-mxp2w              2/2       Running   0          25m       192.168.1.13   k3
kube-system   weave-net-tmmdj              2/2       Running   4          12d       192.168.1.12   k2
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Un reinicio y ¡listo!, problema -espero- resuelto de forma definitiva.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Protege el acceso remoto via API a Docker</title>
      <link>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sun, 07 May 2017 18:33:16 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; explicaba cómo configurar el acceso remoto al API de Docker. El problema es que de esta forma no hay manera de restringir el acceso.&lt;/p&gt;

&lt;p&gt;En este artículo protegemos el acceso usando TLS de manera que sólo se permitan conexiones que presenten un certificado firmado por una CA de confianza.
&lt;/p&gt;

&lt;p&gt;Seguiremos las instrucciones oficiales de Docker &lt;a href=&#34;https://docs.docker.com/engine/security/https/&#34;&gt;Protect the Docker daemon socket&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;creamos-una-ca-claves-para-el-cliente-y-el-servidor-con-openssl&#34;&gt;Creamos una CA, claves para el cliente y el servidor con OpenSSL&lt;/h2&gt;

&lt;p&gt;Primero, en la máquina &lt;em&gt;host&lt;/em&gt; del Docker &lt;em&gt;daemon&lt;/em&gt;, generamos las claves públicas y privadas de la CA (&lt;em&gt;Certification Authority&lt;/em&gt;, la entidad certificadora):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -aes256 -out ca-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................++
..........................++
e is 65537 (0x10001)
Enter pass phrase for ca-key.pem:
Verifying - Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Y a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem
Enter pass phrase for ca-key.pem:
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &#39;.&#39;, the field will be left blank.
-----
Country Name (2 letter code) [AU]:ES
State or Province Name (full name) [Some-State]:Barcelona
Locality Name (eg, city) []:Barcelona
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Ameisin
Organizational Unit Name (eg, section) []:DevOps
Common Name (e.g. server FQDN or YOUR name) []:192.168.1.20
Email Address []: {REDACTED}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora que tenemos una CA, podemos crear la clave para el servidor y la petición de firmado del certificado (&lt;em&gt;certificate signing request&lt;/em&gt;, CSR). Por favor, verifica que &lt;code&gt;Common Name&lt;/code&gt; (es decir, el &lt;em&gt;FQDN&lt;/em&gt; o &lt;em&gt;YOUR Name&lt;/em&gt;) coincide con el nombre del &lt;em&gt;host&lt;/em&gt; que vas a usar para conectar a Docker.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out server-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................................................................................................++
...............................................................++
e is 65537 (0x10001)
# openssl req -subj &amp;quot;/CN=192.168.1.20&amp;quot; -sha256 -new -key server-key.pem -out server.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación vamos a firmar la clave pública con nuestra CA.&lt;/p&gt;

&lt;p&gt;Como las conexiones TLS pueden realizarse usando la dirección IP o un nombre DNS, deben especificarse durante la creación del certificado.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo subjectAltName = IP:192.168.1.20,IP:127.0.0.1 &amp;gt; extfile.cnf
# openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out server-cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=192.168.1.20
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;autenticación-del-cliente&#34;&gt;Autenticación del cliente&lt;/h2&gt;

&lt;p&gt;Para autenticar al cliente, crearemos una clave de cliente y una petición de firmado del certificado.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Para simplificar, los siguientes dos pasos pueden realizarse desde la máquina donde se encuentra el Docker &lt;em&gt;daemon&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out key.pem 4096
Generating RSA private key, 4096 bit long modulus
...............................................................................++
................................................................................................................................................................................++
e is 65537 (0x10001)
# openssl req -subj &#39;/CN=client&#39; -new -key key.pem -out client.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para que la clave permita autenticar al cliente, creamos un fichero de configuración de extensiones:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo extendedKeyUsage = clientAuth &amp;gt; extfile.cnf
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora firmamos la clave:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=client
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de haber generado &lt;code&gt;cert.pem&lt;/code&gt; y &lt;code&gt;server-cert.pem&lt;/code&gt; podemos eliminar las peticiones de firmado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ls
ca-key.pem  ca.srl    client.csr  extfile.cnf  server-cert.pem	server-key.pem
ca.pem	    cert.pem  key.pem     server.csr
# rm -v client.csr server.csr
removed ‘client.csr’
removed ‘server.csr’
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;protección-de-las-claves&#34;&gt;Protección de las claves&lt;/h2&gt;

&lt;p&gt;Con una máscara &lt;code&gt;umask&lt;/code&gt; por defecto de &lt;code&gt;022&lt;/code&gt; las claves secretas que hemos generado dan a todo el mundo acceso de lectura y de escritura a tu usuario y tu grupo.&lt;/p&gt;

&lt;p&gt;Para proteger las claves de daños accidentales, vamos a eliminar los permisos de escritura sobre ellas. Para hacerlas de sólo lectura para tu usuario, usamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0400 ca-key.pem key.pem server-key.pem
mode of ‘ca-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘server-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Los certificados pueden ser leídos por todo el mundo, pero para evitar daños accidentales, mejor eliminamos los permisos de escritura:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0444 ca.pem server-cert.pem cert.pem
mode of ‘ca.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘server-cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configurando-el-api-de-acceso-remoto-de-forma-segura&#34;&gt;Configurando el API de acceso remoto de forma segura&lt;/h2&gt;

&lt;p&gt;Para hacer que el Docker &lt;em&gt;daemon&lt;/em&gt; sólo acepte conexiones de clientes que proporcionen un certificado de confianza de tu CA.&lt;/p&gt;

&lt;p&gt;Para ello, modificamos las opciones de arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H=0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De manera que quede como (lo he dividido en varias líneas por claridad):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ExecStart=/usr/bin/dockerd --tlsverify 		\
         --tlscacert=/root/ca.pem 		\
         --tlscert=/root/server-cert.pem 	\
         --tlskey=/root/server-key.pem 		\
         -H=0.0.0.0:2376 			\
         -H fd://
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación, recargamos la configuración y reinciamos el servicio:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Los primeros intentos de arrancar el &lt;em&gt;daemon&lt;/em&gt; han fallado; ha sido necesario especificar la ruta completa a los certificados y las claves para conseguir que el servicio arrancara.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finalmente, comprobamos que podemos acceder usando el certificado con &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# curl https://192.168.1.20:2376/version --cert /root/cert.pem --key /root/key.pem --cacert /root/ca.pem
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A diferencia de lo que pasaba antes, cuando se intenta acceder a &lt;code&gt;https://192.168.1.9:2376/version&lt;/code&gt; desde otro equipo (sin usar el certificado), obtenemos un error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-url&#34;&gt;This site can’t be reached
192.168.1.9 refused to connect.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Configura un endpoint remoto en Portainer</title>
      <link>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</link>
      <pubDate>Sat, 06 May 2017 17:38:20 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/&#34;&gt;Portainer para gestionar tus contenedores en Docker&lt;/a&gt; usamos &lt;strong&gt;Portainer&lt;/strong&gt; para gestionar el Docker Engine local.&lt;/p&gt;

&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; habilitamos el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;En este artículo configuramos &lt;strong&gt;Portainer&lt;/strong&gt; para conectar con un &lt;em&gt;endpoint&lt;/em&gt; remoto (el API expuesta de un Docker Engine).
&lt;/p&gt;

&lt;p&gt;Accede a &lt;strong&gt;Portainer&lt;/strong&gt; y selecciona &lt;em&gt;Endpoints&lt;/em&gt; en el panel izquierdo.&lt;/p&gt;

&lt;p&gt;Para configurar el &lt;em&gt;endopoint&lt;/em&gt; remoto (no seguro) sólo necesitas proporcionar un nombre para el &lt;em&gt;endpoint&lt;/em&gt; y la URL de acceso:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/1-configure-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/1-configure-endpoint.png&#34; width=935 height=660 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Configura un nuevo endpoint
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;Para identificar qué Docker Engine estoy viendo en cada momento, indico la IP de la máquina, seguido de la plataforma y el &lt;em&gt;host&lt;/em&gt; en el que se encuentra.&lt;/p&gt;

&lt;p&gt;Para cambiar entre los diferentes &lt;em&gt;endpoints&lt;/em&gt; definidos en &lt;strong&gt;Portainer&lt;/strong&gt;, selecciona el que quieres gestionar en el desplegable de la parte superior del panel lateral:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/2-change-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/2-change-endpoint.png&#34; width=450 height=168 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Cambia entre los diferentes endpoints definidos
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Habilita el acceso remoto vía API a Docker</title>
      <link>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sat, 06 May 2017 15:23:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;Portainer permite gestionar &lt;em&gt;endpoints&lt;/em&gt; remotos para Docker (y Docker Swarm) mediante el API REST de Docker Engine. El problema es que el API está desactivado por defecto.&lt;/p&gt;

&lt;p&gt;A continuación indico cómo activar y verificar el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Buscando en Google cómo habilitar el API remoto de Docker Engine probablemente encuentres el artículo
&lt;a href=&#34;https://www.ivankrizsan.se/2016/05/18/enabling-docker-remote-api-on-ubuntu-16-04/&#34;&gt;Enabling Docker Remote API on Ubuntu 16.04&lt;/a&gt;. Como bien dice en el párrafo inicial, no es fácil encontrar unas instrucciones claras sobre cómo configurar el API de principio a fin.&lt;/p&gt;

&lt;p&gt;Lanzando &lt;code&gt;docker man&lt;/code&gt;, vemos que la opción que buscamos es:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-H, --host=[unix:///var/run/docker.sock]: tcp://[host]:[port][path] to bind or
       unix://[/path/to/socket] to use.
         The socket(s) to bind to in daemon mode specified using one or more
         tcp://host:port/path, unix:///path/to/socket, fd://* or fd://socketfd.
         If the tcp port is not specified, then it will default to either 2375 when
         --tls is off, or 2376 when --tls is on, or --tlsverify is specified.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta opción debe pasarse en el arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker. Para configurar esta opción durante el arranque de Docker Engine tenemos dos opciones:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;modificar el arranque del &lt;em&gt;daemon&lt;/em&gt; modificando la configuración de &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;añadiendo las opciones en el fichero de configuración de Docker Engine. Para sistemas Linux con &lt;em&gt;systemd&lt;/em&gt;, la &lt;a href=&#34;https://docs.docker.com/engine/admin/systemd/#start-automatically-at-system-boot&#34;&gt;configuración del &lt;em&gt;daemon&lt;/em&gt; de Docker&lt;/a&gt; se realiza a través del fichero &lt;code&gt;daemon.json&lt;/code&gt; ubicado en &lt;code&gt;/etc/docker/&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;He intentado configurar Docker Engine mediante el segundo método &lt;em&gt;daemon.json&lt;/em&gt; pero no he sido capaz de activar el API.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Primero, hacemos una copia de seguridad del fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# cp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.original
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editamos el fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target docker.socket firewalld.service
Wants=network-online.target
Requires=docker.socket

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H fd:// 
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=1048576
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process
# restart the docker process if it exits prematurely
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea &lt;code&gt;ExecStart=/usr/bin/dockerd -H fd://&lt;/code&gt; y añadimos: &lt;code&gt;-H tcp://0.0.0.0:2375&lt;/code&gt; de manera que quede:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Esto hace que &lt;em&gt;dockerd&lt;/em&gt; escuche en todas las interfaces disponibles. En el caso de la máquina virtual en la que estoy probando, sólo tengo una, pero lo correcto sería especificar la dirección IP donde quieres que escuche &lt;em&gt;dockerd&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Guardamos los cambios.&lt;/p&gt;

&lt;p&gt;Recargamos la configuración y reiniciamos el servicio.&lt;/p&gt;

&lt;p&gt;Para comprobar que hemos el API funciona, lanzamos una consulta usando &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
# curl http://localhost:2375/version
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Debes tener en cuenta que esta configuración &lt;strong&gt;supone un riesgo de seguridad&lt;/strong&gt; al permitir el acceso al API de Docker Engine sin ningún tipo de control.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Troubleshooting Kubernetes (II)</title>
      <link>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</link>
      <pubDate>Sat, 06 May 2017 05:21:09 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</guid>
      <description>&lt;p&gt;Sigo con el &lt;em&gt;troubleshooting&lt;/em&gt; del &lt;em&gt;cuelgue&lt;/em&gt; de los nodos sobre Raspberry Pi 3 del clúster.&lt;/p&gt;

&lt;p&gt;Ayer estuve &lt;em&gt;haciendo limpieza&lt;/em&gt; siguiendo &lt;em&gt;vagamente&lt;/em&gt; la recomendación de &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;esta respuesta&lt;/a&gt; en el hilo &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;Kubernetes memory consumption explosion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;La solución de &lt;code&gt;RenaudWasTaken&lt;/code&gt; al problema de consumo excesivo de memoria (32GB) fue la realizar limpieza de las carpetas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/etcd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Antes de empezar a borrar &lt;em&gt;a lo loco&lt;/em&gt;, revisé el contenido de estas carpetas.&lt;/p&gt;

&lt;h2 id=&#34;var-run-kubernetes&#34;&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;En &lt;code&gt;/var/run/kubernetes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/run/kubernetes/
total 8
drwxr-xr-x  2 root root   80 May  5 18:43 .
drwxr-xr-x 18 root root  600 May  5 18:43 ..
-rw-r--r--  1 root root 1082 May  5 18:43 kubelet.crt
-rw-------  1 root root 1679 May  5 18:43 kubelet.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos ficheros son certificados, por lo que no parecen implicados en el problema y decido no borrarlos.&lt;/p&gt;

&lt;h2 id=&#34;var-lib-kubelet&#34;&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/h2&gt;

&lt;h3 id=&#34;nodo-k2&#34;&gt;Nodo &lt;strong&gt;k2&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Al intentar listar el contenido de la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt;, la Raspberry Pi 3 ha tardado una eternidad (en los primeros intentos he creído que había dejado de responder).&lt;/p&gt;

&lt;p&gt;Finalmente, el resultado del comando ha mostrado una gran cantidad de carpetas dentro de esta carpeta:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods
...
drwx------    2 root root    4096 May  4 23:11 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~207819844.deleting~559419937.deleting~142152710.deleting~494766199.deleting~952339001
drwx------    2 root root    4096 May  5 18:02 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~274250693.deleting~987962315.deleting~680794233.deleting~917929467
drwx------    2 root root    4096 May  4 23:37 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~292131322.deleting~049606881.deleting~105942520.deleting~463246644
drwx------    2 root root    4096 May  4 22:46 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~119491600.deleting~962328406.deleting~220005477.deleting~309794961.deleting~392355244.deleting~378832104.deleting~159122214.deleting~324365539
drwx------    2 root root    4096 Apr 15 20:39 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~162549394.deleting~296869341.deleting~353223099.deleting~018715754.deleting~526835026.deleting~320404022.deleting~453576282.deleting~001809150
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parece como si algo no hubiera funcionado correctamente y hubiera entrado en un bucle, creando carpetas y más carpetas. Además, en el nombre de alguna de estas carpetas aparece &lt;code&gt;..._flannel-cfg...&lt;/code&gt;. Esta ha sido la pista que me ha convencido; al intentar instalar el &lt;em&gt;dashboard&lt;/em&gt; de Kubernetes, tuve problemas precisamente porque no tengo instalado Flannel. Eliminé el &lt;em&gt;pod&lt;/em&gt; y no le di más vueltas.&lt;/p&gt;

&lt;p&gt;Sin embargo, la existencia de estas carpetas parece indicar que la eliminación no fue tan limpia como pensaba y que &lt;em&gt;algo&lt;/em&gt; se quedó atrapado en un bucle.&lt;/p&gt;

&lt;p&gt;He lanzado &lt;code&gt;rm -rf /var/lib/kubelet/pods/&lt;/code&gt; y el comando ha fallado indicando que uno de los &lt;em&gt;pods&lt;/em&gt; estaba en uso. Así que he eliminado poco a poco los &lt;em&gt;pods&lt;/em&gt; hasta que al final:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls /var/lib/kubelet/pods/ -la
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos &lt;em&gt;pods&lt;/em&gt;, sean los que sean, están en uso (no tengo nada desplegado en el clúster, así que deben ser &lt;em&gt;de sistema&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Tras la limpieza, he reiniciado el nodo.&lt;/p&gt;

&lt;h3 id=&#34;nodo-k3&#34;&gt;Nodo &lt;strong&gt;k3&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;El nodo &lt;strong&gt;k3&lt;/strong&gt; no presentaba estas &lt;em&gt;carpetas sospechosas&lt;/em&gt;, pero también he realizado limpieza:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ rm -rf /var/lib/kubelet/pods/
rm: cannot remove ‘/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap’: Directory not empty
rm: cannot remove ‘/var/lib/kubelet/pods/71290201-31bb-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/kube-proxy-token-7zk2k’: Device or resource busy
rm: cannot remove ‘/var/lib/kubelet/pods/ef887c6a-31ba-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/weave-net-token-61scv’: Device or resource busy
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Igual que en el nodo &lt;strong&gt;k2&lt;/strong&gt;, he reiniciado.&lt;/p&gt;

&lt;h2 id=&#34;resultados&#34;&gt;Resultados&lt;/h2&gt;

&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; siguen en estado &lt;code&gt;Ready&lt;/code&gt; tras unas siete y ocho horas, que es bastante más de lo que &lt;em&gt;aguantaban&lt;/em&gt; antes.&lt;/p&gt;

&lt;p&gt;He comprobado que en la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt; sólo aparecen dos &lt;em&gt;pods&lt;/em&gt; (en el nodo &lt;strong&gt;k2&lt;/strong&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En el nodo &lt;strong&gt;k3&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 28
drwxr-x--- 5 root root 12288 May  5 19:44 .
drwxr-x--- 4 root root  4096 Apr 15 14:10 ..
drwxr-x--- 3 root root  4096 May  5 19:33 3a5e2819-21e5-11e7-bcfd-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 514d4c93-31c9-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 c0b9753a-31c9-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Más adelante actualizaré el artículo para verificar si los nodos siguen activos y sin problemas.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instala Weave Net en Kubernetes 1.6</title>
      <link>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</link>
      <pubDate>Fri, 05 May 2017 22:14:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</guid>
      <description>&lt;p&gt;Una de las cosas que más me sorprenden de Kubernetes es que es necesario instalar una &lt;em&gt;capa de red&lt;/em&gt; sobre el clúster.&lt;/p&gt;

&lt;p&gt;En el caso concreto del que he obtenido las &lt;em&gt;capturas de pantalla&lt;/em&gt;, el clúster corre sobre máquinas virtuales con Debian Jessie.
&lt;/p&gt;

&lt;p&gt;La instalación de Weave Net en Kubernetes consiste únicamente en una línea, como explica el artículo: &lt;a href=&#34;https://www.weave.works/weave-net-kubernetes-integration/&#34;&gt;Run Weave Net with Kubernetes in Just One Line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Antes de instalar la &lt;em&gt;red&lt;/em&gt; en el clúster (de momento, de un solo nodo), &lt;em&gt;kubectl&lt;/em&gt; indica que el estado del nodo es &lt;code&gt;NotReady&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k8s       NotReady   5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la salida del comando tenemos que la versión de Kubernetes es la 1.6.1. Este dato será importante más adelante a la hora de escoger el comando de instalación de Weave Net.&lt;/p&gt;

&lt;p&gt;Si obtenemos la lista de &lt;em&gt;pods&lt;/em&gt;, comprobamos que no tenemos ningún &lt;em&gt;pod de red&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   0          5h
kube-system   kube-apiserver-k8s            1/1       Running   0          5h
kube-system   kube-controller-manager-k8s   1/1       Running   0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending   0          5h
kube-system   kube-proxy-l02zn              1/1       Running   0          5h
kube-system   kube-scheduler-k8s            1/1       Running   0          5h
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, los &lt;em&gt;pods&lt;/em&gt; de &lt;em&gt;DNS&lt;/em&gt; &lt;code&gt;kube-dns-*&lt;/code&gt; están en estado &lt;code&gt;Pending&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Siguiendo las instrucciones del artículo de Weave Net, lanzamos el comando de instalación para versiones 1.6 (o superior):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl apply -f https://git.io/weave-kube-1.6
clusterrole &amp;quot;weave-net&amp;quot; created
serviceaccount &amp;quot;weave-net&amp;quot; created
clusterrolebinding &amp;quot;weave-net&amp;quot; created
daemonset &amp;quot;weave-net&amp;quot; created
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obtenemos la lista de &lt;em&gt;pods&lt;/em&gt; de nuevo y observamos que se están creando dos nuevos contenedores:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;operador@k8s:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             0          5h
kube-system   kube-apiserver-k8s            1/1       Running             0          5h
kube-system   kube-controller-manager-k8s   1/1       Running             0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending             0          5h
kube-system   kube-proxy-l02zn              1/1       Running             0          5h
kube-system   kube-scheduler-k8s            1/1       Running             0          5h
kube-system   weave-net-32ptg               0/2       ContainerCreating   0          12s
operador@k8s:~$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De hecho, se creado el &lt;em&gt;daemonset&lt;/em&gt; &amp;ldquo;weave-net&amp;rdquo;. Un &lt;em&gt;daemonset&lt;/em&gt; es un &lt;em&gt;pod&lt;/em&gt; que se crea en cada uno de los nodos del clúster automáticamente. Kubernetes se encarga de descargar la imagen desde DockerHub y arrancar un contenedor. Los nodos en la red creada por Weave Net forman una red &lt;em&gt;mesh&lt;/em&gt; que se configura automáticamente, de manera que es posible agregar nodos adicionales sin necesidad de cambiar ninguna configuración.&lt;/p&gt;

&lt;p&gt;Pasados unos segundos la creación de los nodos se ha completado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$  kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS         RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running        0          5h
kube-system   kube-apiserver-k8s            1/1       Running        0          5h
kube-system   kube-controller-manager-k8s   1/1       Running        0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       ErrImagePull   0          5h
kube-system   kube-proxy-l02zn              1/1       Running        0          5h
kube-system   kube-scheduler-k8s            1/1       Running        0          5h
kube-system   weave-net-32ptg               2/2       Running        0          1m
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalmente, verificamos que el primer nodo del clúster ya es operativo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k8s       Ready     5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, una vez que tenemos la red instalada en el clúster, el &lt;em&gt;pod&lt;/em&gt; &lt;code&gt;kube-dns&lt;/code&gt; comienza la creación de los contenedores (quizás tengas que reiniciar):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             1          11d
kube-system   kube-apiserver-k8s            1/1       Running             1          11d
kube-system   kube-controller-manager-k8s   1/1       Running             1          11d
kube-system   kube-dns-3913472980-4nlg9     0/3       ContainerCreating   0          11d
kube-system   kube-proxy-l02zn              1/1       Running             1          11d
kube-system   kube-scheduler-k8s            1/1       Running             1          11d
kube-system   weave-net-32ptg               2/2       Running             3          10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tras unos segundos, tenemos todos los &lt;em&gt;pods&lt;/em&gt; del clúster funcionales:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   1          11d
kube-system   kube-apiserver-k8s            1/1       Running   1          11d
kube-system   kube-controller-manager-k8s   1/1       Running   1          11d
kube-system   kube-dns-3913472980-4nlg9     3/3       Running   0          11d
kube-system   kube-proxy-l02zn              1/1       Running   1          11d
kube-system   kube-scheduler-k8s            1/1       Running   1          11d
kube-system   weave-net-32ptg               2/2       Running   3          10d
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo tenemos que añadir nodos &lt;em&gt;worker&lt;/em&gt; y hacer crecer el clúster.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Troubleshooting Kubernetes (I)</title>
      <link>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</link>
      <pubDate>Sun, 30 Apr 2017 15:24:35 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</guid>
      <description>&lt;p&gt;Tras la alegría inicial pensando que la configuración de &lt;em&gt;rsyslog&lt;/em&gt; era la causante de los cuelgues de las dos RPi 3 (&lt;a href=&#34;https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/&#34;&gt;El nodo k3 del clúster colgado de nuevo&lt;/a&gt;), pasadas unas horas los dos nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; han dejado de responder de nuevo.&lt;/p&gt;

&lt;p&gt;Así que es el momento de atacar el problema de forma algo más sistemática. Para ello seguiré las instrucciones que proporcina la página de Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/&#34;&gt;Troubleshooting Clusters&lt;/a&gt;.
&lt;/p&gt;

&lt;h2 id=&#34;descripción-del-problema&#34;&gt;Descripción del problema&lt;/h2&gt;

&lt;p&gt;Tras unas horas activos y formando parte del clúster, los dos nodos que corren sobre Raspberry Pi 3 dejan de responder y el clúster los muestra como &lt;em&gt;NotReady&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;El clúster está formado por tres Raspberry Pi; el nodo &lt;em&gt;master&lt;/em&gt; es una Raspberry Pi 2 B mientras que los dos nodos &lt;em&gt;worker&lt;/em&gt; son Raspberry Pi 3 B.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes -o wide
NAME      STATUS     AGE       VERSION   EXTERNAL-IP   OS-IMAGE                        KERNEL-VERSION
k1        Ready      19d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
k2        NotReady   15d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
k3        NotReady   14d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ping&#34;&gt;Ping&lt;/h2&gt;

&lt;h3 id=&#34;ping-al-nombre-del-nodo&#34;&gt;Ping al nombre del nodo&lt;/h3&gt;

&lt;p&gt;La prueba más sencilla para ver si los nodos están colgados, es lanzar un ping desde el portátil:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ping -c5 k2.local
ping: cannot resolve k2.local: Unknown host
$ ping -c5 k3.local
ping: cannot resolve k3.local: Unknown host
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En esta prueba vemos que ninguno de los nodos responde al nombre que &lt;em&gt;publica&lt;/em&gt; el servicio &lt;a href=&#34;https://en.wikipedia.org/wiki/Avahi_(software)&#34;&gt;Avahi&lt;/a&gt; en el sistema.&lt;/p&gt;

&lt;h3 id=&#34;ping-a-la-ip-del-nodo&#34;&gt;Ping a la IP del nodo&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ping -c5 192.168.1.12
PING 192.168.1.12 (192.168.1.12): 56 data bytes
64 bytes from 192.168.1.12: icmp_seq=0 ttl=64 time=3.842 ms
64 bytes from 192.168.1.12: icmp_seq=1 ttl=64 time=6.678 ms
64 bytes from 192.168.1.12: icmp_seq=2 ttl=64 time=10.789 ms
64 bytes from 192.168.1.12: icmp_seq=3 ttl=64 time=7.411 ms
64 bytes from 192.168.1.12: icmp_seq=4 ttl=64 time=10.518 ms

--- 192.168.1.12 ping statistics ---
5 packets transmitted, 5 packets received, 0.0% packet loss
round-trip min/avg/max/stddev = 3.842/7.848/10.789/2.584 ms
$ ping -c5 192.168.1.13
PING 192.168.1.13 (192.168.1.13): 56 data bytes
Request timeout for icmp_seq 0
Request timeout for icmp_seq 1
Request timeout for icmp_seq 2
Request timeout for icmp_seq 3

--- 192.168.1.13 ping statistics ---
5 packets transmitted, 0 packets received, 100.0% packet loss
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En este caso, el nodo &lt;strong&gt;k2&lt;/strong&gt; sí que responde a ping a la IP, mientras que el nodo &lt;strong&gt;k3&lt;/strong&gt; actúa como si estuviera apagado o con la red deshabilitada.&lt;/p&gt;

&lt;h3 id=&#34;ssh&#34;&gt;SSH&lt;/h3&gt;

&lt;p&gt;Aunque el nodo &lt;strong&gt;k2&lt;/strong&gt; responde a ping, no es posible conectar vía SSH; el intento de conectar no tiene éxito, pero tampoco falla (por &lt;em&gt;timeout&lt;/em&gt;, por ejemplo). He probado a conectar tanto desde el portátil como desde el nodo &lt;strong&gt;k1&lt;/strong&gt;, con el mismo resulado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ssh pirate@192.168.1.12

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubelet-describe-node&#34;&gt;kubelet describe node&lt;/h2&gt;

&lt;p&gt;Usamos el comando &lt;code&gt;kubelet describe node&lt;/code&gt; para los dos nodos colgados.&lt;/p&gt;

&lt;h3 id=&#34;nodo-k2&#34;&gt;Nodo &lt;strong&gt;k2&lt;/strong&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k2
Name:			k2
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k2
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			&amp;lt;none&amp;gt;
CreationTimestamp:	Sat, 15 Apr 2017 10:25:31 +0000
Phase:
Conditions:
  Type			Status		LastHeartbeatTime			LastTransitionTime			Reason			Message
  ----			------		-----------------			------------------			------			-------
  OutOfDisk 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  MemoryPressure 	Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  DiskPressure 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  Ready 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
Addresses:		192.168.1.12,192.168.1.12,k2
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			84bf8a2b-b83f-445b-a4b3-250dc6e5db40
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.2.0/24
ExternalID:			k2
Non-terminated Pods:		(2 in total)
  Namespace			Name				CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----				------------	----------	---------------	-------------
  kube-system			kube-proxy-g580s		0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-kxpk6			20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  20m (0%)	0 (0%)		0 (0%)		0 (0%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodo-k3&#34;&gt;Nodo &lt;strong&gt;k3&lt;/strong&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k3
Name:			k3
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k3
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			&amp;lt;none&amp;gt;
CreationTimestamp:	Sat, 15 Apr 2017 14:10:06 +0000
Phase:
Conditions:
  Type			Status		LastHeartbeatTime			LastTransitionTime			Reason			Message
  ----			------		-----------------			------------------			------			-------
  OutOfDisk 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  MemoryPressure 	Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  DiskPressure 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  Ready 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
Addresses:		192.168.1.13,192.168.1.13,k3
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			23bf96e4-ec65-489c-be00-d0fa848265f3
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.3.0/24
ExternalID:			k3
Non-terminated Pods:		(2 in total)
  Namespace			Name				CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----				------------	----------	---------------	-------------
  kube-system			kube-proxy-bkl4g		0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-3bf40			20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  20m (0%)	0 (0%)		0 (0%)		0 (0%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El nodo &lt;strong&gt;k2&lt;/strong&gt; deja de responder a las 11:56:26, mientras que el &lt;strong&gt;k3&lt;/strong&gt; lo hace a las 10:33:45.&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Cuando reinicie los dos nodos lo haré a la misma hora, para comprobar si hay diferencias en el tiempo que tarda en dejar de responder cada nodo.&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Actualizar la zona horaria de las Raspberry Pi.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;nodo-k1-ready&#34;&gt;Nodo &lt;strong&gt;k1&lt;/strong&gt; (&lt;code&gt;Ready&lt;/code&gt;)&lt;/h3&gt;

&lt;p&gt;Como referencia, incluimos el mismo comando para el nodo &lt;em&gt;master&lt;/em&gt; &lt;strong&gt;k1&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k1
Name:			k1
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k1
			node-role.kubernetes.io/master=
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			node-role.kubernetes.io/master:NoSchedule
CreationTimestamp:	Mon, 10 Apr 2017 20:22:32 +0000
Phase:
Conditions:
  Type			Status	LastHeartbeatTime			LastTransitionTime			Reason				Message
  ----			------	-----------------			------------------			------				-------
  OutOfDisk 		False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasSufficientDisk 	kubelet has sufficient disk space available
  MemoryPressure 	False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasSufficientMemory 	kubelet has sufficient memory available
  DiskPressure 		False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasNoDiskPressure 	kubelet has no disk pressure
  Ready 		True 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:20 +0000 	KubeletReady 			kubelet is posting ready status
Addresses:		192.168.1.11,192.168.1.11,k1
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			55e1fad0-d40c-480b-b039-5586ff728d2c
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.0.0/24
ExternalID:			k1
Non-terminated Pods:		(7 in total)
  Namespace			Name					CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----					------------	----------	---------------	-------------
  kube-system			etcd-k1					0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-apiserver-k1			250m (6%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-controller-manager-k1		200m (5%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-dns-279829092-1b27r		260m (6%)	0 (0%)		110Mi (14%)	170Mi (22%)
  kube-system			kube-proxy-3dggd			0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-scheduler-k1			100m (2%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-6qr0l				20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  830m (20%)	0 (0%)		110Mi (14%)	170Mi (22%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;revisando-los-logs-en-el-nodo-master&#34;&gt;Revisando los logs en el nodo &lt;em&gt;master&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;En la guía de &lt;em&gt;Troubleshooting&lt;/em&gt; de Kubernetes, el siguiente paso es revisar los logs. En el caso del nodo &lt;em&gt;master&lt;/em&gt;, los logs relevantes se encuentran en:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-apiserver.log&lt;/code&gt; - El &lt;em&gt;API Server&lt;/em&gt;, encargado de servir la API&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-scheduler.log&lt;/code&gt; - El &lt;em&gt;Scheduler&lt;/em&gt;, encargado de las decisiones de planificar los &lt;em&gt;pods&lt;/em&gt; en los nodos&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-controller-manager.log&lt;/code&gt; - El responsable de gestionar los &lt;em&gt;replication controllers&lt;/em&gt; encargados de mantener el &lt;strong&gt;estado deseado&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sin embargo, los logs indicados &lt;strong&gt;no existen en la ruta indicada&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls /var/log/kube*
ls: cannot access /var/log/kube*: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es probable que la documentación no esté actualizada, así que continuaré en cuanto encuentre los logs para poder revisarlos.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Errores sobre Orphaned pods en syslog</title>
      <link>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</link>
      <pubDate>Sun, 30 Apr 2017 12:55:44 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</guid>
      <description>&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; del clúster dejan de responder pasadas unas horas. La única manera de solucionarlo es reiniciar los nodos. Siguiendo con la revisión de logs, he encontrado que se genera una gran cantidad de entradas en &lt;em&gt;syslog&lt;/em&gt; en referencia a &lt;em&gt;orphaned pods&lt;/em&gt;. Además, el número de estos errores no para de crecer &lt;strong&gt;rápidamente&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
118938
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119022
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119170
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Revisando las últimas entradas del log:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-log&#34;&gt;Apr 30 10:57:57 k2 kubelet[3619]: E0430 10:57:57.186318    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;5064b9d9-2c9e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:01 k2 kubelet[3619]: E0430 10:58:01.759595    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;6c601e9c-2c9c-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.226372    3619 kubelet.go:1549] Unable to mount volumes for pod &amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]; skipping pod
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.238315    3619 pod_workers.go:182] Error syncing pod 4461d51d-2d93-11e7-a7ae-b827eb650fdb (&amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;), skipping: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]
Apr 30 10:58:05 k2 kubelet[3619]: E0430 10:58:05.830432    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;bb4d3ea6-2b80-11e7-9388-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:08 k2 kubelet[3619]: E0430 10:58:08.435567    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;cb23be0d-2d7e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Todas estas entradas se encuentran en los logs del nodo &lt;strong&gt;k2&lt;/strong&gt;, donde no hay ningún &lt;em&gt;pod&lt;/em&gt; en ejecución (a parte de los propios de  Kubernetes que el clúster planifica en los diferentes nodos).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>El nodo k3 del clúster colgado de nuevo</title>
      <link>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</link>
      <pubDate>Sun, 30 Apr 2017 11:39:20 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/&#34;&gt;Múltiples mensajes &amp;lsquo;action 17 suspended&amp;rsquo; en los logs&lt;/a&gt; comentaba que estaba a la espera de obtener resultados; después de apenas unas horas, ya los tengo: &lt;strong&gt;k3&lt;/strong&gt; se ha vuelto a &lt;em&gt;colgar&lt;/em&gt; mientras que &lt;strong&gt;k2&lt;/strong&gt; no.&lt;/p&gt;

&lt;p&gt;Este resultado parece demostrar que la mala configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la causante de los &lt;em&gt;cuelgues&lt;/em&gt; de las RPi 3 en el clúster de Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: El nodo &lt;strong&gt;k2&lt;/strong&gt; sobre RPi3 sigue colgándose :(&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización II&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Parece solucionado&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;A modo de recordatorio, los cambios realizados en los dos nodos sobre Raspberry Pi 3 han sido (incluyo el nodo &lt;strong&gt;k1&lt;/strong&gt; con RPi2):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                                |  k1  |  k2  |  k3  |
                                | RPi2 | RPi3 | RPi3 |
 -------------------------------|------|------|------|
| Modificada conf. de rsyslog   |  No  |  Sí  |  No  |
| Actualización a versión 1.6.2 |  Sí  |  Sí  |  Sí  |
 ----------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hasta ahora, los únicos nodos que se &lt;em&gt;colgaban&lt;/em&gt; eran el &lt;strong&gt;k2&lt;/strong&gt; y el &lt;strong&gt;k3&lt;/strong&gt; (sobre RPi3).&lt;/p&gt;

&lt;p&gt;Al modificar la configuración en de &lt;em&gt;rsyslog&lt;/em&gt; en &lt;strong&gt;k2&lt;/strong&gt; y pasadas unas horas, el único nodo que se sigue colgando es el &lt;strong&gt;k3&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k1        Ready      19d       v1.6.2
k2        Ready      14d       v1.6.2
k3        NotReady   14d       v1.6.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es decir, el fallo a la hora de redirigir los mensajes a &lt;code&gt;/dev/xconsole&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sólo afectan a las RPi3&lt;/li&gt;
&lt;li&gt;provoca que el sistema se acabe colgando&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para solucionarlo, como &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Abre &lt;code&gt;/etc/rsyslog.conf&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Modifica las últimas líneas (al final del fichero) y coméntalas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;# The named pipe /dev/xconsole is for the `xconsole&#39; utility.  To use it,
# you must invoke `xconsole&#39; with the `-file&#39; option:
#
#    $ xconsole -file /dev/xconsole [...]
#
# NOTE: adjust the list below, or you&#39;ll go crazy if you have a reasonably
#      busy site..
#
daemon.*;mail.*;\
    news.err;\
    *.=debug;*.=info;\
    *.=notice;*.=warn       |/dev/xconsole
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;deben quedar como:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;#daemon.*;mail.*;\
#        news.err;\
#        *.=debug;*.=info;\
#        *.=notice;*.=warn       |/dev/xconsole
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podrías comentar la redirección &lt;code&gt;|/dev/xconsole&lt;/code&gt;, pero en este caso el bloque no tendría ninguna funcionalidad, por lo que creo que es &lt;em&gt;más limpio&lt;/em&gt; comentar todo el bloque.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reinicia el equipo mediante &lt;code&gt;reboot&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Múltiples mensajes &#39;action 17 suspended&#39; en los logs</title>
      <link>https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/</link>
      <pubDate>Sun, 30 Apr 2017 08:44:27 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/</guid>
      <description>&lt;p&gt;Investigando las causas por las que los dos nodos con Raspberry Pi 3 se &lt;em&gt;cuelgan&lt;/em&gt;, he encontrado múltiples apariciones de este mensaje en &lt;code&gt;/var/log/messages&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-log&#34;&gt;Apr 30 06:40:42 k3 rsyslogd-2007: action &#39;action 17&#39; suspended, next retry is Sun Apr 30 06:41:12 2017 [try http://www.rsyslog.com/e/2007 ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;De hecho, revisando el origen del problema he encontrado este comando que cuenta las apariciones del mensaje:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo grep &amp;quot;action.*suspend&amp;quot; /var/log/messages | wc -l
1394
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además de &lt;em&gt;spamear&lt;/em&gt; los logs, provoca un montón de escrituras innecesarias sobre la tarjeta microSD, lo que puede acortar la vida útil de la misma.&lt;/p&gt;

&lt;p&gt;No tengo claro si este puede ser la causa que hace que las dos Raspberry Pi 3 se cuelguen pasado un tiempo y que dejen de responder, lo que hace que deba reiniciarlas (desconectando/conectando el cable) para recuperarlas. Sin embargo, en el nodo &lt;em&gt;master&lt;/em&gt; (Raspberry Pi 2 B+) no aparece el mensaje en los logs y no se cuelga (aunque la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la misma).&lt;/p&gt;

&lt;h1 id=&#34;solución-a-los-mensajes-de-rsyslog&#34;&gt;Solución a los mensajes de rsyslog&lt;/h1&gt;

&lt;p&gt;El mensaje de error , es un problema de configuración de la aplicación &lt;code&gt;rsyslog&lt;/code&gt;, que intenta mostrar mensajes en &lt;code&gt;/dev/xconsole&lt;/code&gt;, pero falla.&lt;/p&gt;

&lt;p&gt;La solución la explica Danny Tuppeny en su blog, en &lt;a href=&#34;https://blog.dantup.com/2016/04/removing-rsyslog-spam-on-raspberry-pi-raspbian-jessie/&#34;&gt;Removing [action &amp;lsquo;action 17&amp;rsquo; suspended] rsyslog Spam on Raspberry Pi (Raspian Jessie)&lt;/a&gt;. Él mismo abrió un &lt;em&gt;bug&lt;/em&gt; en RPI-Distro: &lt;a href=&#34;https://github.com/RPi-Distro/repo/issues/28&#34;&gt;Default Raspbian Jessie Lite install spams syslog with &amp;ldquo;rsyslogd-2007: action &amp;lsquo;action 17&amp;rsquo; suspended, next retry is #28&lt;/a&gt; en él explicaba cómo había eliminado la línea que hace referencia a &lt;code&gt;/dev/xconsole&lt;/code&gt; de la configuración de &lt;em&gt;rasyslog&lt;/em&gt; y que el mensaje desaparecía (después de reiniciar).&lt;/p&gt;

&lt;p&gt;Para comprobar si esta es la causa del &lt;em&gt;cuelgue&lt;/em&gt; de la RPi 3, he modificado la configuración de &lt;em&gt;rsyslog&lt;/em&gt; en el nodo &lt;strong&gt;k2&lt;/strong&gt; del clúster, pero no en el &lt;strong&gt;k3&lt;/strong&gt;. De esta forma podré averiguar si la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la causante del &lt;em&gt;cuelgue&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;También he actualizado el sistema (en todos los nodos) y &lt;em&gt;kubelet&lt;/em&gt;, &lt;em&gt;kubectl&lt;/em&gt; y &lt;em&gt;kubeadm&lt;/em&gt; se han actualizado a la versión 1.6.2:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
Setting up libldap-2.4-2:armhf (2.4.40+dfsg-1+deb8u2) ...
Setting up libicu52:armhf (52.1-8+deb8u5) ...
Setting up kubelet (1.6.2-00) ...
Setting up kubectl (1.6.2-00) ...
Setting up kubeadm (1.6.2-00) ...
Processing triggers for libc-bin (2.19-18+deb8u7)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k1        Ready     19d       v1.6.2
k2        Ready     14d       v1.6.2
k3        Ready     14d       v1.6.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo queda esperar -normalmente unas cuantas horas- a ver qué pasa: hay tres posibilidades:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Se cuelga &lt;strong&gt;k3&lt;/strong&gt; pero no &lt;strong&gt;k2&lt;/strong&gt;: La configuración de &lt;em&gt;rsyslog&lt;/em&gt; era la causa.&lt;/li&gt;
&lt;li&gt;Se cuelga &lt;strong&gt;k2&lt;/strong&gt; pero no &lt;strong&gt;k3&lt;/strong&gt;: ¿?&lt;/li&gt;
&lt;li&gt;No se cuelga ni &lt;strong&gt;k2&lt;/strong&gt; ni &lt;strong&gt;k3&lt;/strong&gt;: Era un problema de alguno de los componetes actualizados que sólo afecta a la RPi 3.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Informaré en cuanto tenga resultados.&lt;/p&gt;

&lt;h1 id=&#34;unas-horas-después&#34;&gt;Unas horas después&amp;hellip;&lt;/h1&gt;

&lt;p&gt;Ya tenngo resultados: la configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la que causa el cuelgue del sistema en las RPi3.&lt;/p&gt;

&lt;p&gt;Échale un vistazo a cómo solucionar este error en la entrada: &lt;a href=&#34;https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/&#34;&gt;El nodo k3 del clúster colgado de nuevo&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>