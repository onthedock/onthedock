<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on On The Dock</title>
    <link>https://onthedock.github.io/post/index.xml</link>
    <description>Recent content in Posts on On The Dock</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Handmade with &amp;#9829; by Xavi Aznar</copyright>
    <lastBuildDate>Sun, 02 Jul 2017 22:07:18 +0200</lastBuildDate>
    <atom:link href="https://onthedock.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>IP en mensaje prelogin</title>
      <link>https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/</link>
      <pubDate>Sun, 02 Jul 2017 22:07:18 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170702-ip-en-mensaje-prelogin/</guid>
      <description>&lt;p&gt;En la pantalla de &lt;em&gt;login&lt;/em&gt; en modo consola de los sistemas Linux se muestra un mensaje de bienvenida.&lt;/p&gt;

&lt;p&gt;En este artículo se muestra cómo hacer que se muestre la IP del equipo.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Cuando tenemos una máquina (virtual) configurada para obtener la IP de forma dinámica mediante DHCP, al desconocer por adelantado la IP que se le ha asignado, es necesario conectarse al hipervisor, hacer login en la máquina virtual para, finalmente, obtener la IP &lt;em&gt;actual&lt;/em&gt; de la VM.&lt;/p&gt;

&lt;p&gt;Entonces podemos conectarnos &lt;em&gt;remotamente&lt;/em&gt; usando PuTTY (desde Windows) o un emulador de terminal desde Linux/OSX.&lt;/p&gt;

&lt;p&gt;Sin embargo, hay una manera de agilizar este proceso, aprovechando el mensaje que se muestra antes del login.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Aunque en este caso he usado Alpine Linux, las instrucciones son igualmente válidas para la mayoría de distribuciones.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;En primer lugar necesitamos ejecutar un &lt;em&gt;script&lt;/em&gt; durante el arranque del sistema operativo. En el caso concreto de Alpine Linux, he encontrado la solución en &lt;a href=&#34;https://forum.alpinelinux.org/forum/general-discussion/run-script-boot&#34;&gt;run script on boot&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Ya que no vamos a escribir nuestro propio servicio, usaremos el servicio &lt;em&gt;local&lt;/em&gt;. Para ello, hay que añadir nuestro &lt;em&gt;script&lt;/em&gt; en la carpeta &lt;code&gt;/etc/local.d/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;rc-update add local default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El &lt;code&gt;README&lt;/code&gt; ubicado en &lt;code&gt;/etc/local.d/README&lt;/code&gt; indica que cualquier fichero ejecutable con extensión &lt;code&gt;.start&lt;/code&gt; se lanza al arrancar el servicio, mientras que si la extensión es &lt;code&gt;.stop&lt;/code&gt;, se ejecuta al parar el servicio.&lt;/p&gt;

&lt;p&gt;En mi caso, he usado el &lt;em&gt;script&lt;/em&gt; para obtener la IP de la máquina como se indica en &lt;a href=&#34;http://offbytwo.com/2008/05/09/show-ip-address-of-vm-as-console-pre-login-message.html&#34;&gt;Show IP address of VM as console pre-login message&lt;/a&gt; y la he escrito en el fichero &lt;code&gt;/etc/issue&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/sbin/ifconfig | grep &amp;quot;inet addr&amp;quot; | grep -v &amp;quot;127.0.0.1&amp;quot; | awk &#39;{ print $2 }&#39; | awk -F: &#39;{ print $2 }&#39; &amp;gt; /etc/issue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de convertir el &lt;em&gt;script&lt;/em&gt; en ejecutable, he reinciado la máquina para probar que todo funcionaba como esperaba.&lt;/p&gt;

&lt;p&gt;Tras los mensajes de arranque, se muestra:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.1.208
alpine login:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Así no hace falta hacer &lt;em&gt;login&lt;/em&gt; en la máquina para obtener la IP.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instalación de Alpine linux</title>
      <link>https://onthedock.github.io/post/170604-instalacion-de-alpine-linux/</link>
      <pubDate>Sun, 04 Jun 2017 18:26:48 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170604-instalacion-de-alpine-linux/</guid>
      <description>&lt;p&gt;Alpine Linux se ha convertido en la distribución por defecto con la que construir contenedores.&lt;/p&gt;

&lt;p&gt;Alpine tiene sus propias particularidades, ya que no deriva de otra distribución, de manera que he pensado que sería una buena idea tener una máquina virtual con la que entrenarme.&lt;/p&gt;

&lt;p&gt;En este artículo explico qué diferencias he encontrado en Alpine.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;descargando-alpine-linux&#34;&gt;Descargando Alpine Linux&lt;/h2&gt;

&lt;p&gt;La primera diferencia respecto al resto de distribuciones es el tamaño de la ISO de instalación. En la &lt;a href=&#34;https://alpinelinux.org/downloads/&#34;&gt;página de descarga&lt;/a&gt; de Alpine Linux, tienes varias versiones para descargar. Además de las habituales, en función de la arquitectura (x86, x86-64, Raspberry Pi, Generic ARM), tienes disponibles versiones orientadas a entornos virtuales, para Xen, etc.&lt;/p&gt;

&lt;p&gt;En mi caso he descargado la versión &lt;code&gt;Virtual&lt;/code&gt;, orientada a sistemas virtuales y la imagen de instalación ocupa 35MB!.&lt;/p&gt;

&lt;h2 id=&#34;máquina-virtual&#34;&gt;Máquina virtual&lt;/h2&gt;

&lt;p&gt;He creado una máquina virtual y he conectado la ISO.&lt;/p&gt;

&lt;p&gt;Al arrancar la máquina, el sistema arranca en modo &lt;em&gt;live-CD&lt;/em&gt;, ejecutándose completamente en memoria.&lt;/p&gt;

&lt;p&gt;Para acceder al sistema, teclea &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mi primera sorpresa ha sido que no se ha solicitado la contraseña.&lt;/p&gt;

&lt;p&gt;Una vez dentro, ara configurar el sistema, lanza la utilidad &lt;code&gt;setup-alpine&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Welcome to Alpine!

The Alpine Wiki contains a large amount of how-to guides and general
information about administrating Alpine systems.
See &amp;lt;http://wiki.alpinelinux.org&amp;gt;.

You can setup the system with the command: setup-alpine

You may change this message by editing /etc/motd.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El teclado, por defecto, está en inglés, por lo que el &lt;code&gt;-&lt;/code&gt; se encuentra bajo la tecla &lt;code&gt;?&lt;/code&gt; en un teclado en castellano.&lt;/p&gt;

&lt;p&gt;El script de instalación pasa por los diferentes pasos de configuración:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;teclado: &lt;code&gt;es&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;variacion de teclado: &lt;code&gt;es-cat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;nombre del equipo: &lt;code&gt;alpine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;inicializar interfaz: &lt;code&gt;eth0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;configuración de IP: &lt;code&gt;dhcp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;¿quieres realizar alguna configuración de red manual?: &lt;code&gt;no&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;establecer el password del &lt;code&gt;root&lt;/code&gt;:&lt;/li&gt;
&lt;li&gt;zona horaria: &lt;code&gt;CET&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Proxy: &lt;code&gt;none&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Elección del &lt;em&gt;mirror&lt;/em&gt;: &lt;code&gt;f&lt;/code&gt; (se selecciona el más rápido)&lt;/li&gt;
&lt;li&gt;instalación de servidor de SSH: &lt;code&gt;openssh&lt;/code&gt; (opción por defecto)&lt;/li&gt;
&lt;li&gt;cliente NTP: &lt;code&gt;chrony&lt;/code&gt; (opción por defecto)&lt;/li&gt;
&lt;li&gt;selección de disco: &lt;code&gt;sda&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;uso del disco: &lt;code&gt;sys&lt;/code&gt; (selecciona &lt;code&gt;?&lt;/code&gt; para ver las diferencias entre las opciones presentadas)&lt;/li&gt;
&lt;li&gt;confirmar el borrado del disco: &lt;code&gt;y&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Y ¡ya está! Sólo queda reiniciar.&lt;/p&gt;

&lt;p&gt;En mi caso, he escrito &lt;code&gt;reboot&lt;/code&gt; y el sistema se ha reiniciado al cabo de unos pocos segundos.&lt;/p&gt;

&lt;p&gt;Al hacer login de nuevo, me ha sorprendido que no se me haya solicitado el password y que se haya perdido la configuración introducida :(&lt;/p&gt;

&lt;p&gt;Alpine Linux es una distribución tan ligera -y en la máquina de laboratorio tengo un SSD- que me ha costado un momento darme cuenta de que, al reiniciar, la máquina virtual no ha perdido la configuración, sino que ha arrancado de nuevo la versión del &lt;em&gt;live-CD&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Una vez expulsada la ISO, he reiniciado de nuevo y he accedido al sistema ya instalado en la VM ;)&lt;/p&gt;

&lt;h2 id=&#34;acceso-remoto-vía-ssh&#34;&gt;Acceso remoto vía SSH&lt;/h2&gt;

&lt;p&gt;Por comodidad, prefiero trabajar desde la consola del Mac, pero no quiero crear un nuevo usuario.&lt;/p&gt;

&lt;p&gt;Por defecto, OpenSSH no permite la conexión remota del usuario &lt;code&gt;root&lt;/code&gt;, así que el siguiente paso es modificar el fichero de configuración.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# vi /etc/ssh/sshd_config
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Desplázate hasta la línea &lt;code&gt;PermitRootLogin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pulsa &lt;code&gt;i&lt;/code&gt; para entrar en el modo interactivo de Vi&lt;/li&gt;
&lt;li&gt;Escribe en una nueva línea: &lt;code&gt;PermitRootLogin yes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Pulsa &lt;code&gt;ESC&lt;/code&gt; para volver al modo de comandos&lt;/li&gt;
&lt;li&gt;Escribe &lt;code&gt;:wq&lt;/code&gt; (&lt;em&gt;write&lt;/em&gt;, &lt;em&gt;quit&lt;/em&gt;) para guardar los cambios y salir de Vi.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para que los cambios tengan efecto, reinicia el servicio SSH:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# service sshd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;siguientes-pasos&#34;&gt;Siguientes pasos&lt;/h2&gt;

&lt;p&gt;A continuación realizaré la instalación de algunos paquetes.&lt;/p&gt;

&lt;p&gt;El objetivo es probar el proceso que se realiza durante la creación de una imagen en Docker, pero en un entorno donde poder observar la salida de los comandos ejecutados, etc.&lt;/p&gt;

&lt;h2 id=&#34;resumen&#34;&gt;Resumen&lt;/h2&gt;

&lt;p&gt;En este artículo hemos instalado Alpine Linux en una máquina virtual.&lt;/p&gt;

&lt;p&gt;También hemos modificado el servidor SSH para poder conectar remotamente como &lt;code&gt;root&lt;/code&gt; (por comodidad, en un entorno seguro de laboratorio).&lt;/p&gt;

&lt;p&gt;En los próximos artículos seguiremos familiarizándonos con Alpine Linux.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Revisión de conceptos</title>
      <link>https://onthedock.github.io/post/170528-revision-de-conceptos/</link>
      <pubDate>Sun, 28 May 2017 07:59:31 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170528-revision-de-conceptos/</guid>
      <description>&lt;p&gt;Después de estabilizar el clúster, el siguiente paso es poner en marcha aplicaciones. Pero ¿qué es exactamente lo que hay que desplegar?: ¿&lt;em&gt;pods&lt;/em&gt;?, ¿&lt;em&gt;replication controllers&lt;/em&gt;?, ¿&lt;em&gt;deployments&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Muchos artículos empiezan creando el fichero YAML para un &lt;em&gt;pod&lt;/em&gt;, después construyen el &lt;em&gt;replication controller&lt;/em&gt;, etc&amp;hellip; Sin embargo, revisando la documentación oficial, crear &lt;em&gt;pods&lt;/em&gt; directamente en Kubernetes no tiene mucho sentido.&lt;/p&gt;

&lt;p&gt;En este artículo intento determinar qué objetos son los que deben crearse en un clúster Kubernetes.
&lt;/p&gt;

&lt;h2 id=&#34;pod&#34;&gt;Pod&lt;/h2&gt;

&lt;p&gt;La unidad fundamental de despliegue en Kubernetes es el &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/&#34;&gt;&lt;strong&gt;Pod&lt;/strong&gt;&lt;/a&gt;. Un &lt;em&gt;pod&lt;/em&gt; sería el equivalente a la mínima unidad funcional de la aplicación.&lt;/p&gt;

&lt;p&gt;En general, un &lt;em&gt;pod&lt;/em&gt; contendrá únicamente un contenedor, aunque no tiene que ser así: si tenemos dos contenedores que actúan de forma conjunta, podemos desplegarlos dentro de un solo &lt;em&gt;pod&lt;/em&gt;. Dentro de un &lt;em&gt;pod&lt;/em&gt; todos los contenedores se pueden comunicar entre ellos usando &lt;code&gt;localhost&lt;/code&gt;, por lo que es una manera sencilla de desplegar en Kubernetes aplicaciones que, aunque hayan sido &lt;em&gt;containerizadas&lt;/em&gt;, no puedan modificarse para comunicarse con otras partes de la aplicación usando una IP o un nombre DNS (porque la aplicación espera que el resto de &lt;em&gt;partes&lt;/em&gt; de la aplicación estén en el mismo equipo).&lt;/p&gt;

&lt;p&gt;En este sentido, todos los contenedores dentro de un &lt;em&gt;pod&lt;/em&gt; se podría decir que están instaladas en una mismo equipo (como un &lt;em&gt;stack&lt;/em&gt; LAMP).&lt;/p&gt;

&lt;p&gt;Sin embargo, un &lt;em&gt;pod&lt;/em&gt; es un elemento &lt;strong&gt;no-durable&lt;/strong&gt;, es decir, que puede fallar o ser eliminado en cualquier momento. Por tanto, &lt;strong&gt;no es una buena idea desplegar &lt;em&gt;pods&lt;/em&gt; individuales en Kubernetes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Como indica la &lt;a href=&#34;https://kubernetes.io/docs/api-reference/v1.6/#pod-v1-core&#34;&gt;documentación para los &lt;em&gt;pods&lt;/em&gt; de la API para la versión 1.6 de Kubernetes&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It is recommended that users create Pods only through a Controller, and not directly.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;replicaset-y-replication-controller&#34;&gt;ReplicaSet y Replication Controller&lt;/h2&gt;

&lt;p&gt;El &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;&lt;strong&gt;Replication Controller&lt;/strong&gt;&lt;/a&gt; o la versión &lt;em&gt;mejorada&lt;/em&gt;, el &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;strong&gt;ReplicaSet&lt;/strong&gt;&lt;/a&gt; se encarga de mantener un determinado número de réplicas del &lt;em&gt;pod&lt;/em&gt; en el clúster.&lt;/p&gt;

&lt;p&gt;El &lt;em&gt;ReplicaSet&lt;/em&gt; asegura que un determinado número de copias -&lt;strong&gt;réplicas&lt;/strong&gt;- del &lt;em&gt;pod&lt;/em&gt; se encuentran en ejecución en el clúster en todo momento. Por tanto, si alguno de los &lt;em&gt;pods&lt;/em&gt; es eliminado, el &lt;em&gt;ReplicaSet&lt;/em&gt; se encarga de crear un nuevo &lt;em&gt;pod&lt;/em&gt;. Para ello, el &lt;em&gt;ReplicaSet&lt;/em&gt; incluye una plantilla con la que crear nuevos &lt;em&gt;pods&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Así, el &lt;em&gt;ReplicaSet&lt;/em&gt; define el &lt;strong&gt;estado deseado&lt;/strong&gt; de la aplicación: cuántas copias de mi aplicación quiero tener en todo momento en ejecución en el clúster.
Modificando el número de réplicas para el &lt;em&gt;ReplicaSet&lt;/em&gt; podemos &lt;strong&gt;escalar&lt;/strong&gt; (incrementar o reducir) el número de copias en ejecución en función de las necesidades.&lt;/p&gt;

&lt;p&gt;Por tanto, parece que el mejor candidato para ponerse a definir ficheros &lt;code&gt;YAML&lt;/code&gt; y desplegar aplicaciones en el clúster de Kubernetes sería un &lt;em&gt;ReplicaSet&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Si embargo, la &lt;a href=&#34;https://kubernetes.io/docs/api-reference/v1.6/#replicaset-v1beta1-extensions&#34;&gt;documentación oficial&lt;/a&gt; nos ofrece otra opción:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In many cases it is recommended to create a Deployment instead of ReplicaSet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Es decir, tenemos una opción mejor: el &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;El &lt;em&gt;Deployment&lt;/em&gt; añade la capacidad de poder actualizar la aplicación definida en un &lt;em&gt;ReplicaSet&lt;/em&gt; sin pérdida de servicio, mediante &lt;strong&gt;actualización continua&lt;/strong&gt; (&lt;em&gt;rolling update&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Si el estado deseado de la aplicación son tres réplicas de un pod basado en &lt;code&gt;yomismo/app-1.0&lt;/code&gt; y queremos actualizar a &lt;code&gt;yomismo/app-2.0&lt;/code&gt;, el &lt;em&gt;Deployment&lt;/em&gt; se encarga de realizar la transición de la versión 1.0 a la 2.0 de forma que no haya interrupción del servicio. La estrategia de actualización puede definirse manualmente, pero sin entrar en detalles, Kubernetes se encarga de ir eliminado progresivamente las réplicas de la aplicación v1.0 y sustituirlas por las de la v2.0.&lt;/p&gt;

&lt;p&gt;El proceso se hace de forma controlada, por lo que si surgen problemas con la nueva versión de la aplicación, la actualización se detiene y es posible realizar &lt;em&gt;marcha atrás&lt;/em&gt; hacia la versión estable.&lt;/p&gt;

&lt;h2 id=&#34;resumiendo&#34;&gt;Resumiendo&lt;/h2&gt;

&lt;p&gt;Así pues, después de leer la sección de &lt;a href=&#34;https://kubernetes.io/docs/concepts/&#34;&gt;Concepts&lt;/a&gt; de la documentación de Kubernetes, parece que ya tengo claro cuál es el proceso para desplegar aplicaciones en Kubernetes.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;En Docker

&lt;ol&gt;
&lt;li&gt;Crear fichero &lt;code&gt;Dockerfile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Construir imagen personalizada&lt;/li&gt;
&lt;li&gt;Subir imagen a un &lt;em&gt;Registry&lt;/em&gt; (de momento, DockerHub)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;En Kubernetes

&lt;ol&gt;
&lt;li&gt;Crear fichero &lt;code&gt;YAML&lt;/code&gt; definiendo el &lt;em&gt;Deployment&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Crear &lt;em&gt;Deployment&lt;/em&gt; en el clúster&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hay otros objetos específicos que pueden ser más adecuados para tus necesidades:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;DaemonSets&lt;/em&gt; : despliegan una copia de un &lt;em&gt;pod&lt;/em&gt; en cada nodo del clúster. Por ejemplo, un antivirus, o una herramienta de gestión de logs, etc&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Jobs&lt;/em&gt; y &lt;em&gt;CronJobs&lt;/em&gt;: crean &lt;em&gt;pods&lt;/em&gt; hasta asegurar que un número determinado finaliza con éxito, lo que completa el &lt;em&gt;job&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;StatefulSets&lt;/em&gt; : todavía en Beta, asignan una identidad única a los &lt;em&gt;pods&lt;/em&gt;, lo que garantiza que se creen o escalen en un orden determinado.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;siguientes-pasos&#34;&gt;Siguientes pasos&lt;/h2&gt;

&lt;p&gt;Al final de este proceso tendré una aplicación &lt;em&gt;simple&lt;/em&gt; desplegada en el clúster. Con &amp;ldquo;sencilla&amp;rdquo; quiero decir que las diferentes instancias de la aplicación actuan de forma independiente. Un ejemplo sería un servidor web: con el &lt;em&gt;deployment&lt;/em&gt; sería posible escalar la aplicación para dar respuesta a la demanda en todo momento y actualizar el contenido de la web sin interrupciones.&lt;/p&gt;

&lt;p&gt;El siguiente paso es crear una aplicación &lt;em&gt;compleja&lt;/em&gt; en la que tengamos, por ejemplo, un &lt;em&gt;frontend&lt;/em&gt; y un &lt;em&gt;backend&lt;/em&gt;. Para estas situaciones, necesitaremos definir un &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34;&gt;&lt;strong&gt;servicio&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introduccion a YAML</title>
      <link>https://onthedock.github.io/post/170525-introduccion-a-yaml/</link>
      <pubDate>Thu, 25 May 2017 18:34:11 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170525-introduccion-a-yaml/</guid>
      <description>&lt;p&gt;YAML es el lenguaje en el que se definen los &lt;em&gt;pods&lt;/em&gt;, los &lt;em&gt;deployments&lt;/em&gt; y demás estructuras en Kubernetes. Todos los artículos que he leído sobre cómo crear un fichero de definición del &lt;em&gt;pod&lt;/em&gt; (&lt;em&gt;deployment&lt;/em&gt;, etc) se centran en el &lt;strong&gt;contenido&lt;/strong&gt; del fichero.&lt;/p&gt;

&lt;p&gt;Pero en mi caso, echaba de menos una explicación de &lt;strong&gt;cómo&lt;/strong&gt; se crea el fichero, qué reglas se siguen a la hora de &lt;em&gt;describir&lt;/em&gt; la configuración en formato YAML.&lt;/p&gt;

&lt;p&gt;Afortunadamente el lenguaje YAML es muy sencillo y basta con conocer un par de estructuras para crear los ficheros de configuración de Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;YAML es un lenguaje de marcado muy simple, basado en ficheros de texto plano legible por los humanos. Este formato se utiliza dentro del mundillo del software para almacenar información de tipo configuración.&lt;/p&gt;

&lt;p&gt;YAML son las siglas de &lt;em&gt;Yet Another Markup Language&lt;/em&gt; (Otro lenguaje de marcado más) o &lt;em&gt;YAML Ain&amp;rsquo;t Markup Language&lt;/em&gt; (YAML no es un lenguaje de marcado), depende de a quién preguntes.&lt;/p&gt;

&lt;p&gt;Usar YAML para las definiciones de Kubernetes proporciona las siguientes ventajas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conveniencia&lt;/strong&gt; No es necesario especificar todos los parámetros en la línea de comandos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mantenimiento&lt;/strong&gt; Los ficheros YAML puede ser gestionados por un sistema de control de versiones, de manera que se pueden registrar los cambios.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibilidad&lt;/strong&gt; Es posible crear estructuras mucho más complejas usando YAML de lo que puede conseguirse desde la línea de comandos.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;YAML es un superconjunto de JSON, lo que significa que cualquier fichero JSON válido también es un fichero YAML válido.&lt;/p&gt;

&lt;p&gt;Como consejos generales a la hora de crear un fichero YAML:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Usa siempre la codificación UTF-8 para evitar errores.&lt;/li&gt;
&lt;li&gt;No uses &lt;strong&gt;nunca&lt;/strong&gt; tabulaciones&lt;/li&gt;
&lt;li&gt;Usa una fuente monoespaciada para visualizar/editar el contenido de los ficheros YAML.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sólo necesitas conocer dos tipos de estructuras en YAML:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Listas&lt;/li&gt;
&lt;li&gt;Mapas&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A parte de los mapas y las listas, también te puede resultar útil saber que cualquier línea que comience con un &lt;code&gt;#&lt;/code&gt; se considera un comentario y es ignorada.&lt;/p&gt;

&lt;h2 id=&#34;mapas-yaml&#34;&gt;Mapas YAML&lt;/h2&gt;

&lt;p&gt;Los mapas te permiten asociar parejas de nombres y valores, lo que es conveniente cuando estás tratando con información relativa a configuraciones. Por ejemplo, puedes tener una configuración que empiece como:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;---
apiVersion: v1
kind: Pod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;La primera línea es un separador, y es opcional a no ser que trates de definir múltiples estructuras en un solo fichero. En el fichero puedes ver que tenemos dos valores, &lt;code&gt;v1&lt;/code&gt; y &lt;code&gt;Pod&lt;/code&gt;, asociados a dos claves, &lt;code&gt;apiVersion&lt;/code&gt; y &lt;code&gt;kind&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;No es necesario que los valores estén entrecomillados (con comillas simples o dobles), excepto para asegurarte de que no se interpreta algún caracter especial con un significado diferente a su valor literal.&lt;/p&gt;

&lt;p&gt;Las parejas clave-valor contenidas en un mapa se almacenan sin orden, por lo que puedes especificarlas en el orden que quieras.&lt;/p&gt;

&lt;p&gt;El fichero YAML anterior es equivalente a:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;---
kind: Pod
apiVersion: v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podemos anidar mapas dentro de mapas para crear estructuras más complejas, como:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;---
apiVersion: v1
kind: Pod
metadata:
   name: rss-site
   labels:
      app: web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En este caso tenemos un mapa llamado &lt;code&gt;metadata&lt;/code&gt; que contiene otros dos mapas; el primero &lt;code&gt;name: rss-site&lt;/code&gt; y el segundo, &lt;code&gt;labels&lt;/code&gt;, contiene como valor otro mapa &lt;code&gt;app: web&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Puedes anidar tantos mapas dentro de mapas como quieras.&lt;/p&gt;

&lt;p&gt;Para indicar que un mapa está contenido en otro, se usa la indentación. En el ejemplo anterior hemos usado una indentación de 3 espacios, pero el número de espacios no importa, siempre que sea &lt;strong&gt;consistente&lt;/strong&gt; en el fichero. El procesador de YAML interpreta las claves y valores en la misma profundidad de indentación como al mismo nivel (por ejemplo, &lt;code&gt;name&lt;/code&gt; y &lt;code&gt;labels&lt;/code&gt;), mientras que si están indentadas, interpreta que están &lt;em&gt;contenidas&lt;/em&gt; unos en otros (como en el caso de &lt;code&gt;labels&lt;/code&gt; y &lt;code&gt;app: web&lt;/code&gt;).&lt;/p&gt;

&lt;h2 id=&#34;listas-en-yaml&#34;&gt;Listas en YAML&lt;/h2&gt;

&lt;p&gt;Una lista, en YAML, es una secuencia de objetos, o lo que es lo mismo, una colección ordenada de valores. En este caso los valores no están asociados con una clave, sino con un índice posicional obtenido del orden en el que están especificados en la lista. Por ejemplo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;args
   - sleep
   - 1000
   - message
   - &amp;quot;Hello World!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Puede haber cualquier número de elementos en una lista.&lt;/p&gt;

&lt;p&gt;Como en el caso de las parejas clave-valor, los elementos de una lista se encuentran indentados con el mismo número de espacios bajo el identificador (la clave) de la lista; cada elemento de la lista va precedido por un &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Del mismo modo que podemos anidar mapas en mapas, podemos anidar listas en listas, mapas en listas y cualquier combinación imaginable. Un ejemplo de mapas y listas anidados:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;# Configuracion ficticia
spec:
   containers:
      - name: front-end
        image: nginx
        ports:
           - containerPort: 80
      - name: rss-reader
        image: xavi/rss-reader
        ports:
           - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En resumen, tenemos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mapas&lt;/strong&gt;, que son grupos no ordenados de parejas de clave y valor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Listas&lt;/strong&gt;, que son colecciones ordenadas de elementos individuales&lt;/li&gt;
&lt;li&gt;Mapas de mapas&lt;/li&gt;
&lt;li&gt;Mapas de listas&lt;/li&gt;
&lt;li&gt;Listas de mapas&lt;/li&gt;
&lt;li&gt;Listas de listas&lt;/li&gt;
&lt;li&gt;etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Básicamente, cualquier estructura que puedas imaginar, se puede construir a partir de estos dos elementos.&lt;/p&gt;

&lt;p&gt;Con estos conocimientos básicos, espero que ahora te resulte mucho más sencillo interpretar los ficheros de configuración de &lt;em&gt;pods&lt;/em&gt;, &lt;em&gt;deployments&lt;/em&gt;, etc. Y no sólo en Kubernetes; las configuraciones en formato YAML son usadas por un montón de productos diferentes.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Vagrant: primeras impresiones</title>
      <link>https://onthedock.github.io/post/170521-vagrant-primeras-impresiones/</link>
      <pubDate>Sun, 21 May 2017 09:26:45 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170521-vagrant-primeras-impresiones/</guid>
      <description>&lt;p&gt;He estado probando &lt;a href=&#34;https://www.vagrantup.com&#34;&gt;Vagrant&lt;/a&gt; para automatizar la creación de máquinas virtuales en las que probar Docker, etc.&lt;/p&gt;

&lt;p&gt;En este artículo comento mis primeras impresiones con Vagrant.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;La &lt;em&gt;tagline&lt;/em&gt; de Vagrant es &lt;em&gt;Development Environments made easy&lt;/em&gt;. Atraido con la posibilidad de ser capaz de crear entornos de forma automática, instalé Vagrant. Tenía la sensación de que sería una especie de &lt;em&gt;Docker para máquinas virtuales&lt;/em&gt;: hay un repositorio público de &lt;em&gt;boxes&lt;/em&gt; llamado &lt;a href=&#34;https://atlas.hashicorp.com/boxes/search?&#34;&gt;Atlas&lt;/a&gt; y con unos comandos como &lt;code&gt;vagrant init&lt;/code&gt; y &lt;code&gt;vagrant up&lt;/code&gt; parece posible &lt;em&gt;levantar&lt;/em&gt; un conjunto de máquinas preconfiguradas y listas para trabajar.&lt;/p&gt;

&lt;p&gt;Pensaba que Vagrant trabajaba únicamente con VirtualBox, así que me alegré al ver que es posible trabajar con otros &lt;em&gt;providers&lt;/em&gt;, en particular al ver que soporta Hyper-V &lt;em&gt;out of the box&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Busqué una máquina con Ubuntu 16 LTS en Atlas filtrando por &lt;code&gt;provider hyperv&lt;/code&gt; y encontré &lt;a href=&#34;https://atlas.hashicorp.com/kmm/boxes/ubuntu-xenial64&#34;&gt;https://atlas.hashicorp.com/kmm/boxes/ubuntu-xenial64&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Mediante &lt;code&gt;vagrant init kmm/ubuntu-xenial64&lt;/code&gt; se crea el Vagrantfile, que describe las configuración de la VM.&lt;/p&gt;

&lt;p&gt;Para arrancar la máquina es necesario especificar el &lt;em&gt;provider&lt;/em&gt;, ya que por defecto se asume VirtualBox: &lt;code&gt;vagrant up --provider hyperv&lt;/code&gt;. El comando debe ejecutarse con permisos de administrador (debido a una limitación de Hyper-V).&lt;/p&gt;

&lt;p&gt;Al lanzar el comando por primera vez, como no tengo una copia local de la &lt;em&gt;box&lt;/em&gt;, debe descargarse. A diferencia de los contenedores, el tamaño de la máquina virtual es considerable. Afortunadamente la velocidad de la conexión y el hecho de que sólo tengo que hacerlo una vez mitigan este primer inconveniente.&lt;/p&gt;

&lt;p&gt;El comando inicializa la máquina y la registra en Hyper-V con el nombre &lt;code&gt;ubuntu-xenial64&lt;/code&gt;, arranca la máquina y obtiene una IP del DHCP. También se crea una clave SSH para poder conectar a la máquina y se monta una carpeta compartida entre la máquina virtual y el &lt;em&gt;host&lt;/em&gt; local (en la carpeta desde donde se lanza el comando &lt;code&gt;vagrant up&lt;/code&gt;). La máquina virtual también se crea en esa carpeta.&lt;/p&gt;

&lt;h2 id=&#34;conectando-vía-ssh&#34;&gt;Conectando vía SSH&lt;/h2&gt;

&lt;p&gt;En la documentación se indica que para conectar a la VM, hay que usar el comando &lt;code&gt;vagrant ssh&lt;/code&gt;. En Windows no funciona porque no hay un cliente SSH instalado por defecto.&lt;/p&gt;

&lt;p&gt;En Windows lo habitual es usar PuTTY, pero resulta que la clave privada que se ha generado no es compatible y es necesario convertirla al formato &lt;code&gt;ppk&lt;/code&gt;, usando &lt;a href=&#34;https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html&#34;&gt;PuTTYgen&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Al no tener una contraseña, esto significa que si quiero conectar desde otro equipo (por ejemplo, desde el Mac), tengo que copiar la clave privada al equipo.&lt;/p&gt;

&lt;h2 id=&#34;nombre-de-la-máquina-virtual&#34;&gt;Nombre de la máquina virtual&lt;/h2&gt;

&lt;p&gt;Aunque en la documentación se indica que puede especificarse el nombre de la máquina virtual mediante la opción &lt;a href=&#34;https://www.vagrantup.com/docs/hyperv/configuration.html#vmname&#34;&gt;&lt;code&gt;vmname&lt;/code&gt;&lt;/a&gt;, en mi caso no ha funcionado.&lt;/p&gt;

&lt;h2 id=&#34;ip-de-la-máquina-virtual&#34;&gt;IP de la máquina virtual&lt;/h2&gt;

&lt;p&gt;Aunque la máquina virtual arranca con la IP configurada en modo DHCP, me gustaría especificar la IP de la máquina a crear. Buscando en Google he encontrado que la IP debería poder configurarse mediante el parámetro: &lt;code&gt;config.vm.network :public_network, ip: &amp;quot;192.168.1.30&amp;quot;&lt;/code&gt;, tal y como se indica en &lt;a href=&#34;https://serverfault.com/questions/418422/public-static-ip-for-vagrant-box&#34;&gt;Public static ip for vagrant box&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;De nuevo, por algún motivo, no ha funcionado, obteniendo siempre la IP vía DHCP.&lt;/p&gt;

&lt;p&gt;No soy el único al que le pasa, por lo que parece: &lt;a href=&#34;https://github.com/cogitatio/vagrant-hostsupdater/issues/132&#34;&gt;static IP not set correctly&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;instalación-de-docker&#34;&gt;Instalación de Docker&lt;/h2&gt;

&lt;p&gt;Al final del fichero &lt;code&gt;Vagrantfile&lt;/code&gt; hay un apartado sobre &lt;em&gt;provisioning&lt;/em&gt;, para poder instalar paquetes adicionales en la máquina virtual. En mi caso, estaba interesado en instalar Docker, por ejemplo.&lt;/p&gt;

&lt;p&gt;Aunque en la documentación se indica que se pueden lanzar los comandos &lt;em&gt;tal cual&lt;/em&gt;, de nuevo no ha funcionado como esperaba:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;apt-get update
apt-get install docker.io -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tras el primer fallo he modificado el fichero &lt;code&gt;Vagrantfile&lt;/code&gt; para que los comandos se lancen como &lt;code&gt;root&lt;/code&gt; (&lt;code&gt;sudo apt-get update&lt;/code&gt;), pero no sólo no he solucionado el problema, sino que además al intentar lanzar manualmente, conectado a la máquina virtual la instalación, obtenía un error indicando que el fichero estaba en uso.&lt;/p&gt;

&lt;h2 id=&#34;montaje-de-la-carpeta-compartida&#34;&gt;Montaje de la carpeta compartida&lt;/h2&gt;

&lt;p&gt;Inicialmente he pensado que sería una buena idea poder disponer de una carpeta &lt;em&gt;de intercambio&lt;/em&gt;. Después de tener que lanzar múltiples máquinas virtuales para las pruebas de cambiar el nombre de la máquina virtual, la configuración de la IP, etc, me he dado cuenta que la carpeta compartida no sólo no me aporta nada, sino que ralentiza el proceso; se solicita usuario y password para montar la carpeta, de manera que el script se detiene (aunque la máquina arranca en Hyper-V). Incluso después de terminar el proceso mediante &lt;code&gt;Ctrl+C&lt;/code&gt; he tenido problemas para seguir ejecutando otros comandos (&lt;code&gt;vagrant destroy&lt;/code&gt;). En este caso, Ruby -el lenguaje usado por Vagrant- seguía en memoria y ha sido necesario matarlo a través del adminstrador de tareas para poder seguir ejecutando comandos Vagrant.&lt;/p&gt;

&lt;h2 id=&#34;ubicación-de-la-máquina-virtual&#34;&gt;Ubicación de la máquina virtual&lt;/h2&gt;

&lt;p&gt;Otro problema que me he encontrado es que la máquina virtual se crea en la misma carpeta desde donde se lanza el fichero &lt;code&gt;Vagrantfile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;En Hyper-V se puede definir una ruta por defecto donde almacenar las máquinas virtuales, pero parece que Vagrant ignora esta configuración.&lt;/p&gt;

&lt;p&gt;En el equipo de laboratorio tengo dos discos, un SSD y un disco mecánico y he distribuido las máquinas virtuales según mis preferencias. Así que el hecho de que Vagrant cree las máquinas sin tener en cuenta la configuración del proveedor Hyper-V supone un problema que debería corregir, buscando en la configuración de Vagrant (si es que es posible modificar este comportamiento).&lt;/p&gt;

&lt;h2 id=&#34;conclusión&#34;&gt;Conclusión&lt;/h2&gt;

&lt;p&gt;Mi objetivo es automatizar la creación de una nueva máquina virtual con Docker y/o Kubernetes instalado.&lt;/p&gt;

&lt;p&gt;Tengo dos máquinas virtuales exportadas con Docker y Kubernetes, respectivamente, por lo que Vagrant no aporta nada que no pueda hacer ahora mismo importando las máquinas en Hyper-V.&lt;/p&gt;

&lt;p&gt;Mediante un script en Powershell importo la máquina virtual en Hyper-V con el nombre especificado; no he conseguido hacer lo mismo en Vagrant.&lt;/p&gt;

&lt;p&gt;Personalmente prefiero crear un par de scripts con Powershell para conseguir especificar la IP, el nombre del host, etc que no tener que lidiar con el fichero de configuración de Vagrant.&lt;/p&gt;

&lt;p&gt;Después de importar la máquina o de crearla vía Vagrant, tengo que conectar igualmente a la VM para cambiar el &lt;code&gt;hostname&lt;/code&gt; y especificar una IP estática. Al usar la importación en Hyper-V ya tengo instalado Docker y/o Kubernetes, que en las máquinas creadas con Vagrant debo instalar manualmente.&lt;/p&gt;

&lt;p&gt;No tengo claro si los problemas que he encontrado con Vagrant se deben a mi desconocimiento del producto o a problemas de &lt;em&gt;concepto&lt;/em&gt;. Quizás es el hecho de usar un proveedor diferente al que se usa por defecto (Hyper-V vs VirtualBox) o al hecho de que el equipo donde se ejecutan las máquinas virtuales es diferente al equipo de &lt;em&gt;desarrollo&lt;/em&gt; (por el fichero &lt;code&gt;private_key&lt;/code&gt; para conectar vía SSH).&lt;/p&gt;

&lt;p&gt;En cualquier caso, Vagrant no se adapta a mis necesidades, creando fricción, por lo que seguiré buscando otras soluciones para automatizar la creación de las máquinas virtuales.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Normas para estructurar ficheros implicados en la creación de contenedores</title>
      <link>https://onthedock.github.io/post/170520-normas-para-estructurar-ficheros-implicados-en-la-creacion-de-contenedores/</link>
      <pubDate>Sat, 20 May 2017 19:59:44 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170520-normas-para-estructurar-ficheros-implicados-en-la-creacion-de-contenedores/</guid>
      <description>&lt;p&gt;El proceso desde la creación a la ejecución del contenedor se puede separar en varias fases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Creación de la imagen (mediante la redacción de un fichero &lt;code&gt;Dockerfile&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Construcción de la imagen&lt;/li&gt;
&lt;li&gt;Ejecución del contenedores&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Para tener los diferentes ficheros implicados en el proceso organizados de forma homogénea, me he autoimpuesto las siguientes reglas a la hora de estructurar los repositorios.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h2&gt;

&lt;p&gt;El primero paso para ejecutar un contenedor es crear la imagen en la que está basado. Para ello debes crear un fichero &lt;code&gt;Dockerfile&lt;/code&gt; en el que se indica la imagen base usada y los diferentes pasos de instalación de paquetes, configuración de usuarios, volúmenes y puertos expuestos.&lt;/p&gt;

&lt;p&gt;En la creación de la imagen intervienen, además del fichero &lt;code&gt;Dockerfile&lt;/code&gt;, ficheros de configuración, etc que se copian a la imagen desde la carpeta donde se encuentra el fichero &lt;code&gt;Dockerfile&lt;/code&gt; (el llamado &lt;em&gt;contexto&lt;/em&gt;, ver &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/build/#options&#34;&gt;Documentación oficial de &lt;code&gt;docker build&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Para gestionar los cambios sobre estos ficheros, lo más sencillo es guardarlos en un repositorio y tener un registro de todos los cambios que se van introduciendo a lo largo del tiempo.&lt;/p&gt;

&lt;p&gt;Todos los ficheros relacionados con la &lt;em&gt;creación&lt;/em&gt; de la imagen se colocan en una carpeta llamada &lt;code&gt;build&lt;/code&gt;, con el &lt;code&gt;Dockerfile&lt;/code&gt; y los ficheros de configuración, etc, agrupados en sus correspondientes carpetas.&lt;/p&gt;

&lt;p&gt;En esta carpeta también se incluyen un fichero con instrucciones para la creación de la imagen (condiciones en las que reutilizar la cache, puntos a tener en cuenta, etc) y un &lt;em&gt;script&lt;/em&gt; para lanzar la creación de la imagen de forma siempre igual (quizás el script borra ficheros temporales o descargados en ejecuciones anteriores, por ejemplo).&lt;/p&gt;

&lt;h2 id=&#34;construcción-de-la-imagen&#34;&gt;Construcción de la imagen&lt;/h2&gt;

&lt;p&gt;Una vez creado el &lt;code&gt;Dockerfile&lt;/code&gt;, &lt;em&gt;construyes&lt;/em&gt; la imagen mediante &lt;code&gt;docker build&lt;/code&gt;. Aunque en general la construcción se realiza mediante un sólo comando de la forma &lt;code&gt;docker build -t {repositorio/etiqueta} .&lt;/code&gt;, puede ser interesante disponer de documentación con indicaciones sobre las reglas de etiquetado de la imagen definidas por la empresa o similar.&lt;/p&gt;

&lt;h2 id=&#34;ejecución-del-contenedor&#34;&gt;Ejecución del contenedor&lt;/h2&gt;

&lt;p&gt;Finalmente la creación de contenedores basados en la imagen se realiza mediante un comando &lt;code&gt;docker run&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A la hora de ejecutar el contenedor la instrucción puede incluir el nombre del contenedor final, la relación entre puertos del &lt;em&gt;host&lt;/em&gt; y el contenedor, el montaje de volúmenes, etc. En algunos casos, el contenedor admite parámetros que se pasan al comando definido en la instrucción &lt;code&gt;CMD&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Para evitar errores o simplemente para no teclear una y otra vez comandos larguísimos para ejecutar el contenedor, podemos crear un &lt;em&gt;script&lt;/em&gt; que lance el contenedor con los parámetros necesarios, así como documentación de la funcionalidad proporcionada por el contenedor, etc.&lt;/p&gt;

&lt;p&gt;Estos ficheros se guardan en el carpeta llamada &lt;code&gt;run&lt;/code&gt;; básicamente el comando para lanzar la creación del contenedor de forma homogénea y las instrucciones con información sobre el uso del contenedor, volúmenes, etc.&lt;/p&gt;

&lt;h2 id=&#34;carpetas&#34;&gt;Carpetas&lt;/h2&gt;

&lt;p&gt;Para estructurar todos los ficheros implicados en el proceso de creación de un contenedor he definido la siguiente estructura de carpetas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./nombre-contenedor/
 |
 ├─Readme.md
 ├─build/
 | ├─Dockerfile
 | ├─build.sh
 | ├─Build-Instructions.md
 | ├─{context-files}/
 | ├─...
 | ├─{context-files}/
 ├─run/
 | ├─run.sh
 | ├─Run-Instructions.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;motivación&#34;&gt;Motivación&lt;/h2&gt;

&lt;p&gt;No he encontrado ningún artículo sobre la organización de los ficheros implicados en el creación de imágenes o de los flujos de trabajo asociados a estos procesos. Tampoco sobre las normas a la hora de etiquetar las imágenes o si se realizan validaciones a la hora de obtener/subir imágenes de repositorios públicos.&lt;/p&gt;

&lt;p&gt;Incluso en una empresa en la que el proceso de desarrollo y operación de las aplicaciones gire alrededor del concepto &lt;em&gt;DevOp&lt;/em&gt;, puede haber otros implicados en el proceso &lt;em&gt;administrativo&lt;/em&gt; del ciclo de vida de la aplicación: decisiones estratégicas, a nivel de seguridad, de &lt;em&gt;compliance&lt;/em&gt; con leyes como la protección de datos, etc.&lt;/p&gt;

&lt;p&gt;En los artículos/conferencias lo habitual es explicar soluciones técnicas sin entrar nunca en estos procesos que relacionan IT con el resto de departamentos de la empresa.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>El nodo k3 sigue colgandose por culpa de Flannel</title>
      <link>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</link>
      <pubDate>Wed, 17 May 2017 21:02:21 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</guid>
      <description>&lt;p&gt;En la entrada &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt; encontré restos de la instalación de &lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt; en la Raspberry Pi. Eliminé los &lt;em&gt;pods&lt;/em&gt; que hacían referencia a Flannel y conseguí que el nodo &lt;strong&gt;k2&lt;/strong&gt; no se volviera a colgar.&lt;/p&gt;

&lt;p&gt;Sin embargo, el problema sigue dándose en el nodo &lt;strong&gt;k3&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Revisando el contenido de &lt;code&gt;/var/lib/kubernetes/pods/&lt;/code&gt; he visto que algunos &lt;em&gt;pods&lt;/em&gt; hacían referencia, todavía, a Flannel.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~439642480.deleting~747926470.deleting~067946013.deleting~791070092.deleting~964331938.deleting~717873461.deleting~755129373.deleting~499171027
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~504683568.deleting~184413472.deleting~138413964.deleting~985160408.deleting~943143520.deleting~459558341.deleting~578589077.deleting~501462031.deleting~769373718
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~613861491.deleting~841547526.deleting~012178845.deleting~177797190.deleting~192052322.deleting~958792988.deleting~338401309.deleting~623810479.deleting~369130424
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~288881360.deleting~534630955.deleting~520377076.deleting~598159984.deleting~426698803.deleting~142931759.deleting~872800923.deleting~808586860
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~470691428.deleting~439642480.deleting~747926470.deleting~067946013.deleting~791070092.deleting~622848191.deleting~646325460.deleting~868409130.deleting~824166496
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~668636622.deleting~334825066.deleting~737147422.deleting~055159245.deleting~572255670.deleting~485248219.deleting~690855316.deleting~753094008.deleting~457647557
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~756292309.deleting~273222811.deleting~039503494.deleting~182629307.deleting~984614903.deleting~081831640.deleting~628560452.deleting~303652395.deleting~450650534
/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap/wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~883601122.deleting~535739903.deleting~385002935.deleting~558075878.deleting~174007749.deleting~757820208.deleting~194356513.deleting~813327027.deleting~485662152
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta vez he detectado el problema al intentar calcular el espacio usado por esta carpeta, ya que la Raspberry Pi se ha quedado como &amp;ldquo;colgada&amp;rdquo;, aunque al lanzar &lt;em&gt;htop&lt;/em&gt; no se observaba un uso excesivo de CPU.&lt;/p&gt;

&lt;p&gt;Finalmente, he usado el mismo sistema que la otra vez: eliminar todas las subcarpetas de cada uno de los &lt;em&gt;pods&lt;/em&gt; (dejando únicamente los que no se pueden borrar al estar en uso).&lt;/p&gt;

&lt;p&gt;Después de la purga masiva de &lt;code&gt;rm -rf /var/lib/kubelet/pods/&lt;/code&gt; sólo han quedado dos carpetas &lt;em&gt;en uso&lt;/em&gt;; el número corresponde con el número de &lt;em&gt;pods&lt;/em&gt; planificados sobre el nodo &lt;strong&gt;k3&lt;/strong&gt; desde &lt;code&gt;kubectl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                         READY     STATUS    RESTARTS   AGE       IP             NODE
kube-system   etcd-k1                      1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-apiserver-k1            1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-controller-manager-k1   1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-dns-279829092-1b27r     3/3       Running   12         36d       10.32.0.2      k1
kube-system   kube-proxy-20t3b             1/1       Running   0          25m       192.168.1.13   k3
kube-system   kube-proxy-3dggd             1/1       Running   4          36d       192.168.1.11   k1
kube-system   kube-proxy-5b8k3             1/1       Running   2          12d       192.168.1.12   k2
kube-system   kube-scheduler-k1            1/1       Running   4          36d       192.168.1.11   k1
kube-system   weave-net-6qr0l              2/2       Running   8          36d       192.168.1.11   k1
kube-system   weave-net-mxp2w              2/2       Running   0          25m       192.168.1.13   k3
kube-system   weave-net-tmmdj              2/2       Running   4          12d       192.168.1.12   k2
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Un reinicio y ¡listo!, problema -espero- resuelto de forma definitiva.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Protege el acceso remoto via API a Docker</title>
      <link>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sun, 07 May 2017 18:33:16 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170507-protege-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; explicaba cómo configurar el acceso remoto al API de Docker. El problema es que de esta forma no hay manera de restringir el acceso.&lt;/p&gt;

&lt;p&gt;En este artículo protegemos el acceso usando TLS de manera que sólo se permitan conexiones que presenten un certificado firmado por una CA de confianza.
&lt;/p&gt;

&lt;p&gt;Seguiremos las instrucciones oficiales de Docker &lt;a href=&#34;https://docs.docker.com/engine/security/https/&#34;&gt;Protect the Docker daemon socket&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;creamos-una-ca-claves-para-el-cliente-y-el-servidor-con-openssl&#34;&gt;Creamos una CA, claves para el cliente y el servidor con OpenSSL&lt;/h2&gt;

&lt;p&gt;Primero, en la máquina &lt;em&gt;host&lt;/em&gt; del Docker &lt;em&gt;daemon&lt;/em&gt;, generamos las claves públicas y privadas de la CA (&lt;em&gt;Certification Authority&lt;/em&gt;, la entidad certificadora):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -aes256 -out ca-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................++
..........................++
e is 65537 (0x10001)
Enter pass phrase for ca-key.pem:
Verifying - Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Y a continuación:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem
Enter pass phrase for ca-key.pem:
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &#39;.&#39;, the field will be left blank.
-----
Country Name (2 letter code) [AU]:ES
State or Province Name (full name) [Some-State]:Barcelona
Locality Name (eg, city) []:Barcelona
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Ameisin
Organizational Unit Name (eg, section) []:DevOps
Common Name (e.g. server FQDN or YOUR name) []:192.168.1.20
Email Address []: {REDACTED}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora que tenemos una CA, podemos crear la clave para el servidor y la petición de firmado del certificado (&lt;em&gt;certificate signing request&lt;/em&gt;, CSR). Por favor, verifica que &lt;code&gt;Common Name&lt;/code&gt; (es decir, el &lt;em&gt;FQDN&lt;/em&gt; o &lt;em&gt;YOUR Name&lt;/em&gt;) coincide con el nombre del &lt;em&gt;host&lt;/em&gt; que vas a usar para conectar a Docker.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out server-key.pem 4096
Generating RSA private key, 4096 bit long modulus
............................................................................................................................................................................................................................................................................................................................++
...............................................................++
e is 65537 (0x10001)
# openssl req -subj &amp;quot;/CN=192.168.1.20&amp;quot; -sha256 -new -key server-key.pem -out server.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación vamos a firmar la clave pública con nuestra CA.&lt;/p&gt;

&lt;p&gt;Como las conexiones TLS pueden realizarse usando la dirección IP o un nombre DNS, deben especificarse durante la creación del certificado.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo subjectAltName = IP:192.168.1.20,IP:127.0.0.1 &amp;gt; extfile.cnf
# openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out server-cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=192.168.1.20
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;autenticación-del-cliente&#34;&gt;Autenticación del cliente&lt;/h2&gt;

&lt;p&gt;Para autenticar al cliente, crearemos una clave de cliente y una petición de firmado del certificado.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Para simplificar, los siguientes dos pasos pueden realizarse desde la máquina donde se encuentra el Docker &lt;em&gt;daemon&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl genrsa -out key.pem 4096
Generating RSA private key, 4096 bit long modulus
...............................................................................++
................................................................................................................................................................................++
e is 65537 (0x10001)
# openssl req -subj &#39;/CN=client&#39; -new -key key.pem -out client.csr
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para que la clave permita autenticar al cliente, creamos un fichero de configuración de extensiones:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# echo extendedKeyUsage = clientAuth &amp;gt; extfile.cnf
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora firmamos la clave:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \
&amp;gt;   -CAcreateserial -out cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=client
Getting CA Private Key
Enter pass phrase for ca-key.pem:
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Después de haber generado &lt;code&gt;cert.pem&lt;/code&gt; y &lt;code&gt;server-cert.pem&lt;/code&gt; podemos eliminar las peticiones de firmado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# ls
ca-key.pem  ca.srl    client.csr  extfile.cnf  server-cert.pem	server-key.pem
ca.pem	    cert.pem  key.pem     server.csr
# rm -v client.csr server.csr
removed ‘client.csr’
removed ‘server.csr’
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;protección-de-las-claves&#34;&gt;Protección de las claves&lt;/h2&gt;

&lt;p&gt;Con una máscara &lt;code&gt;umask&lt;/code&gt; por defecto de &lt;code&gt;022&lt;/code&gt; las claves secretas que hemos generado dan a todo el mundo acceso de lectura y de escritura a tu usuario y tu grupo.&lt;/p&gt;

&lt;p&gt;Para proteger las claves de daños accidentales, vamos a eliminar los permisos de escritura sobre ellas. Para hacerlas de sólo lectura para tu usuario, usamos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0400 ca-key.pem key.pem server-key.pem
mode of ‘ca-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
mode of ‘server-key.pem’ changed from 0644 (rw-r--r--) to 0400 (r--------)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Los certificados pueden ser leídos por todo el mundo, pero para evitar daños accidentales, mejor eliminamos los permisos de escritura:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# chmod -v 0444 ca.pem server-cert.pem cert.pem
mode of ‘ca.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘server-cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
mode of ‘cert.pem’ changed from 0644 (rw-r--r--) to 0444 (r--r--r--)
#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configurando-el-api-de-acceso-remoto-de-forma-segura&#34;&gt;Configurando el API de acceso remoto de forma segura&lt;/h2&gt;

&lt;p&gt;Para hacer que el Docker &lt;em&gt;daemon&lt;/em&gt; sólo acepte conexiones de clientes que proporcionen un certificado de confianza de tu CA.&lt;/p&gt;

&lt;p&gt;Para ello, modificamos las opciones de arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H=0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De manera que quede como (lo he dividido en varias líneas por claridad):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ExecStart=/usr/bin/dockerd --tlsverify 		\
         --tlscacert=/root/ca.pem 		\
         --tlscert=/root/server-cert.pem 	\
         --tlskey=/root/server-key.pem 		\
         -H=0.0.0.0:2376 			\
         -H fd://
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A continuación, recargamos la configuración y reinciamos el servicio:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Los primeros intentos de arrancar el &lt;em&gt;daemon&lt;/em&gt; han fallado; ha sido necesario especificar la ruta completa a los certificados y las claves para conseguir que el servicio arrancara.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finalmente, comprobamos que podemos acceder usando el certificado con &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# curl https://192.168.1.20:2376/version --cert /root/cert.pem --key /root/key.pem --cacert /root/ca.pem
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A diferencia de lo que pasaba antes, cuando se intenta acceder a &lt;code&gt;https://192.168.1.9:2376/version&lt;/code&gt; desde otro equipo (sin usar el certificado), obtenemos un error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-url&#34;&gt;This site can’t be reached
192.168.1.9 refused to connect.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Configura un endpoint remoto en Portainer</title>
      <link>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</link>
      <pubDate>Sat, 06 May 2017 17:38:20 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-configura-un-endpoint-remoto-en-portainer/</guid>
      <description>&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170429-portainer-para-gestionar-tus-contenedores-en-docker/&#34;&gt;Portainer para gestionar tus contenedores en Docker&lt;/a&gt; usamos &lt;strong&gt;Portainer&lt;/strong&gt; para gestionar el Docker Engine local.&lt;/p&gt;

&lt;p&gt;En el artículo &lt;a href=&#34;https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/&#34;&gt;Habilita el API remoto de Docker&lt;/a&gt; habilitamos el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;En este artículo configuramos &lt;strong&gt;Portainer&lt;/strong&gt; para conectar con un &lt;em&gt;endpoint&lt;/em&gt; remoto (el API expuesta de un Docker Engine).
&lt;/p&gt;

&lt;p&gt;Accede a &lt;strong&gt;Portainer&lt;/strong&gt; y selecciona &lt;em&gt;Endpoints&lt;/em&gt; en el panel izquierdo.&lt;/p&gt;

&lt;p&gt;Para configurar el &lt;em&gt;endopoint&lt;/em&gt; remoto (no seguro) sólo necesitas proporcionar un nombre para el &lt;em&gt;endpoint&lt;/em&gt; y la URL de acceso:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/1-configure-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/1-configure-endpoint.png&#34; width=935 height=660 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Configura un nuevo endpoint
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;


&lt;p&gt;Para identificar qué Docker Engine estoy viendo en cada momento, indico la IP de la máquina, seguido de la plataforma y el &lt;em&gt;host&lt;/em&gt; en el que se encuentra.&lt;/p&gt;

&lt;p&gt;Para cambiar entre los diferentes &lt;em&gt;endpoints&lt;/em&gt; definidos en &lt;strong&gt;Portainer&lt;/strong&gt;, selecciona el que quieres gestionar en el desplegable de la parte superior del panel lateral:&lt;/p&gt;

&lt;figure&gt;
  &lt;amp-img src=&#34;https://onthedock.github.io/images/170506/2-change-endpoint.png&#34; alt=&#34;Configura un endpoint remoto en Portainer images/170506/2-change-endpoint.png&#34; width=450 height=168 layout=&#34;responsive&#34;&gt;&lt;/amp-img&gt;
  
  &lt;figcaption&gt;
    
    Cambia entre los diferentes endpoints definidos
    
  &lt;/figcaption&gt;
  
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Habilita el acceso remoto vía API a Docker</title>
      <link>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</link>
      <pubDate>Sat, 06 May 2017 15:23:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-habilita-el-acceso-remoto-via-api-a-docker/</guid>
      <description>&lt;p&gt;Portainer permite gestionar &lt;em&gt;endpoints&lt;/em&gt; remotos para Docker (y Docker Swarm) mediante el API REST de Docker Engine. El problema es que el API está desactivado por defecto.&lt;/p&gt;

&lt;p&gt;A continuación indico cómo activar y verificar el acceso remoto al API de Docker Engine.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Buscando en Google cómo habilitar el API remoto de Docker Engine probablemente encuentres el artículo
&lt;a href=&#34;https://www.ivankrizsan.se/2016/05/18/enabling-docker-remote-api-on-ubuntu-16-04/&#34;&gt;Enabling Docker Remote API on Ubuntu 16.04&lt;/a&gt;. Como bien dice en el párrafo inicial, no es fácil encontrar unas instrucciones claras sobre cómo configurar el API de principio a fin.&lt;/p&gt;

&lt;p&gt;Lanzando &lt;code&gt;docker man&lt;/code&gt;, vemos que la opción que buscamos es:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-H, --host=[unix:///var/run/docker.sock]: tcp://[host]:[port][path] to bind or
       unix://[/path/to/socket] to use.
         The socket(s) to bind to in daemon mode specified using one or more
         tcp://host:port/path, unix:///path/to/socket, fd://* or fd://socketfd.
         If the tcp port is not specified, then it will default to either 2375 when
         --tls is off, or 2376 when --tls is on, or --tlsverify is specified.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta opción debe pasarse en el arranque del &lt;em&gt;daemon&lt;/em&gt; de Docker. Para configurar esta opción durante el arranque de Docker Engine tenemos dos opciones:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;modificar el arranque del &lt;em&gt;daemon&lt;/em&gt; modificando la configuración de &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;añadiendo las opciones en el fichero de configuración de Docker Engine. Para sistemas Linux con &lt;em&gt;systemd&lt;/em&gt;, la &lt;a href=&#34;https://docs.docker.com/engine/admin/systemd/#start-automatically-at-system-boot&#34;&gt;configuración del &lt;em&gt;daemon&lt;/em&gt; de Docker&lt;/a&gt; se realiza a través del fichero &lt;code&gt;daemon.json&lt;/code&gt; ubicado en &lt;code&gt;/etc/docker/&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;He intentado configurar Docker Engine mediante el segundo método &lt;em&gt;daemon.json&lt;/em&gt; pero no he sido capaz de activar el API.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Primero, hacemos una copia de seguridad del fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# cp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.original
#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editamos el fichero &lt;code&gt;/lib/systemd/system/docker.service&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# nano /lib/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target docker.socket firewalld.service
Wants=network-online.target
Requires=docker.socket

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd -H fd:// 
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=1048576
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process
# restart the docker process if it exits prematurely
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modificamos la línea &lt;code&gt;ExecStart=/usr/bin/dockerd -H fd://&lt;/code&gt; y añadimos: &lt;code&gt;-H tcp://0.0.0.0:2375&lt;/code&gt; de manera que quede:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Esto hace que &lt;em&gt;dockerd&lt;/em&gt; escuche en todas las interfaces disponibles. En el caso de la máquina virtual en la que estoy probando, sólo tengo una, pero lo correcto sería especificar la dirección IP donde quieres que escuche &lt;em&gt;dockerd&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Guardamos los cambios.&lt;/p&gt;

&lt;p&gt;Recargamos la configuración y reiniciamos el servicio.&lt;/p&gt;

&lt;p&gt;Para comprobar que hemos el API funciona, lanzamos una consulta usando &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# systemctl daemon-reload
# systemctl restart docker
# curl http://localhost:2375/version
{&amp;quot;Version&amp;quot;:&amp;quot;17.05.0-ce&amp;quot;,&amp;quot;ApiVersion&amp;quot;:&amp;quot;1.29&amp;quot;,&amp;quot;MinAPIVersion&amp;quot;:&amp;quot;1.12&amp;quot;,&amp;quot;GitCommit&amp;quot;:&amp;quot;89658be&amp;quot;,&amp;quot;GoVersion&amp;quot;:&amp;quot;go1.7.5&amp;quot;,&amp;quot;Os&amp;quot;:&amp;quot;linux&amp;quot;,&amp;quot;Arch&amp;quot;:&amp;quot;amd64&amp;quot;,&amp;quot;KernelVersion&amp;quot;:&amp;quot;3.16.0-4-amd64&amp;quot;,&amp;quot;BuildTime&amp;quot;:&amp;quot;2017-05-04T22:04:27.257991431+00:00&amp;quot;}
#
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Debes tener en cuenta que esta configuración &lt;strong&gt;supone un riesgo de seguridad&lt;/strong&gt; al permitir el acceso al API de Docker Engine sin ningún tipo de control.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Tags, categorias y organización en Hugo</title>
      <link>https://onthedock.github.io/post/170506-tags-categorias-archetypes-en-hugo/</link>
      <pubDate>Sat, 06 May 2017 06:23:50 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-tags-categorias-archetypes-en-hugo/</guid>
      <description>&lt;p&gt;A medida que aumenta el número de artículos me he dado cuenta de que es necesario tener algún conjunto de reglas para organizar los ficheros que componen el blog.&lt;/p&gt;

&lt;p&gt;El problema no está en los ficheros de Hugo, sino en los ficheros generados por mi: artículos, imágenes, etc.
&lt;/p&gt;

&lt;p&gt;Hugo genera sitios web a partir de un conjunto de ficheros organizados en carpetas: el contenido se encuentra en &lt;code&gt;$HUGO/content&lt;/code&gt;, las imágenes en &lt;code&gt;$HUGO/static/images&lt;/code&gt; (o dentro de la carpeta equivalente del &lt;em&gt;theme&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Con unos pocos artículos, es fácil identificar qué fichero corresponde a un artículo concreto (simplemente a partir del título). Pero cuando el número de ficheros aumenta, la cosa se complica; el sistama operativo ordena los ficheros por orden alfabético, mientras que en el blog están organizados por fecha de creación. Para complicar más las cosas, el nombre del fichero &lt;em&gt;puede&lt;/em&gt; no ser el mismo que el título del artículo.&lt;/p&gt;

&lt;p&gt;Pasa algo parecido con las imágenes; con unas pocas no hay problema, pero cuando tenga un montón, será complicado identificar qué imagen corresponde a cada artículo.&lt;/p&gt;

&lt;p&gt;Para evitar complicaciones, creo que lo mejor es definir unas reglas sobre cómo organizar el contenido y definir &lt;em&gt;consistentemente&lt;/em&gt; etiquetas, categorías, etc.&lt;/p&gt;

&lt;h2 id=&#34;categorías&#34;&gt;Categorías&lt;/h2&gt;

&lt;p&gt;El blog está orientado al &lt;em&gt;DIY tecnológico&lt;/em&gt; en el aprendizaje sobre contenedores y tecnologías relacionadas.&lt;/p&gt;

&lt;p&gt;En este sentido, para simplificar, he decidido limitarme a dos categorías básicas &lt;code&gt;Dev&lt;/code&gt; y &lt;code&gt;Ops&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;De momento estoy montando el clúster de Kubernetes, realizando troubleshooting, etc, por lo que la mayoría de artículos son de la categoría &lt;code&gt;Ops&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Cuando tenga el clúster montado y pueda crear &lt;em&gt;pods&lt;/em&gt; (&lt;em&gt;replication controllers&lt;/em&gt;, etc), empezaré con la parte más &lt;code&gt;Dev&lt;/code&gt;; de momento, los únicos artículos &lt;code&gt;Dev&lt;/code&gt; son los relacionados con Hugo, su configuración, etc.&lt;/p&gt;

&lt;h2 id=&#34;etiquetas&#34;&gt;Etiquetas&lt;/h2&gt;

&lt;p&gt;El objetivo de las etiquetas es permitir organizar de forma flexible los artículos en conjuntos relacionados. Esta flexibilidad puede degenerar rápidamente en un montón de etiquetas que se usan sólo una vez y que no aportan nada.&lt;/p&gt;

&lt;p&gt;Para evitar definir un conjunto de etiquetas estricto -y perder flexibilidad- he pensado que lo mejor es definir unas &lt;em&gt;reglas&lt;/em&gt; sobre qué etiquetas son necesarias en cada artículo.&lt;/p&gt;

&lt;p&gt;Ya he utilizado este sistema en otra ocasión y ha funcionado mucho mejor que otras alternativas que intenté en el pasado.&lt;/p&gt;

&lt;p&gt;La primera etiqueta se refiere a la arquitectura; puede ser &lt;code&gt;x64&lt;/code&gt; o &lt;code&gt;arm&lt;/code&gt;, básicamente. En el primer caso entran tanto equipos físicos como máquinas virtuales; en este caso, no uso ninguna etiqueta concreta (esta es la arquitectura &lt;em&gt;default&lt;/em&gt; para contenedores).&lt;/p&gt;

&lt;p&gt;En el segundo tenemos las Raspberry Pi, para las que uso la etiqueta &lt;code&gt;RASPBERRY PI&lt;/code&gt;, aunque igual me planteo usar &lt;code&gt;ARM&lt;/code&gt; (o las dos). Esto es porque no descarto &lt;em&gt;ampliar la familia&lt;/em&gt; e incorporar alguna Orange Pi al clúster.&lt;/p&gt;

&lt;p&gt;El siguiente nivel sería identificar el sistema operativo: hasta ahora sólo estoy usando Debian (en máquinas virtuales) o Hypriot OS (en las Raspberry Pi). Si en algún momento empiezo a probar contenedores sobre Windows, sólo tengo que añadir esta etiqueta.&lt;/p&gt;

&lt;p&gt;Por encima del sistema operativo tendría la capa de producto: Docker o Kubernetes, por el momento. También  Hugo, por ejemplo, aunque no sea el objetivo principal del blog.&lt;/p&gt;

&lt;p&gt;Finalmente, alguna etiqueta específica sobre el tema del artículo.&lt;/p&gt;

&lt;p&gt;Creo que esta organización de etiquetas permite identificar todo el &lt;em&gt;stack&lt;/em&gt; usado y así distinguir, especialmente pasado un tiempo, qué componentes estaba usando en cada momento (aunque no se expliciten en el artículo).&lt;/p&gt;

&lt;p&gt;Sería interesante poder incluir el número de versión de &lt;em&gt;cada capa&lt;/em&gt; (RPi 1,2,3, versión del SO, de Docker y Kubernetes&amp;hellip;) pero no se me ocurre cómo hacerlo de manera que sea a la vez útil para agrupar artículos y sin provocar &lt;em&gt;ruido&lt;/em&gt; (mogollón de etiquetas similares, como con &lt;code&gt;raspberry pi&lt;/code&gt;, &lt;code&gt;raspberry pi 2&lt;/code&gt; para poder agrupar por RPi, pero también sobre sólo los artículos sobre RPi2 y no los RPi 3, por ejemplo).&lt;/p&gt;

&lt;h2 id=&#34;fecha-en-el-nombre-de-fichero&#34;&gt;Fecha en el nombre de fichero&lt;/h2&gt;

&lt;p&gt;Para que los ficheros se muestren en un orden similar en el blog y en el sistema de ficheros del portátil, el truco es sencillo: los prefijo con la fecha: &lt;code&gt;yymmdd-nombre-articulo.md&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;No es habitual que escriba varios artículos el mismo día, pero incluso cuando lo hago, diferenciar entre dos o tres artículos no supone un problema.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;$ ls -la content/post/
...
2.6K Apr 30 15:18 170430-k3-colgado-de-nuevo.md
3.5K Apr 30 12:23 170430-multiples-mensajes-action-17-suspended.md
 12K May  6 05:23 170430-troubleshooting-kubernetes-i.md
6.1K May  5 22:51 170505-instala-weave-net-en-kubernetes-1.6.md
4.6K May  6 08:33 170506-tags-categorias-archetypes-en-hugo.md
6.0K May  6 06:11 170506-troubleshooting-kubernetes-ii.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En cuanto al nombre del fichero, uso el guión &lt;code&gt;-&lt;/code&gt; como sustituyo del espacio en el nombre del fichero. Si cambio el título del artículo, renombro el fichero para que &lt;strong&gt;siempre&lt;/strong&gt; el nombre del fichero y el artículo coincidan el máximo posible.&lt;/p&gt;

&lt;h2 id=&#34;agrupar-imágenes&#34;&gt;Agrupar imágenes&lt;/h2&gt;

&lt;p&gt;Al estar conectado por consola a las máquinas virtuales o las Raspberry Pi, no suelo hacer muchas capturas de pantalla.&lt;/p&gt;

&lt;p&gt;En un blog o un wiki, la propia plataforma se encarga de gestionar las imágenes y nunca he tenido problema porque dos imágenes tuvieran el mismo nombre. La inclusión de la imagen en el artículo se realiza de forma gráfica, por lo que el nombre de la imagen tampoco era importante.&lt;/p&gt;

&lt;p&gt;Sin embargo, al escribir los artículos en markdown y tener que enlazar las imágenes manualmente, el nombre del fichero de la imagen &lt;strong&gt;es relevante&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;He decidido crear una carpeta para evitar problemas de &lt;em&gt;colisión de nombres&lt;/em&gt; y tener organizadas todas las imágenes de un mismo artículo. El nombre de la carpeta corresponde a la fecha del artículo (de nuevo, en formato &lt;code&gt;yymmdd&lt;/code&gt;). Dentro de cada carpeta las imágenes se llaman como se tengan que llamar (de manera que tengan un nombre descriptivo) pero sin preocuparme de si ya existe otra imagen con el mismo nombre de fichero.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Troubleshooting Kubernetes (II)</title>
      <link>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</link>
      <pubDate>Sat, 06 May 2017 05:21:09 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</guid>
      <description>&lt;p&gt;Sigo con el &lt;em&gt;troubleshooting&lt;/em&gt; del &lt;em&gt;cuelgue&lt;/em&gt; de los nodos sobre Raspberry Pi 3 del clúster.&lt;/p&gt;

&lt;p&gt;Ayer estuve &lt;em&gt;haciendo limpieza&lt;/em&gt; siguiendo &lt;em&gt;vagamente&lt;/em&gt; la recomendación de &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;esta respuesta&lt;/a&gt; en el hilo &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;Kubernetes memory consumption explosion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;La solución de &lt;code&gt;RenaudWasTaken&lt;/code&gt; al problema de consumo excesivo de memoria (32GB) fue la realizar limpieza de las carpetas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/etcd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Antes de empezar a borrar &lt;em&gt;a lo loco&lt;/em&gt;, revisé el contenido de estas carpetas.&lt;/p&gt;

&lt;h2 id=&#34;var-run-kubernetes&#34;&gt;&lt;code&gt;/var/run/kubernetes&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;En &lt;code&gt;/var/run/kubernetes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/run/kubernetes/
total 8
drwxr-xr-x  2 root root   80 May  5 18:43 .
drwxr-xr-x 18 root root  600 May  5 18:43 ..
-rw-r--r--  1 root root 1082 May  5 18:43 kubelet.crt
-rw-------  1 root root 1679 May  5 18:43 kubelet.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos ficheros son certificados, por lo que no parecen implicados en el problema y decido no borrarlos.&lt;/p&gt;

&lt;h2 id=&#34;var-lib-kubelet&#34;&gt;&lt;code&gt;/var/lib/kubelet&lt;/code&gt;&lt;/h2&gt;

&lt;h3 id=&#34;nodo-k2&#34;&gt;Nodo &lt;strong&gt;k2&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Al intentar listar el contenido de la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt;, la Raspberry Pi 3 ha tardado una eternidad (en los primeros intentos he creído que había dejado de responder).&lt;/p&gt;

&lt;p&gt;Finalmente, el resultado del comando ha mostrado una gran cantidad de carpetas dentro de esta carpeta:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods
...
drwx------    2 root root    4096 May  4 23:11 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~207819844.deleting~559419937.deleting~142152710.deleting~494766199.deleting~952339001
drwx------    2 root root    4096 May  5 18:02 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~274250693.deleting~987962315.deleting~680794233.deleting~917929467
drwx------    2 root root    4096 May  4 23:37 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~108562981.deleting~938554959.deleting~743974077.deleting~274346355.deleting~292131322.deleting~049606881.deleting~105942520.deleting~463246644
drwx------    2 root root    4096 May  4 22:46 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~119491600.deleting~962328406.deleting~220005477.deleting~309794961.deleting~392355244.deleting~378832104.deleting~159122214.deleting~324365539
drwx------    2 root root    4096 Apr 15 20:39 wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_wrapped_flannel-cfg.deleting~443381808.deleting~162549394.deleting~296869341.deleting~353223099.deleting~018715754.deleting~526835026.deleting~320404022.deleting~453576282.deleting~001809150
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parece como si algo no hubiera funcionado correctamente y hubiera entrado en un bucle, creando carpetas y más carpetas. Además, en el nombre de alguna de estas carpetas aparece &lt;code&gt;..._flannel-cfg...&lt;/code&gt;. Esta ha sido la pista que me ha convencido; al intentar instalar el &lt;em&gt;dashboard&lt;/em&gt; de Kubernetes, tuve problemas precisamente porque no tengo instalado Flannel. Eliminé el &lt;em&gt;pod&lt;/em&gt; y no le di más vueltas.&lt;/p&gt;

&lt;p&gt;Sin embargo, la existencia de estas carpetas parece indicar que la eliminación no fue tan limpia como pensaba y que &lt;em&gt;algo&lt;/em&gt; se quedó atrapado en un bucle.&lt;/p&gt;

&lt;p&gt;He lanzado &lt;code&gt;rm -rf /var/lib/kubelet/pods/&lt;/code&gt; y el comando ha fallado indicando que uno de los &lt;em&gt;pods&lt;/em&gt; estaba en uso. Así que he eliminado poco a poco los &lt;em&gt;pods&lt;/em&gt; hasta que al final:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls /var/lib/kubelet/pods/ -la
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Estos &lt;em&gt;pods&lt;/em&gt;, sean los que sean, están en uso (no tengo nada desplegado en el clúster, así que deben ser &lt;em&gt;de sistema&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Tras la limpieza, he reiniciado el nodo.&lt;/p&gt;

&lt;h3 id=&#34;nodo-k3&#34;&gt;Nodo &lt;strong&gt;k3&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;El nodo &lt;strong&gt;k3&lt;/strong&gt; no presentaba estas &lt;em&gt;carpetas sospechosas&lt;/em&gt;, pero también he realizado limpieza:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ rm -rf /var/lib/kubelet/pods/
rm: cannot remove ‘/var/lib/kubelet/pods/3a5e2819-21e5-11e7-bcfd-b827eb650fdb/volumes/kubernetes.io~configmap’: Directory not empty
rm: cannot remove ‘/var/lib/kubelet/pods/71290201-31bb-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/kube-proxy-token-7zk2k’: Device or resource busy
rm: cannot remove ‘/var/lib/kubelet/pods/ef887c6a-31ba-11e7-a0ed-b827eb650fdb/volumes/kubernetes.io~secret/weave-net-token-61scv’: Device or resource busy
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Igual que en el nodo &lt;strong&gt;k2&lt;/strong&gt;, he reiniciado.&lt;/p&gt;

&lt;h2 id=&#34;resultados&#34;&gt;Resultados&lt;/h2&gt;

&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; siguen en estado &lt;code&gt;Ready&lt;/code&gt; tras unas siete y ocho horas, que es bastante más de lo que &lt;em&gt;aguantaban&lt;/em&gt; antes.&lt;/p&gt;

&lt;p&gt;He comprobado que en la carpeta &lt;code&gt;/var/lib/kubelet/pods&lt;/code&gt; sólo aparecen dos &lt;em&gt;pods&lt;/em&gt; (en el nodo &lt;strong&gt;k2&lt;/strong&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 24
drwxr-x--- 4 root root 12288 May  5 18:40 .
drwxr-x--- 4 root root  4096 Apr 15 09:08 ..
drwxr-x--- 5 root root  4096 May  5 18:43 c0323b0f-31bd-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 18:43 f2da9dfb-31bd-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En el nodo &lt;strong&gt;k3&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls -la /var/lib/kubelet/pods/
total 28
drwxr-x--- 5 root root 12288 May  5 19:44 .
drwxr-x--- 4 root root  4096 Apr 15 14:10 ..
drwxr-x--- 3 root root  4096 May  5 19:33 3a5e2819-21e5-11e7-bcfd-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 514d4c93-31c9-11e7-a0ed-b827eb650fdb
drwxr-x--- 5 root root  4096 May  5 19:37 c0b9753a-31c9-11e7-a0ed-b827eb650fdb
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Más adelante actualizaré el artículo para verificar si los nodos siguen activos y sin problemas.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Instala Weave Net en Kubernetes 1.6</title>
      <link>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</link>
      <pubDate>Fri, 05 May 2017 22:14:36 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</guid>
      <description>&lt;p&gt;Una de las cosas que más me sorprenden de Kubernetes es que es necesario instalar una &lt;em&gt;capa de red&lt;/em&gt; sobre el clúster.&lt;/p&gt;

&lt;p&gt;En el caso concreto del que he obtenido las &lt;em&gt;capturas de pantalla&lt;/em&gt;, el clúster corre sobre máquinas virtuales con Debian Jessie.
&lt;/p&gt;

&lt;p&gt;La instalación de Weave Net en Kubernetes consiste únicamente en una línea, como explica el artículo: &lt;a href=&#34;https://www.weave.works/weave-net-kubernetes-integration/&#34;&gt;Run Weave Net with Kubernetes in Just One Line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Antes de instalar la &lt;em&gt;red&lt;/em&gt; en el clúster (de momento, de un solo nodo), &lt;em&gt;kubectl&lt;/em&gt; indica que el estado del nodo es &lt;code&gt;NotReady&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS     AGE       VERSION
k8s       NotReady   5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En la salida del comando tenemos que la versión de Kubernetes es la 1.6.1. Este dato será importante más adelante a la hora de escoger el comando de instalación de Weave Net.&lt;/p&gt;

&lt;p&gt;Si obtenemos la lista de &lt;em&gt;pods&lt;/em&gt;, comprobamos que no tenemos ningún &lt;em&gt;pod de red&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   0          5h
kube-system   kube-apiserver-k8s            1/1       Running   0          5h
kube-system   kube-controller-manager-k8s   1/1       Running   0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending   0          5h
kube-system   kube-proxy-l02zn              1/1       Running   0          5h
kube-system   kube-scheduler-k8s            1/1       Running   0          5h
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, los &lt;em&gt;pods&lt;/em&gt; de &lt;em&gt;DNS&lt;/em&gt; &lt;code&gt;kube-dns-*&lt;/code&gt; están en estado &lt;code&gt;Pending&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Siguiendo las instrucciones del artículo de Weave Net, lanzamos el comando de instalación para versiones 1.6 (o superior):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl apply -f https://git.io/weave-kube-1.6
clusterrole &amp;quot;weave-net&amp;quot; created
serviceaccount &amp;quot;weave-net&amp;quot; created
clusterrolebinding &amp;quot;weave-net&amp;quot; created
daemonset &amp;quot;weave-net&amp;quot; created
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obtenemos la lista de &lt;em&gt;pods&lt;/em&gt; de nuevo y observamos que se están creando dos nuevos contenedores:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;operador@k8s:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             0          5h
kube-system   kube-apiserver-k8s            1/1       Running             0          5h
kube-system   kube-controller-manager-k8s   1/1       Running             0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       Pending             0          5h
kube-system   kube-proxy-l02zn              1/1       Running             0          5h
kube-system   kube-scheduler-k8s            1/1       Running             0          5h
kube-system   weave-net-32ptg               0/2       ContainerCreating   0          12s
operador@k8s:~$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De hecho, se creado el &lt;em&gt;daemonset&lt;/em&gt; &amp;ldquo;weave-net&amp;rdquo;. Un &lt;em&gt;daemonset&lt;/em&gt; es un &lt;em&gt;pod&lt;/em&gt; que se crea en cada uno de los nodos del clúster automáticamente. Kubernetes se encarga de descargar la imagen desde DockerHub y arrancar un contenedor. Los nodos en la red creada por Weave Net forman una red &lt;em&gt;mesh&lt;/em&gt; que se configura automáticamente, de manera que es posible agregar nodos adicionales sin necesidad de cambiar ninguna configuración.&lt;/p&gt;

&lt;p&gt;Pasados unos segundos la creación de los nodos se ha completado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$  kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS         RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running        0          5h
kube-system   kube-apiserver-k8s            1/1       Running        0          5h
kube-system   kube-controller-manager-k8s   1/1       Running        0          5h
kube-system   kube-dns-3913472980-4nlg9     0/3       ErrImagePull   0          5h
kube-system   kube-proxy-l02zn              1/1       Running        0          5h
kube-system   kube-scheduler-k8s            1/1       Running        0          5h
kube-system   weave-net-32ptg               2/2       Running        0          1m
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalmente, verificamos que el primer nodo del clúster ya es operativo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME      STATUS    AGE       VERSION
k8s       Ready     5h        v1.6.1
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Además, una vez que tenemos la red instalada en el clúster, el &lt;em&gt;pod&lt;/em&gt; &lt;code&gt;kube-dns&lt;/code&gt; comienza la creación de los contenedores (quizás tengas que reiniciar):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS              RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running             1          11d
kube-system   kube-apiserver-k8s            1/1       Running             1          11d
kube-system   kube-controller-manager-k8s   1/1       Running             1          11d
kube-system   kube-dns-3913472980-4nlg9     0/3       ContainerCreating   0          11d
kube-system   kube-proxy-l02zn              1/1       Running             1          11d
kube-system   kube-scheduler-k8s            1/1       Running             1          11d
kube-system   weave-net-32ptg               2/2       Running             3          10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tras unos segundos, tenemos todos los &lt;em&gt;pods&lt;/em&gt; del clúster funcionales:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s                      1/1       Running   1          11d
kube-system   kube-apiserver-k8s            1/1       Running   1          11d
kube-system   kube-controller-manager-k8s   1/1       Running   1          11d
kube-system   kube-dns-3913472980-4nlg9     3/3       Running   0          11d
kube-system   kube-proxy-l02zn              1/1       Running   1          11d
kube-system   kube-scheduler-k8s            1/1       Running   1          11d
kube-system   weave-net-32ptg               2/2       Running   3          10d
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora sólo tenemos que añadir nodos &lt;em&gt;worker&lt;/em&gt; y hacer crecer el clúster.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Troubleshooting Kubernetes (I)</title>
      <link>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</link>
      <pubDate>Sun, 30 Apr 2017 15:24:35 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</guid>
      <description>&lt;p&gt;Tras la alegría inicial pensando que la configuración de &lt;em&gt;rsyslog&lt;/em&gt; era la causante de los cuelgues de las dos RPi 3 (&lt;a href=&#34;https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/&#34;&gt;El nodo k3 del clúster colgado de nuevo&lt;/a&gt;), pasadas unas horas los dos nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; han dejado de responder de nuevo.&lt;/p&gt;

&lt;p&gt;Así que es el momento de atacar el problema de forma algo más sistemática. Para ello seguiré las instrucciones que proporcina la página de Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/&#34;&gt;Troubleshooting Clusters&lt;/a&gt;.
&lt;/p&gt;

&lt;h2 id=&#34;descripción-del-problema&#34;&gt;Descripción del problema&lt;/h2&gt;

&lt;p&gt;Tras unas horas activos y formando parte del clúster, los dos nodos que corren sobre Raspberry Pi 3 dejan de responder y el clúster los muestra como &lt;em&gt;NotReady&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;El clúster está formado por tres Raspberry Pi; el nodo &lt;em&gt;master&lt;/em&gt; es una Raspberry Pi 2 B mientras que los dos nodos &lt;em&gt;worker&lt;/em&gt; son Raspberry Pi 3 B.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes -o wide
NAME      STATUS     AGE       VERSION   EXTERNAL-IP   OS-IMAGE                        KERNEL-VERSION
k1        Ready      19d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
k2        NotReady   15d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
k3        NotReady   14d       v1.6.2    &amp;lt;none&amp;gt;        Raspbian GNU/Linux 8 (jessie)   4.4.50-hypriotos-v7+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ping&#34;&gt;Ping&lt;/h2&gt;

&lt;h3 id=&#34;ping-al-nombre-del-nodo&#34;&gt;Ping al nombre del nodo&lt;/h3&gt;

&lt;p&gt;La prueba más sencilla para ver si los nodos están colgados, es lanzar un ping desde el portátil:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ping -c5 k2.local
ping: cannot resolve k2.local: Unknown host
$ ping -c5 k3.local
ping: cannot resolve k3.local: Unknown host
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En esta prueba vemos que ninguno de los nodos responde al nombre que &lt;em&gt;publica&lt;/em&gt; el servicio &lt;a href=&#34;https://en.wikipedia.org/wiki/Avahi_(software)&#34;&gt;Avahi&lt;/a&gt; en el sistema.&lt;/p&gt;

&lt;h3 id=&#34;ping-a-la-ip-del-nodo&#34;&gt;Ping a la IP del nodo&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ping -c5 192.168.1.12
PING 192.168.1.12 (192.168.1.12): 56 data bytes
64 bytes from 192.168.1.12: icmp_seq=0 ttl=64 time=3.842 ms
64 bytes from 192.168.1.12: icmp_seq=1 ttl=64 time=6.678 ms
64 bytes from 192.168.1.12: icmp_seq=2 ttl=64 time=10.789 ms
64 bytes from 192.168.1.12: icmp_seq=3 ttl=64 time=7.411 ms
64 bytes from 192.168.1.12: icmp_seq=4 ttl=64 time=10.518 ms

--- 192.168.1.12 ping statistics ---
5 packets transmitted, 5 packets received, 0.0% packet loss
round-trip min/avg/max/stddev = 3.842/7.848/10.789/2.584 ms
$ ping -c5 192.168.1.13
PING 192.168.1.13 (192.168.1.13): 56 data bytes
Request timeout for icmp_seq 0
Request timeout for icmp_seq 1
Request timeout for icmp_seq 2
Request timeout for icmp_seq 3

--- 192.168.1.13 ping statistics ---
5 packets transmitted, 0 packets received, 100.0% packet loss
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En este caso, el nodo &lt;strong&gt;k2&lt;/strong&gt; sí que responde a ping a la IP, mientras que el nodo &lt;strong&gt;k3&lt;/strong&gt; actúa como si estuviera apagado o con la red deshabilitada.&lt;/p&gt;

&lt;h3 id=&#34;ssh&#34;&gt;SSH&lt;/h3&gt;

&lt;p&gt;Aunque el nodo &lt;strong&gt;k2&lt;/strong&gt; responde a ping, no es posible conectar vía SSH; el intento de conectar no tiene éxito, pero tampoco falla (por &lt;em&gt;timeout&lt;/em&gt;, por ejemplo). He probado a conectar tanto desde el portátil como desde el nodo &lt;strong&gt;k1&lt;/strong&gt;, con el mismo resulado:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ssh pirate@192.168.1.12

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubelet-describe-node&#34;&gt;kubelet describe node&lt;/h2&gt;

&lt;p&gt;Usamos el comando &lt;code&gt;kubelet describe node&lt;/code&gt; para los dos nodos colgados.&lt;/p&gt;

&lt;h3 id=&#34;nodo-k2&#34;&gt;Nodo &lt;strong&gt;k2&lt;/strong&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k2
Name:			k2
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k2
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			&amp;lt;none&amp;gt;
CreationTimestamp:	Sat, 15 Apr 2017 10:25:31 +0000
Phase:
Conditions:
  Type			Status		LastHeartbeatTime			LastTransitionTime			Reason			Message
  ----			------		-----------------			------------------			------			-------
  OutOfDisk 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  MemoryPressure 	Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  DiskPressure 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  Ready 		Unknown 	Sun, 30 Apr 2017 11:56:26 +0000 	Sun, 30 Apr 2017 11:57:11 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
Addresses:		192.168.1.12,192.168.1.12,k2
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			84bf8a2b-b83f-445b-a4b3-250dc6e5db40
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.2.0/24
ExternalID:			k2
Non-terminated Pods:		(2 in total)
  Namespace			Name				CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----				------------	----------	---------------	-------------
  kube-system			kube-proxy-g580s		0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-kxpk6			20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  20m (0%)	0 (0%)		0 (0%)		0 (0%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodo-k3&#34;&gt;Nodo &lt;strong&gt;k3&lt;/strong&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k3
Name:			k3
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k3
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			&amp;lt;none&amp;gt;
CreationTimestamp:	Sat, 15 Apr 2017 14:10:06 +0000
Phase:
Conditions:
  Type			Status		LastHeartbeatTime			LastTransitionTime			Reason			Message
  ----			------		-----------------			------------------			------			-------
  OutOfDisk 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  MemoryPressure 	Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  DiskPressure 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
  Ready 		Unknown 	Sun, 30 Apr 2017 10:33:45 +0000 	Sun, 30 Apr 2017 10:34:28 +0000 	NodeStatusUnknown 	Kubelet stopped posting node status.
Addresses:		192.168.1.13,192.168.1.13,k3
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			23bf96e4-ec65-489c-be00-d0fa848265f3
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.3.0/24
ExternalID:			k3
Non-terminated Pods:		(2 in total)
  Namespace			Name				CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----				------------	----------	---------------	-------------
  kube-system			kube-proxy-bkl4g		0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-3bf40			20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  20m (0%)	0 (0%)		0 (0%)		0 (0%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El nodo &lt;strong&gt;k2&lt;/strong&gt; deja de responder a las 11:56:26, mientras que el &lt;strong&gt;k3&lt;/strong&gt; lo hace a las 10:33:45.&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Cuando reinicie los dos nodos lo haré a la misma hora, para comprobar si hay diferencias en el tiempo que tarda en dejar de responder cada nodo.&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Actualizar la zona horaria de las Raspberry Pi.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;nodo-k1-ready&#34;&gt;Nodo &lt;strong&gt;k1&lt;/strong&gt; (&lt;code&gt;Ready&lt;/code&gt;)&lt;/h3&gt;

&lt;p&gt;Como referencia, incluimos el mismo comando para el nodo &lt;em&gt;master&lt;/em&gt; &lt;strong&gt;k1&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl describe node k1
Name:			k1
Role:
Labels:			beta.kubernetes.io/arch=arm
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=k1
			node-role.kubernetes.io/master=
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			node-role.kubernetes.io/master:NoSchedule
CreationTimestamp:	Mon, 10 Apr 2017 20:22:32 +0000
Phase:
Conditions:
  Type			Status	LastHeartbeatTime			LastTransitionTime			Reason				Message
  ----			------	-----------------			------------------			------				-------
  OutOfDisk 		False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasSufficientDisk 	kubelet has sufficient disk space available
  MemoryPressure 	False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasSufficientMemory 	kubelet has sufficient memory available
  DiskPressure 		False 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:10 +0000 	KubeletHasNoDiskPressure 	kubelet has no disk pressure
  Ready 		True 	Sun, 30 Apr 2017 14:01:10 +0000 	Sun, 30 Apr 2017 06:37:20 +0000 	KubeletReady 			kubelet is posting ready status
Addresses:		192.168.1.11,192.168.1.11,k1
Capacity:
 cpu:		4
 memory:	882632Ki
 pods:		110
Allocatable:
 cpu:		4
 memory:	780232Ki
 pods:		110
System Info:
 Machine ID:			9989a26f06984d6dbadc01770f018e3b
 System UUID:			9989a26f06984d6dbadc01770f018e3b
 Boot ID:			55e1fad0-d40c-480b-b039-5586ff728d2c
 Kernel Version:		4.4.50-hypriotos-v7+
 OS Image:			Raspbian GNU/Linux 8 (jessie)
 Operating System:		linux
 Architecture:			arm
 Container Runtime Version:	docker://Unknown
 Kubelet Version:		v1.6.2
 Kube-Proxy Version:		v1.6.2
PodCIDR:			10.244.0.0/24
ExternalID:			k1
Non-terminated Pods:		(7 in total)
  Namespace			Name					CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----					------------	----------	---------------	-------------
  kube-system			etcd-k1					0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-apiserver-k1			250m (6%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-controller-manager-k1		200m (5%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-dns-279829092-1b27r		260m (6%)	0 (0%)		110Mi (14%)	170Mi (22%)
  kube-system			kube-proxy-3dggd			0 (0%)		0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-scheduler-k1			100m (2%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			weave-net-6qr0l				20m (0%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  830m (20%)	0 (0%)		110Mi (14%)	170Mi (22%)
Events:		&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;revisando-los-logs-en-el-nodo-master&#34;&gt;Revisando los logs en el nodo &lt;em&gt;master&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;En la guía de &lt;em&gt;Troubleshooting&lt;/em&gt; de Kubernetes, el siguiente paso es revisar los logs. En el caso del nodo &lt;em&gt;master&lt;/em&gt;, los logs relevantes se encuentran en:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-apiserver.log&lt;/code&gt; - El &lt;em&gt;API Server&lt;/em&gt;, encargado de servir la API&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-scheduler.log&lt;/code&gt; - El &lt;em&gt;Scheduler&lt;/em&gt;, encargado de las decisiones de planificar los &lt;em&gt;pods&lt;/em&gt; en los nodos&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/log/kube-controller-manager.log&lt;/code&gt; - El responsable de gestionar los &lt;em&gt;replication controllers&lt;/em&gt; encargados de mantener el &lt;strong&gt;estado deseado&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sin embargo, los logs indicados &lt;strong&gt;no existen en la ruta indicada&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls /var/log/kube*
ls: cannot access /var/log/kube*: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Es probable que la documentación no esté actualizada, así que continuaré en cuanto encuentre los logs para poder revisarlos.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Errores sobre Orphaned pods en syslog</title>
      <link>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</link>
      <pubDate>Sun, 30 Apr 2017 12:55:44 +0200</pubDate>
      
      <guid>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</guid>
      <description>&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; del clúster dejan de responder pasadas unas horas. La única manera de solucionarlo es reiniciar los nodos. Siguiendo con la revisión de logs, he encontrado que se genera una gran cantidad de entradas en &lt;em&gt;syslog&lt;/em&gt; en referencia a &lt;em&gt;orphaned pods&lt;/em&gt;. Además, el número de estos errores no para de crecer &lt;strong&gt;rápidamente&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
118938
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119022
$ grep &amp;quot;kubelet_volumes.go:114] Orphaned pod&amp;quot; /var/log/syslog | wc -l
119170
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Revisando las últimas entradas del log:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-log&#34;&gt;Apr 30 10:57:57 k2 kubelet[3619]: E0430 10:57:57.186318    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;5064b9d9-2c9e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:01 k2 kubelet[3619]: E0430 10:58:01.759595    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;6c601e9c-2c9c-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.226372    3619 kubelet.go:1549] Unable to mount volumes for pod &amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]; skipping pod
Apr 30 10:58:03 k2 kubelet[3619]: E0430 10:58:03.238315    3619 pod_workers.go:182] Error syncing pod 4461d51d-2d93-11e7-a7ae-b827eb650fdb (&amp;quot;weave-net-bs9bs_kube-system(4461d51d-2d93-11e7-a7ae-b827eb650fdb)&amp;quot;), skipping: timeout expired waiting for volumes to attach/mount for pod &amp;quot;kube-system&amp;quot;/&amp;quot;weave-net-bs9bs&amp;quot;. list of unattached/unmounted volumes=[weavedb cni-bin cni-bin2 cni-conf dbus lib-modules weave-net-token-61scv]
Apr 30 10:58:05 k2 kubelet[3619]: E0430 10:58:05.830432    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;bb4d3ea6-2b80-11e7-9388-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
Apr 30 10:58:08 k2 kubelet[3619]: E0430 10:58:08.435567    3619 kubelet_volumes.go:114] Orphaned pod &amp;quot;cb23be0d-2d7e-11e7-a7ae-b827eb650fdb&amp;quot; found, but volume paths are still present on disk.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Todas estas entradas se encuentran en los logs del nodo &lt;strong&gt;k2&lt;/strong&gt;, donde no hay ningún &lt;em&gt;pod&lt;/em&gt; en ejecución (a parte de los propios de  Kubernetes que el clúster planifica en los diferentes nodos).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>